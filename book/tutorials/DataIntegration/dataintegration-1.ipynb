{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suffering-union",
   "metadata": {},
   "source": [
    "# Data integration with ICESat-2 - Part I\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "**Goals**\n",
    "- Identify and locate non-ICESat-2 data sets\n",
    "- Acquiring data from the cloud or via download\n",
    "- Open data in Pandas and Xarray and basic functioning of DataFrames\n",
    "```\n",
    "\n",
    "```{admonition} Key Takeaway\n",
    "You will be able to open a time series of Cloud Optimized Geotiffs with multiple bands from the cloud directly into memory in Xarray and visualize them with ICESat-2 and ATM data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934df76-18b3-4581-bc96-6b51f930c2c4",
   "metadata": {},
   "source": [
    "For this tutorial, feel free to run the code along with us as we live code by downsizing the zoom window and splitting your screen (or using two screens). Or you can simply watch the zoom walkthrough. Don't worry if you fall behind on the code. The notebook is standalone and you can easily run the code at your own pace another time to catch anything you missed. \n",
    "\n",
    "We will have one exercise you can type into a notebook, or figure out in a separate document. We will also ask some questions that you can responsd to in the tutorial Slack channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d863c-5ad1-4b9f-9fa0-416ff04396a4",
   "metadata": {},
   "source": [
    "## Computing environment\n",
    "\n",
    "We'll be using the following open source Python libraries in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49392ebb-e958-4e9c-87c5-b883430d3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyleaflet\n",
    "from ipyleaflet import Map, GeoData, LayersControl,Rectangle, basemaps, basemap_to_tiles, TileLayer, SplitMapControl, Polygon\n",
    "\n",
    "import ipywidgets\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5720d5-d805-4c24-a92e-a2238805cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import satsearch\n",
    "from satsearch import Search\n",
    "import geopandas as gpd\n",
    "import ast\n",
    "import pandas as pd\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display, Image\n",
    "import intake # if you've installed intake-STAC, it will automatically import alongside intake\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import rasterio as rio\n",
    "from rasterio.session import AWSSession\n",
    "from rasterio.plot import show\n",
    "import rioxarray as rxr\n",
    "from dask.utils import SerializableLock\n",
    "import os\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "# Suppress library deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99d916-deb8-4af0-9fce-1e38e5c18d87",
   "metadata": {},
   "source": [
    "## 1. Identify and acquire the ICESat2 product(s) of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d66e60-1c1d-4887-b275-bd92deffae73",
   "metadata": {},
   "source": [
    "* What is the application of this product?\n",
    "* What region and resolution is needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095b2a6-2ab5-440e-a3b8-620cf5c1868a",
   "metadata": {},
   "source": [
    "#### Download ICESat-2 ATL03 data from desired region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b2612c-9808-4b32-94f5-6cd007fbe4e4",
   "metadata": {},
   "source": [
    "Remember icepyx? We are going to use that again to download some ICESat-2 ATL06 data over our region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd8ca2-fa0b-4fb6-b03f-19d8419d3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import icepyx as ipx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1a4d6-69ee-4687-905b-c384e55d6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the necessary icepyx parameters\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = 'hackweek_kml_jakobshavan.kml' # KML polygon centered on Jakobshavan\n",
    "date_range = ['2019-04-01', '2019-04-30']\n",
    "rgts = ['338'] # IS-2 RGT of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a9398-11bc-4a25-a19e-137746429896",
   "metadata": {},
   "source": [
    "You may notice that we specified a RGT track. As seen below, a large number of ICESat-2 overpasses occur for Jakobshavan. In the interest of time (and computer memory), we are going to look at only one of these tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d389e8f-84a8-41e6-8c43-e56c5f126877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image of area of interest (data viz tutorial will get in deeper so don't explain much):\n",
    "center = [69.2, -50]\n",
    "zoom = 7\n",
    "\n",
    "# Open KML file for visualizing\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "jk = gpd.read_file(spatial_extent, driver='KML')\n",
    "\n",
    "m = Map(basemap=basemap_to_tiles(basemaps.NASAGIBS.ModisAquaTrueColorCR, '2020-07-18'),center=center,zoom=zoom)\n",
    "geo_data = GeoData(geo_dataframe = jk)\n",
    "\n",
    "m.add_layer(geo_data)\n",
    "m.add_control(LayersControl())\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2c02c-a607-4937-a89a-c0db822ab4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Query object\n",
    "region = ipx.Query(short_name, spatial_extent, date_range, tracks=rgts)\n",
    "\n",
    "# Show the available granules\n",
    "region.avail_granules(ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e44e93-9c3a-41b3-bdf4-433fae6f554a",
   "metadata": {},
   "source": [
    "Looks like we have an ICESat-2 track! Let's quickly visualize the data to ensure that there are no clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45957b30-4aab-4191-be11-7bfaaa0e2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request information from OpenAltimetry\n",
    "cyclemap, rgtmap = region.visualize_elevation()\n",
    "\n",
    "rgtmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55b081-09e8-40bc-9306-7bee59725498",
   "metadata": {},
   "source": [
    "Looks good! Now it's time to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353d1a2-e7b5-4803-a59a-9b793c23e2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Earthdata credentials\n",
    "uid =\n",
    "email = \n",
    "region.earthdata_login(uid, email)\n",
    "\n",
    "# Order the granules\n",
    "region.order_granules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218b130-aa73-4c62-821e-2bbf456ad064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "path = '/home/jovyan/website2022/book/tutorials/DataIntegration/'\n",
    "region.download_granules(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c92508-d237-4203-84c5-fccc31ecc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Load the ICESat-2 data. We will just look at the central beams (GT2R/L)\n",
    "is2_file = 'processed_ATL06_20190420093051_03380303_005_01_full.h5'\n",
    "with h5py.File(is2_file, 'r') as f:\n",
    "    is2_gt2r = pd.DataFrame(data={'lat': f['gt2r/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2r/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2r/land_ice_segments/h_li'][:]})\n",
    "    is2_gt2l = pd.DataFrame(data={'lat': f['gt2l/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2l/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2l/land_ice_segments/h_li'][:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778b651-2079-4aa4-b9e9-bd4f460c3ff8",
   "metadata": {},
   "source": [
    "## 2. Identify other products of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30999772-d615-4009-a34e-91ae7c5b5627",
   "metadata": {},
   "source": [
    "### Question - Respond in Slack\n",
    "What research problems have you wanted to address that require more than one dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8793d3-1abf-4ecd-ae0e-7c948c0527d5",
   "metadata": {},
   "source": [
    "### Where are other data sets stored?\n",
    "\n",
    "* Cloud datasets in AWS \n",
    "   https://registry.opendata.aws/ \n",
    "* NASA EarthData\n",
    "   https://search.earthdata.nasa.gov/search/\n",
    "* ESA Copernicus Hub\n",
    "   https://scihub.copernicus.eu\n",
    "* Etc.\n",
    "\n",
    "More on this in the Cloud Computing Tools tutorial\n",
    "\n",
    "Today, we will show ATM (non-AWS) and Landsat (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc23b5-b0a0-4feb-acce-25b8980f79a3",
   "metadata": {},
   "source": [
    "## 3. Acquire non-cloud data and open:  ATM data access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ff2e2-3ad8-4014-8ebd-2413ffb5a468",
   "metadata": {},
   "source": [
    "Why did we choose April 2019 and RGT 338? In Spring 2019, an airborne campaign called Operation IceBridge was flown across Jakobshavan as validation for ICESat-2. Onboard was the Airborne Topographic Mapper, a lidar that works at both 532 nm (like ICESat-2) and 1064 nm (near-infrared). More information about Operation IceBridge and ATM may be found here: https://nsidc.org/data/icebridge\n",
    "\n",
    "Here, we are going to try and co-register ATM spot measurements with ICESat-2. Because both data sets are rather large, this can be computationally expensive, so we will only consider one flight track with the ATM 532 nm beam.\n",
    "\n",
    "Operation IceBridge data is not available on the cloud, so this data was downloaded directly from NSIDC. If you are interested in using IceBridge data, NSIDC has a useful data portal here: https://nsidc.org/icebridge/portal/map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37608a-69c9-408b-9e8e-781edd93a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ATM data into a DataFrame\n",
    "atm_file = 'ILATM2_20190506_151600_smooth_nadir3seg_50pt.csv'\n",
    "atm_l2 = pd.read_csv(atm_file)\n",
    "\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db1141-a749-40df-b622-5c11ce3fea2c",
   "metadata": {},
   "source": [
    "We opened this data into a Pandas DataFrame, which is a handy tool for Earth data exploration and analysis. The column names derive automatically from the first row of the csv and each row corresponds to an ATM measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec8435-262f-460b-ba30-e46feeb5cdb3",
   "metadata": {},
   "source": [
    "### Opening and manipulating data in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab7dbb-ad2a-4f23-b883-5ddc376ca131",
   "metadata": {},
   "source": [
    "Pandas excels at helping you explore, clean, and process tabular data, such as data stored in spreadsheets or databases. In pandas, a Series is a 1-D data table and a DataFrame is the 2-D data table, which we just saw above.\n",
    "\n",
    "Read csv, the funtion we used above, is the easiest way to open a csv data file into a Pandas DataFrame. We can specify formating, data selection, indexing, and much more when reading any data into a Pandas DataFrame. Below we read in the data again, specifying different headers, assigning the first column as the index, and assigning the first column as the header (even though we are renaming it, so that it doesn't mistake the first row in the csv as data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ee715-1307-4520-8092-cb1344aeda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with specific headers\n",
    "headers = ['UTC', 'Lat', 'Lon',\n",
    "       'Height', 'South-to-North_Slope',\n",
    "       'West-to-East_Slope', 'RMS_Fit', 'Number_Measurements',\n",
    "       'Number_Measurements_Removed',\n",
    "       'Distance_Of_Block_To_The_Right_Of_Aircraft', 'Track_Identifier']\n",
    "atm_l2 = pd.read_csv(atm_file, names=headers, index_col=0, header=0)\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e62c0-c684-4607-bd35-4d8d73c9e482",
   "metadata": {},
   "source": [
    "Now we can explore the data and DataFrame functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878d354-6a04-464d-8073-7fc19032c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the names of the columns\n",
    "atm_l2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ada8f-28e7-4142-8f26-b987555bb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data in only one of those columns\n",
    "atm_l2['Lat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d061be-cbde-4d8a-871d-640454498b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, but another way\n",
    "atm_l2.Lat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ba867-452c-484e-995a-3cb61f68db15",
   "metadata": {},
   "source": [
    "If we want something more intuitive for our index, we can create a column of datetime objects that uses the date from the ATM file name and seconds_of_day column as date and time information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88611022-c007-450a-94de-6793349a12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "atm_l2 = atm_l2.reset_index()\n",
    "\n",
    "# Use the date in the string of the file name to create a datetime of the date\n",
    "date = pd.to_datetime(atm_file[7:15])\n",
    "\n",
    "# Use UTC seconds_of_day column to calculate time of day that we will use to add time to the datetime\n",
    "time = pd.to_timedelta(atm_l2.UTC, unit='s')\n",
    "\n",
    "# Add the time to date and set as index\n",
    "atm_l2['DateTime'] = date + time\n",
    "atm_l2 = atm_l2.set_index('DateTime')\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06f88e-d7f3-46c3-a0a1-24e6e3af5993",
   "metadata": {},
   "source": [
    "Now we can easily slice data by date, month, day, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60247b-f5f3-4d85-877f-32c0b55b6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['2019-05-06'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2232f-c4a2-4942-b5df-ea1addc936fb",
   "metadata": {},
   "source": [
    "Or by a range of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437bbc1-9ee8-4747-8f28-c65c52e0f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['2019-05-06':'2019-05-07']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58c7e1-0bbf-4600-aa3d-89769145382e",
   "metadata": {},
   "source": [
    "#### Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a719fd6-d631-493b-b0d9-e870d8236364",
   "metadata": {},
   "source": [
    "Two methods for slicing:\n",
    "\n",
    "* .loc for label-based indexing\n",
    "* .iloc for positional indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d72be9-880d-41ef-a4d6-1c6a4beae727",
   "metadata": {},
   "source": [
    "Use loc to slice specific indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767957b0-d163-493f-8214-23a9ac02d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efbefa8-e742-492f-9251-dd9ae6933bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.loc['2019-05-06 15:16:09.500']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d96268-56c3-45ad-a054-bbe2a1b0fc34",
   "metadata": {},
   "source": [
    "Select indices and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d4c71-8dc3-4b2f-a3a7-7a05d81042a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.loc['2019-05-06 15:16:09.500', ['Lat', 'Lon', 'Height']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe53650-d5f8-4cb1-8cb0-9f2529d180fd",
   "metadata": {},
   "source": [
    "Use iloc to select by row and column number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538551c-689f-4506-8839-017a2b3459bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.iloc[[0,1],[2,3,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478453b-0d95-41ab-a5ca-ae0f111c6f59",
   "metadata": {},
   "source": [
    "#### Statistical manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa5f2cb-1cd8-4107-b8e0-8f5a6b360900",
   "metadata": {},
   "source": [
    "Say we want to know the mean ATM height for the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429cc14-7e2b-4d37-944b-2956efca8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['Height'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae523d-daf2-46de-9b15-36ae7de6791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['Height'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eaf9da-7c4d-45cd-b749-d89781623375",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['Number_Measurements'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3d180-a112-4eaf-adcc-9e53e7d35940",
   "metadata": {},
   "source": [
    "If we want to know the mean of each column grouped for each Track Identifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc1908-f0cb-4856-bca5-4c7bf4a77fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rows together by a column and take the mean of each group\n",
    "atm_l2.groupby('Track_Identifier').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe0a19-3ede-43b2-97ee-1fa54682ad77",
   "metadata": {},
   "source": [
    "Groupby is pretty complex. You can dive deeper here: \n",
    "    https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de52fa3-676f-4446-9b2b-0e3df7c3f922",
   "metadata": {},
   "source": [
    "You can also resample your data to get the mean of all measurements at 30 second intervals..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef557f6b-e978-4196-8ed1-a2fe064bd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.resample('30S').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e5878-e444-4891-b290-7626275ee002",
   "metadata": {},
   "source": [
    "To use your own functions, one might first try to use a for loop to iterate over rows or columns of data. Pandas has made an easy and fast alternative, __apply()__. This function acts as a map() function in Python. It takes a function as an input and applies the function to an entire DataFrame. \n",
    "\n",
    "Something easy could be to take the median of each column of the data. We specify the np.median function and axis=0 to pass in all rows and iterate over each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f1541-3047-4669-a30d-902c32042598",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.apply(np.median, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b80c35-56d8-4216-bf82-2edd08132c73",
   "metadata": {},
   "source": [
    "Say you want to use only specific rows or columns in your function. For instance, you want to know the total number of measurements (i.e. Number_Measurements + Number_Measurements_Removed). We already have a function that takes two columns and adds them together. Now we want to apply it to the data.\n",
    "\n",
    "First, we call the .apply() method on the atm_l2 dataframe. Then use the lambda function to iterate over the rows (or columns) of the dataframe. For every row, we grab the Number_Measurements and Number_Measurements_Removed columns and pass them to the calc_total function. Finally, we specify the axis=1 to tell the .apply() method that we are passing in columns to apply the function to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82ab1a-1d9d-40bd-8853-be51a5a3aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_total(a,b):\n",
    "    total = a + b\n",
    "    return total\n",
    "\n",
    "atm_l2['Total_Measurements'] = atm_l2.apply(lambda row: calc_total(row['Number_Measurements'], row['Number_Measurements_Removed']), axis=1)\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d66bfe-d1f5-47bf-b286-f583f2af560d",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31814149-471d-438e-a3cb-06f587e2ab1c",
   "metadata": {},
   "source": [
    "Try making .apply work with this new function to create a new column, 'Distance_to_Jakobshavn', using the Lat and Lon columns as inputs. We've already supplied the function, which requires a latitude and longitude input to calculate the distance between the ATM measurement and a specified point on the Jakobshavn Glacier ice front (`jlat`/`jlon`). Complete the line that applies the function to those columns in the data to get the Distance_to_Jakobshavn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61273a29-9d07-4d2e-8272-aad6851e22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "def distance(a,b):\n",
    "    '''\n",
    "    Calculate distance between a set point and a lat lon pair from the data\n",
    "    a = lat\n",
    "    b = lon\n",
    "    '''\n",
    "    \n",
    "    jlat,jlon = 69.2330, -49.2434\n",
    "    \n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(jlat)\n",
    "    lon1 = radians(jlon)\n",
    "    lat2 = radians(a)\n",
    "    lon2 = radians(b)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "atm_l2['Distance_to_Jakobshavn']  = .....\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c5439-b443-4ab0-bcb0-68811f4d12d6",
   "metadata": {},
   "source": [
    "Great work! Now let's reset the index to start fresh for multi-indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7d385-7b9d-4a88-9ebf-84e96711508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2 = atm_l2.reset_index()\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c7cfc-d241-49ca-ac2e-097916898711",
   "metadata": {},
   "source": [
    "#### Multi-indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a07859-f8f7-4653-941c-b89a69ba237e",
   "metadata": {},
   "source": [
    "Multi-level indexing opens the door to more sophisticated data analysis and manipulation, especially for working with higher dimensional data. In essence, it enables you to store and manipulate data with an arbitrary number of dimensions in lower dimensional data structures like Series (1d) and DataFrame (2d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b1f16-1b3f-425f-8295-6fd8508810ed",
   "metadata": {},
   "source": [
    "Here, we will demonstrate a few basic things you can do with MultiIndexing. If we wanted to think about our data by DateTime and then by Track Identifier, we would set both colomns as indices..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1facdde2-2694-450d-aed5-64c325c155b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2 = atm_l2.set_index(['DateTime','Track_Identifier']).sort_index()\n",
    "atm_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb491ac-8c5a-4561-a34a-926653255483",
   "metadata": {
    "tags": []
   },
   "source": [
    "For some terminology, the `levels` of a MultiIndex are the former column names (UTC, Track_Identifier). The `labels` are the actual values in a `level`, (2019-05-06 15:16:09.500, 0-3, ...). `Levels` can be referred to by name or position, with 0 being the outermost level.\n",
    "\n",
    "Slicing the outermost index level is pretty easy, we just use our regular .loc[row_indexer, column_indexer] to grab a couple datetimes we want. We'll select the columns Lat and Lon where the UTC was `2019-05-06 15:16:09.500` and `2019-05-06 15:16:09.750`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e93f75-5b55-4220-9e70-761c7451f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.loc['2019-05-06 15:16:09.500',['Lat','Lon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864dfa9-af81-4916-8596-3387f1d060f4",
   "metadata": {},
   "source": [
    "If you wanted to select the rows whose track was 0 or 1, .loc wants [row_indexer, column_indexer] so let's wrap the two elements of our row indexer (the list of UTCs and the Tracks) in a tuple to make it a single unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc2e79-8ad6-405b-bf84-5a63d8e467db",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.loc[('2019-05-06 15:16:09.500',[0,1]),['Lat','Lon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e280f-2324-44a2-9d26-cda88b787611",
   "metadata": {},
   "source": [
    "Now you want info from any UTC but for the specific tracks again (0,1). Below the : indicates all labels in this level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1574fd-5ac8-459e-87b9-851fdc8c8c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.loc[pd.IndexSlice[:,[0,1]], ['Lat','Lon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a7686-5ae6-472c-951e-87ed0ee2dadc",
   "metadata": {},
   "source": [
    "You can do a lot with groupby, pivoting, and reshaping, but I won't dive into that in this tutorial. You can check out more here: https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html\n",
    "\n",
    "Now that we are oriented with the Pandas DataFrame, let's get back to the ATM data we have opened and co-register it with the ICESat-2 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f3368-2574-41f8-aa58-14900138bdf9",
   "metadata": {},
   "source": [
    "### Co-register ICESat-2 with ATM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b6049-514c-4109-a16d-a925b3710c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ATM data into a DataFrame\n",
    "atm_file = 'ILATM2_20190506_151600_smooth_nadir3seg_50pt.csv'\n",
    "atm_l2 = pd.read_csv(atm_file)\n",
    "\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ff1c7-504e-47bf-9bf4-5a64380add00",
   "metadata": {},
   "source": [
    "The ATM L2 file contains plenty of information, including surface height estimates and slope of the local topography. It also contains a track identifier - ATM takes measurements from multiple parts of the aircraft, namely starboard, port, and nadir. To keep things simple, we will filter the DataFrame to only look at the nadir track (Track_Identifier = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a12af-309c-49a2-b86f-4a4b7aa8ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2 = atm_l2[atm_l2['Track_Identifier']==0]\n",
    "\n",
    "# Change the longitudes to be consistent with ICESat-2\n",
    "atm_l2['Longitude(deg)'] -= 360\n",
    "\n",
    "print(atm_l2.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ce392-f306-489b-8bc7-f1679a975d20",
   "metadata": {},
   "source": [
    "Let's take a quick look at where ATM is relative to ICESat-2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297da1cf-e878-4172-91b9-d35ef9095f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the ICESat-2 data to the ATM latitudes\n",
    "is2_gt2r = is2_gt2r[(is2_gt2r['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2r['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "is2_gt2l = is2_gt2l[(is2_gt2l['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2l['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "\n",
    "\n",
    "# Set up a map with the flight tracks as overlays\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles, Polyline\n",
    "\n",
    "m = Map(\n",
    "    basemap=basemap_to_tiles(basemaps.Esri.WorldImagery),\n",
    "    center=(69.25, 310.35-360),\n",
    "    zoom=10\n",
    ")\n",
    "\n",
    "gt2r_line = Polyline(\n",
    "    locations=[\n",
    "        [is2_gt2r['lat'].min(), is2_gt2r['lon'].max()],\n",
    "        [is2_gt2r['lat'].max(), is2_gt2r['lon'].min()]\n",
    "    ],\n",
    "    color=\"green\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(gt2r_line)\n",
    "\n",
    "gt2l_line = Polyline(\n",
    "    locations=[\n",
    "        [is2_gt2l['lat'].min(), is2_gt2l['lon'].max()],\n",
    "        [is2_gt2l['lat'].max(), is2_gt2l['lon'].min()]\n",
    "    ],\n",
    "    color=\"green\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(gt2l_line)\n",
    "\n",
    "atm_line = Polyline(\n",
    "    locations=[\n",
    "        [atm_l2['Latitude(deg)'].min(), atm_l2['Longitude(deg)'].max()],\n",
    "        [atm_l2['Latitude(deg)'].max(), atm_l2['Longitude(deg)'].min()]\n",
    "    ],\n",
    "    color=\"orange\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(atm_line)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4688c-7820-4ea8-bfeb-b89ff55fd6fa",
   "metadata": {},
   "source": [
    "Looks like ATM aligns very closely with the left beam (GT2L), so hopefully the two beams will agree. The terrain over this region is quite rough, so we may expect some differences between ATM and GT2R. ICESat-2 also flew over Jakobshavan 16 days before ATM, so there might be slight differences due to ice movement.\n",
    "\n",
    "We have looked at how we can quickly download ICESat-2 and airborne lidar data, and process them using Pandas. We will engage in a more thorough analysis in the Data Integration session tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd1289-0941-4e82-b1ce-4e3c771ee665",
   "metadata": {},
   "source": [
    "## 3.  Search and open (Landsat) images from the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b29274-2555-4c31-8fd0-f80c071afae9",
   "metadata": {},
   "source": [
    "Next we will show how we can open and manipulate cloud-based Landsat imagery (raster data) for analysis with ICESat-2 data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c5c58-d5d4-46d2-95be-53f3eb212878",
   "metadata": {},
   "source": [
    "What if you want to search for Landsat 8 data over an area of interest? Browsing through lists of files is cumbersome. Instead of downloading, we can use cloud-optimized approaches that require no downloading to search and access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e7977-d795-467c-9b10-8d13bda22c00",
   "metadata": {},
   "source": [
    "### Cloud optimized approaches\n",
    "* Organize data as an aggregation of small, independently retrievable objects (e.g., zarr, HDF2, Geotiff in the Cloud)\n",
    "* Allow access to pieces of large objects (e.g., Cloud-Optimized GeoTIFF3, OPeNDAP4 in the Cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d6cdc-869b-4912-a2b8-9834ac76c48f",
   "metadata": {},
   "source": [
    "We will be working with Cloud Optimized GeoTIFF (COG). A COG is a GeoTIFF file with an internal organization that enables more efficient workflows in the cloud environment.  It does this by leveraging the ability of clients issuing ​HTTP GET range requests to ask for just the parts of a file they need instead of having to open the entire image or data set (see more at https://www.cogeo.org/).\n",
    "\n",
    "An Amazon Web Services (AWS) account is required to directly access Landsat data in\n",
    "the cloud housed within the usgs-landsat S3 requester pays bucket. Landsat is stored in the AWS USWest data center, which is the same data center our Hub is located so egress of Landsat is free. Users can find all objects within the Landsat record by using either the Satellite (SAT) Application Programing Interface (API) or SpatioTemporal Asset Catalog\n",
    "(STAC) Browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92589138-1d61-4bda-a26f-4d55909745d3",
   "metadata": {},
   "source": [
    "This is an example url for accessing a band in the cloud from an S3 bucket:\\\n",
    "s3://usgs-landsat/collection02/level-2/standard/oli-tirs/2020/202/025/LC08_L1TP_202025_20190420_20190507_01_T1/LC08_L1TP_202025_20190420_20190507_01_T1_B4.TIF\n",
    "\n",
    "For more information about accessing Landsat S3, this is the User Manual: \\\n",
    "https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-2032-Landsat-Commercial-Cloud-Direct-Access-Users-Guide-v2.pdf.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05076b32-9f21-42ea-b4bc-de8f6fc98f65",
   "metadata": {},
   "source": [
    "### Search for Landsat imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1ce5f-7b4a-4935-b234-a4b65ac3c109",
   "metadata": {
    "tags": []
   },
   "source": [
    "To explore and access COG's easily we will use a SpatioTemporal Asset Catalog (STAC). The STAC specification provides a common, machine-readable (JSON) format for describing a wide range of geospatial datasets. STAC’s goal is to make it easier to index and discover any geospatial dataset that can be described by a spatial extent and time. STAC is the way geospatial asset metadata is structured and queried and it makes querrying S3 buckets easier. Learn more here: https://github.com/radiantearth/stac-spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0ca2b-8f04-4825-82e0-920ef67be0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up credentials for acquiring images through dask/xarray\n",
    "os.environ[\"AWS_REQUEST_PAYER\"] = \"requester\"\n",
    "\n",
    "# Sets up proper credentials for acquiring data through rasterio\n",
    "aws_session = AWSSession(boto3.Session(), requester_pays=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae1ee5-fa89-40ac-9daf-79ce5b84ad5a",
   "metadata": {},
   "source": [
    "Extract geometry bounds from the ICESat-2 KML file used above so that we can perform the Landsat spatial search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853034d-165c-4f7d-9ed0-4fcd31d05c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract geometry bounds\n",
    "geom = jk.geometry[0]\n",
    "print(geom.bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8462f9c-131a-46c2-8f97-7c7197010031",
   "metadata": {},
   "source": [
    "We will search for imagery in STAC catalog using satsearch: \n",
    "https://github.com/sat-utils/sat-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24141fa8-38eb-4ddb-a585-b8805c7cfd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search STAC API for Landsat images based on a bounding box, date and other metadata if desired\n",
    "\n",
    "bbox = (geom.bounds[0], geom.bounds[1], geom.bounds[2], geom.bounds[3]) #(west, south, east, north) \n",
    "\n",
    "timeRange = '2019-05-06/2019-05-07'\n",
    "    \n",
    "results = Search.search(url='https://ibhoyw8md9.execute-api.us-west-2.amazonaws.com/prod',\n",
    "                        collection='usgs-landsat/collection02/',\n",
    "                        datetime=timeRange,\n",
    "                        bbox=bbox,    \n",
    "                        # properties=properties,\n",
    "                        sort=['<datetime'])\n",
    "\n",
    "print('%s items' % results.found())\n",
    "items = results.items()\n",
    "\n",
    "# Save search to geojson file\n",
    "gjson_outfile = f'/home/jovyan/website2022/book/tutorials/DataIntegration/Landsat.geojson'\n",
    "items.save(gjson_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae7e6c-a0c8-4d86-8e7c-4e2cd7cc8afb",
   "metadata": {},
   "source": [
    "We can include property searches, such as path, row, cloud-cover, as well with the `properties` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22daee1c-297a-4a0f-8e19-6dd2ad326b28",
   "metadata": {},
   "source": [
    "We are given a satsearch collection of items (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f887b-8acd-4867-a675-100d9d2cdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e4165-59ff-4127-a91b-cf34f0286566",
   "metadata": {},
   "source": [
    "Load the geojson file into geopandas and inspect the items we want to collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geojson file\n",
    "gf = gpd.read_file(gjson_outfile)\n",
    "gf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dcf7c-e970-417d-ad4c-8e8c9540b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot search area of interest and frames on a map using Holoviz Libraries (more on these later)\n",
    "cols = gf.loc[:,('id','landsat:wrs_path','landsat:wrs_row','geometry')]\n",
    "footprints = cols.hvplot(geo=True, line_color='k', hover_cols=['landsat:wrs_path','landsat:wrs_row'], alpha=0.2, title='Landsat 8 T1',tiles='ESRI')\n",
    "tiles = gv.tile_sources.CartoEco.options(width=700, height=500) \n",
    "labels = gv.tile_sources.StamenLabels.options(level='annotation')\n",
    "tiles * footprints * labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a6ed8-2743-45a6-82ed-eea895349d17",
   "metadata": {},
   "source": [
    "### Intake all scenes using the intake-STAC library\n",
    "Intake-STAC facilitates discovering, exploring, and loading spatio-temporal datasets.\n",
    "\n",
    "Intake-STAC provides Intake Drivers for SpatioTemporal Asset Catalogs (STAC). This provides a simple toolkit for working with STAC catalogs and for loading STAC assets as Xarray objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f38d9-f8cb-4e84-89d1-6428702f9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_stac_item_collection(items)\n",
    "list(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0991aa5-35bb-4c45-9899-87a127486cc0",
   "metadata": {},
   "source": [
    "Let's explore the metadata and keys for the first scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42faf5-cccf-47ff-9bf8-f70ebd78f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sceneids = list(catalog)\n",
    "item3 = catalog[sceneids[3]]\n",
    "item3.metadata\n",
    "# for keys in item0.keys():\n",
    "#     print (keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a14425-8c85-482e-b6b6-ea0b246db046",
   "metadata": {},
   "source": [
    "We can explore the metadata for any of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957c5e3-ec7b-4b51-a6fc-0fdc8d1c0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item3['blue'].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb00dde-fc04-483d-b5ec-2e2e561af774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url needed to grab data from the S3 bucket\n",
    "# From the satsearch catalog:\n",
    "items[3].asset('blue')['alternate']['s3']['href'] # can use item asset name or title (blue or B2)\n",
    "\n",
    "# From the intake-STAC catalog\n",
    "item3.blue.metadata['alternate']['s3']['href'] # must use item asset name (blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d64cb1-76ce-4cec-884f-087c514ccbef",
   "metadata": {},
   "source": [
    "### Open and visualize each image using RasterIO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a3220-bf2b-4034-99e3-4ff3c2888ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n",
    "\n",
    "# Retrieve first scene using rio\n",
    "item_url = item3.blue.metadata['alternate']['s3']['href']\n",
    "\n",
    "# Read and plot with grid coordinates \n",
    "with rio.Env(aws_session):\n",
    "    with rio.open(item_url) as src:\n",
    "        fig, ax = plt.subplots(figsize=(9,8))\n",
    "        \n",
    "        # To plot\n",
    "        show(src,1)\n",
    "        \n",
    "        # To open data into a numpy array\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f75d6-13f3-453e-84f4-7fb0dc787150",
   "metadata": {},
   "source": [
    "We can open directly into xarray using RasterIO..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913afed8-4113-47c7-bab1-5b45e11d8692",
   "metadata": {},
   "source": [
    "### Manipulating data in Xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32afc99-538f-48ba-b2a5-37870fb75b9e",
   "metadata": {},
   "source": [
    "Pandas and Xarray have very similar structures and ways of manipulating data, but Pandas excels with 2-D data and Xarray is ideal for higher dimension data. Xarray introduces labels in the form of dimensions, coordinates and attributes on top of Pandas-like DataFrames.\n",
    "\n",
    "Xarray has 2 fundamental data structures:\n",
    "\n",
    "* `DataArray`, which holds single multi-dimensional variables and its coordinates\n",
    "* `Dataset`, which holds multiple variables that potentially share the same coordinates\n",
    "\n",
    "<img src=\"https://xarray-contrib.github.io/xarray-tutorial/_images/xarray-data-structures.png\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37b4de-4f47-4f3b-a7e1-528c7d197017",
   "metadata": {},
   "source": [
    "Xarray doesn’t just keep track of labels on arrays – it uses them to provide a powerful and concise interface. We will only scratch the surface here on what Xarray can do. To learn more, there are great Xarray tutorials here: https://xarray-contrib.github.io/xarray-tutorial/online-tutorial-series/01_xarray_fundamentals.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140eaec-ac8e-4469-a217-00842c350666",
   "metadata": {},
   "source": [
    "We can use RasterIO to easily open into an Xarray `DataArray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496924ac-5bff-4ee2-9122-46a6bb61b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "rastxr = xr.open_dataarray(item_url,engine='rasterio')\n",
    "rastxr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811fd61-56c5-4b02-92b1-1babb8732d42",
   "metadata": {},
   "source": [
    "Or a `DataSet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797be23-d755-4dd0-972e-1d8ab6426647",
   "metadata": {},
   "outputs": [],
   "source": [
    "rastxr = xr.open_dataset(item_url,engine='rasterio')\n",
    "rastxr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e566c91-e3e5-46a0-af33-080ed0c2e4b3",
   "metadata": {},
   "source": [
    "We can open using rioxarray, which integrates RasterIO and Xarray and is the most efficient way of opening using RasterIO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2d221-9ab8-48bb-a898-6130d12d8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "\n",
    "rastrxr = rxr.open_rasterio(item_url)\n",
    "rastrxr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d93829-e9a5-4d83-a55a-a28e2afccd35",
   "metadata": {},
   "source": [
    "We can see `Attributes` have been added to the Xarray using the same url.\n",
    "\n",
    "Beyond what Xarray and Rasterio provide, Rioxarray has these added benefits (plus others):\n",
    "* It supports multidimensional datasets such as netCDF.\n",
    "* It loads in the CRS, transform, and nodata metadata in standard CF & GDAL locations.\n",
    "* It supports masking and scaling data with the masked and mask_and_scale kwargs.\n",
    "* It loads raster metadata into the attributes.\n",
    "\n",
    "For more info: https://corteva.github.io/rioxarray/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee58074-466e-4933-8c35-330ac2030aa9",
   "metadata": {},
   "source": [
    "Another convenient means for opening a lot of raster data into Xarray is using Dask. Xarray integrates with Dask to support parallel computations and streaming computation on datasets that don’t fit into memory. So this is perfect when you want to process a lot of data. \n",
    "\n",
    "Dask divides arrays into many small pieces, called chunks, each of which is presumed to be small enough to fit into memory.\n",
    "\n",
    "Unlike NumPy, which has eager evaluation, operations on Dask arrays are lazy. Operations queue up a series of tasks mapped over blocks, and no computation is performed until you actually ask values to be computed (e.g., to print results to your screen or write to disk). At that point, data is loaded into memory and computation proceeds in a streaming fashion, block-by-block.\n",
    "\n",
    "More on Dask in the Cloud Computing Tools tutorial.\n",
    "\n",
    "To expand our Xarray toolbox for working with larger data sets that we don't necessarily want entirely in memory, we will start by reading in 3 bands of a Landsat scene to Xarray using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1c642-9fe6-4381-899f-84972a432237",
   "metadata": {},
   "outputs": [],
   "source": [
    "sceneid = catalog[sceneids[0]]\n",
    "print (sceneid.name)\n",
    "\n",
    "band_names = ['red','green','blue']\n",
    "\n",
    "bands = []\n",
    "\n",
    "# Construct xarray for scene\n",
    "for band_name in band_names:\n",
    "    # Specify chunk size (x,y), Landsat COG is natively in 512 chunks so is best to use this or a multiple\n",
    "    band = sceneid[band_name](chunks=dict(band=1, x=2048, y=2048),urlpath=sceneid[band_name].metadata['alternate']['s3']['href']).to_dask()\n",
    "    band['band'] = [band_name]\n",
    "    bands.append(band)\n",
    "scene = xr.concat(bands, dim='band')\n",
    "scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3876e4f-4a64-498d-a6a0-47654bb3bb5b",
   "metadata": {},
   "source": [
    "We can choose not to specify chunk sizes and have everything read as one chunk. Though, when we load larger sets of imagery, like this one, we can change these chunk sizes to use dask. Typically, it’s best to align dask chunks with the way image chunks (typically called “tiles”) are stored on disk or cloud storage buckets. The landsat data is stored on AWS S3 in a tiled Geotiff format where tiles are 512x512, so we should pick some multiple of that, and typically aim for chunk sizes of ~100Mb (although this is subjective). You can read more about dask chunks here: https://docs.dask.org/en/latest/array-best-practices.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c580e73-d076-47ac-9593-e995e46f84e6",
   "metadata": {},
   "source": [
    "Similarly as with pandas, we can explore variables easily. This time we can work with coordinates (equivalent to indices in pandas). Here x might often be the longitude (it can be renamed to this actually):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dc6f5-150c-410d-bc69-2cfd071cc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445764f1-155b-4cf8-b353-6e029a283627",
   "metadata": {},
   "source": [
    "We can also keep track of arbitrary metadata (called attributes) in the form of a Python dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a99544-9ca6-4f6c-aa59-dcc5ee29b5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scene.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4549dcd-7346-4949-9221-5d13b346d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e1be2-f638-4913-b35f-93c76827a2d2",
   "metadata": {},
   "source": [
    "We can apply operations over dimensions by name. Here, if we want to slice the data to only have the blue band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382c443-237e-4d89-af5c-730290e26817",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.sel(band='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba2eb2-3e6c-45d2-93bc-d1c719c0356a",
   "metadata": {},
   "source": [
    "Notice that instead of loc (from Pandas) we are using sel, but they funtion synonymously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9371ce-bc08-4b9b-9efe-ba5c6db5f6be",
   "metadata": {},
   "source": [
    "Mathematical operations (e.g., x - y) vectorize across multiple dimensions (array broadcasting) based on dimension names. Let's determine the mean reflectance for the blue band:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d5c55-ea57-491b-882c-9f9c31d5de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.sel(band='blue').mean()#.values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4407ba-4fc2-4f09-aea4-f670fa8fb8f8",
   "metadata": {},
   "source": [
    "And you can easily use the split-apply-combine paradigm with groupby, which I won't show here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d5255-322f-42f3-a085-3a22520bb377",
   "metadata": {},
   "source": [
    "The N-dimensional nature of xarray’s data structures makes it suitable for dealing with multi-dimensional scientific data, and its use of dimension names instead of axis labels (dim='time' instead of axis=0) makes such arrays much more manageable than the raw numpy ndarray or pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe58fc9-2fb3-4512-bbf1-356b19565596",
   "metadata": {},
   "source": [
    "Now let's open all the bands and multiple days together into an xarray. This will be a more complex xarray with an extra `'time'` dimension. Since the catalog we have has a combination of Level 1 and 2 data, let's keep only the scene IDs for Level 1 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae28dc-51f2-4041-ad5a-16f29437467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sceneids = list(catalog)\n",
    "sceneids = [sceneid for sceneid in sceneids if sceneid.endswith('_T1')]\n",
    "sceneids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6a08f-260c-4a6c-a841-9e11b52a3fa0",
   "metadata": {},
   "source": [
    "Let's create the time variable first for the xarray time dimension. This finds the desired scene IDs in the geopandas dataframe we have above and extracts their 'datetime' information. These datetimes get recorded into an Xarray variable object for 'time'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92652324-3c6d-42e7-b531-f8e08df343d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time variable for time dim\n",
    "time_var = xr.Variable('time',gf.loc[gf.id.isin(sceneids)]['datetime'])\n",
    "time_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de262fa6-7c1e-4d19-95d5-894a8b89ecb2",
   "metadata": {},
   "source": [
    "Now we will search and collect band names for grabbing each desired band. We will just grab the bands that have 30 m pixels. This provides and example of how you can search data in the STAC catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f5945-62c0-4360-8930-9c2c24f4c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_names = []\n",
    "\n",
    "# Get band names\n",
    "sceneid = catalog[sceneids[1]]\n",
    "for k in sceneid.keys():\n",
    "    M = getattr(sceneid, k).metadata\n",
    "    if 'eo:bands' in M:\n",
    "        resol = M['eo:bands'][0]['gsd']\n",
    "        print(k, resol)\n",
    "        if resol == 30: \n",
    "            band_names.append(k)\n",
    "            \n",
    "# Add qa band\n",
    "band_names.append('qa_pixel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df7e3a-2bf5-4e07-a157-aa7bc2565224",
   "metadata": {},
   "source": [
    "And now open all of it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18488506-2397-4a14-a732-a785cd279f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to xarray\n",
    "# In xarray dataframe nans are in locations where concat of multiple scenes has expanded the grid (i.e. different path/rows).\n",
    "scenes = []\n",
    "\n",
    "for sceneid in sceneids:\n",
    "    sceneid = catalog[sceneid]\n",
    "    print (sceneid.name)\n",
    "\n",
    "    bands = []\n",
    "\n",
    "    # Construct xarray for scene, open each band, append and concatenate together to create a scene, \n",
    "    # then append and concatenate each scene to create the full dataframe \n",
    "    for band_name in band_names:\n",
    "        band = sceneid[band_name](chunks=dict(band=1, x=2048, y=2048),urlpath=sceneid[band_name].metadata['alternate']['s3']['href']).to_dask()\n",
    "        band['band'] = [band_name]\n",
    "        bands.append(band)\n",
    "    scene = xr.concat(bands, dim='band')\n",
    "    scenes.append(scene)\n",
    "\n",
    "# Concatenate scenes with time variable\n",
    "ls_scenes = xr.concat(scenes, dim=time_var)\n",
    "\n",
    "ls_scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b592a6-e14e-4ef2-b7a9-016c719788f7",
   "metadata": {},
   "source": [
    "We now have 2 Landsat scenes with all of the bands we are interested in stored in an xarray. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab467d-8b99-445a-b6de-feb236423c0c",
   "metadata": {},
   "source": [
    "We can also easily subset and visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71507c0f-b8a9-48ad-bf17-e3ed84dbd0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbands = ['blue', 'nir08', 'swir16']\n",
    "\n",
    "# Select the first datetime\n",
    "t = ls_scenes.time.values[1]\n",
    "print (t)\n",
    "\n",
    "# Create a upper left and lower right coordinates for subsetting \n",
    "ulx = 300000\n",
    "uly = 7695000\n",
    "lrx = 330000\n",
    "lry = 7670000\n",
    "\n",
    "image = ls_scenes.sel(time=t,band=sbands,y=slice(lry,uly),x=slice(ulx,lrx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a74e5-eb70-4fe5-b153-b827ca4e434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "image.sel(band='blue').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f36f2-e123-489f-a4ca-ed1e44292161",
   "metadata": {},
   "source": [
    "Since this data is in UTM 22N, we can reproject to the standard lat/lon coordinate system (WGS-84) and map with the ICESat-2 and ATM lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fcc0dc-8538-4aff-b186-6bb7140529cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.rio.reproject(4326)\n",
    "\n",
    "ISlats = [is2_gt2r['lat'].min(), is2_gt2r['lat'].max()]\n",
    "# ISlons = (is2_gt2r['lon'].max(), is2_gt2r['lon'].min())\n",
    "ISlons = [-55.624,-55.646]\n",
    "\n",
    "ATMlats = [atm_l2['Latitude(deg)'].min(), atm_l2['Latitude(deg)'].max()]\n",
    "# ATMlons = [atm_l2['Longitude(deg)'].max(), atm_l2['Longitude(deg)'].min()]\n",
    "ATMlons = [-55.624,-55.646]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "image.sel(band='blue').plot()\n",
    "plt.plot(ISlons,ISlats,color = 'green')\n",
    "plt.plot(ATMlons,ATMlats,color = 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5cab5-c4e6-46d7-ae86-ea01b522fd35",
   "metadata": {},
   "source": [
    "The reprojection to WGS-84 didn't calculated the longitudes to be 6 degrees off, so we shifted the IS2 and ATM data for ease of visualization. This issue only seems to arise with reprojections from some UTM projections and should not be an issue with most data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-amateur",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed the tutorial. In this tutorial you have gained the skills to: \n",
    "* Search for non-ICESat-2 datasets\n",
    "* Open data into pandas and xarray dataframes/arrays, and \n",
    "* Manipulate, visualize, and explore the data\n",
    "\n",
    "We have concluded by mapping the three data sets together. More to come in the second data integration tutorial tomorrow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
