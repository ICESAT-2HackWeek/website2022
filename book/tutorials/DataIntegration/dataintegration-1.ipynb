{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suffering-union",
   "metadata": {},
   "source": [
    "# Data integration with ICESat-2 - Part I\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "**Goals**\n",
    "- Identify and locate non-ICESat-2 data sets\n",
    "- Acquiring data from the cloud or via download\n",
    "- Open data in Pandas and Xarray and basic functioning of DataFrames\n",
    "```\n",
    "☝️ This formatting is a Jupyter Book [admonition](https://jupyterbook.org/content/content-blocks.html#notes-warnings-and-other-admonitions), that uses a custom version of Markdown called {term}`MyST`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d55f37-40b8-453d-90a6-2a45b22482a0",
   "metadata": {},
   "source": [
    "## Needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6cf3e-4735-4ef9-87be-50f5bc641cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sat-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b292d-9c40-4381-857d-ab36da55d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdebc7-660e-47cd-8cf0-521a9557b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install intake-stac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d863c-5ad1-4b9f-9fa0-416ff04396a4",
   "metadata": {},
   "source": [
    "## Computing environment\n",
    "\n",
    "We'll be using the following open source Python libraries in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49392ebb-e958-4e9c-87c5-b883430d3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyleaflet\n",
    "from ipyleaflet import Map, GeoData, LayersControl,Rectangle, basemaps, basemap_to_tiles, TileLayer, SplitMapControl, Polygon\n",
    "\n",
    "import ipywidgets\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5720d5-d805-4c24-a92e-a2238805cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import satsearch\n",
    "from satsearch import Search\n",
    "import geopandas as gpd\n",
    "import ast\n",
    "import pandas as pd\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display, Image\n",
    "import intake # if you've installed intake-STAC, it will automatically import alongside intake\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import rasterio as rio\n",
    "from rasterio.session import AWSSession\n",
    "from rasterio.plot import show\n",
    "from dask.utils import SerializableLock\n",
    "import os\n",
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "# Suppress library deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99d916-deb8-4af0-9fce-1e38e5c18d87",
   "metadata": {},
   "source": [
    "## Identify and acquire the ICESat2 product(s) of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d66e60-1c1d-4887-b275-bd92deffae73",
   "metadata": {},
   "source": [
    "* What is the application of this product?\n",
    "* What region and resolution is needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095b2a6-2ab5-440e-a3b8-620cf5c1868a",
   "metadata": {},
   "source": [
    "#### Download ICESat-2 ATL03 data from desired region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b2612c-9808-4b32-94f5-6cd007fbe4e4",
   "metadata": {},
   "source": [
    "Remember icepyx? We are going to use that again to download some ICESat-2 ATL06 data over our region of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd8ca2-fa0b-4fb6-b03f-19d8419d3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import icepyx as ipx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd1a4d6-69ee-4687-905b-c384e55d6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the necessary icepyx parameters\n",
    "short_name = 'ATL06'\n",
    "spatial_extent = 'hackweek_kml_jakobshavan.kml' # KML polygon centered on Jakobshavan\n",
    "date_range = ['2019-04-01', '2019-04-30']\n",
    "rgts = ['338'] # IS-2 RGT of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a9398-11bc-4a25-a19e-137746429896",
   "metadata": {},
   "source": [
    "You may notice that we specified a RGT track. As seen below, a large number of ICESat-2 overpasses occur for Jakobshavan. In the interest of time (and computer memory), we are going to look at only one of these tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d389e8f-84a8-41e6-8c43-e56c5f126877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image of area of interest (data viz tutorial will get in deeper so don't explain much):\n",
    "center = [69.2, -50]\n",
    "zoom = 7\n",
    "\n",
    "# Open KML file for visualizing\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\n",
    "jk = gpd.read_file(spatial_extent, driver='KML')\n",
    "\n",
    "m = Map(basemap=basemap_to_tiles(basemaps.NASAGIBS.ModisAquaTrueColorCR, '2020-07-18'),center=center,zoom=zoom)\n",
    "geo_data = GeoData(geo_dataframe = jk)\n",
    "\n",
    "m.add_layer(geo_data)\n",
    "m.add_control(LayersControl())\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2c02c-a607-4937-a89a-c0db822ab4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Query object\n",
    "region = ipx.Query(short_name, spatial_extent, date_range, tracks=rgts)\n",
    "\n",
    "# Show the available granules\n",
    "region.avail_granules(ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e44e93-9c3a-41b3-bdf4-433fae6f554a",
   "metadata": {},
   "source": [
    "Looks like we have an ICESat-2 track! Let's quickly visualize the data to ensure that there are no clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45957b30-4aab-4191-be11-7bfaaa0e2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request information from OpenAltimetry\n",
    "cyclemap, rgtmap = region.visualize_elevation()\n",
    "\n",
    "rgtmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55b081-09e8-40bc-9306-7bee59725498",
   "metadata": {},
   "source": [
    "Looks good! Now it's time to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353d1a2-e7b5-4803-a59a-9b793c23e2ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Earthdata credentials\n",
    "uid = 'zhfair'\n",
    "email = 'zhfair@umich.edu'\n",
    "region.earthdata_login(uid, email)\n",
    "\n",
    "# Order the granules\n",
    "region.order_granules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218b130-aa73-4c62-821e-2bbf456ad064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "path = '/home/jovyan/website2022/book/tutorials/DataIntegration/'\n",
    "region.download_granules(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c92508-d237-4203-84c5-fccc31ecc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Load the ICESat-2 data. We will just look at the central beams (GT2R/L)\n",
    "is2_file = 'processed_ATL06_20190420093051_03380303_005_01_full.h5'\n",
    "with h5py.File(is2_file, 'r') as f:\n",
    "    is2_gt2r = pd.DataFrame(data={'lat': f['gt2r/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2r/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2r/land_ice_segments/h_li'][:]})\n",
    "    is2_gt2l = pd.DataFrame(data={'lat': f['gt2l/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2l/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2l/land_ice_segments/h_li'][:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778b651-2079-4aa4-b9e9-bd4f460c3ff8",
   "metadata": {},
   "source": [
    "## Identify other products of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30999772-d615-4009-a34e-91ae7c5b5627",
   "metadata": {},
   "source": [
    "1) What research problems have you wanted to address that require more than one dataset?\n",
    "\n",
    "2) What datasets have you been interested in using with IS2 to expand its use?\n",
    "    * Satellite or in situ data sets?\n",
    "\n",
    "\n",
    "3) What are some datasets you’ve wished you could use, but have had difficulty finding? Why? \n",
    "\n",
    "We will show ATM (non-AWS) and Landsat (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8793d3-1abf-4ecd-ae0e-7c948c0527d5",
   "metadata": {},
   "source": [
    "### Where are the other data sets stored?\n",
    "\n",
    "* Cloud datasets in AWS \n",
    "   https://registry.opendata.aws/ \n",
    "* NASA EarthData\n",
    "   https://search.earthdata.nasa.gov/search/\n",
    "* ESA Copernicus Hub\n",
    "   https://scihub.copernicus.eu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee3bdf-4c2b-47e7-9840-700cac1e9091",
   "metadata": {},
   "source": [
    "ATM: https://search.earthdata.nasa.gov/search/granules?p=C1000000062-NSIDC_ECS&pg[0][v]=f&pg[0][gsk]=-start_date&sb[0]=-50.56835%2C68.83371%2C-49.00139%2C69.56591&fi=ATM&as[instrument][0]=ATM&tl=1646767454.667!3!!&m=67.08956509568404!-48.11572265625!6!1!0!0%2C2 \\\n",
    "Sentinel-2: https://registry.opendata.aws/sentinel-2/ \\\n",
    "Landsat: https://registry.opendata.aws/usgs-landsat/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc23b5-b0a0-4feb-acce-25b8980f79a3",
   "metadata": {},
   "source": [
    "## Acquire non-cloud data and open: ATM data access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ff2e2-3ad8-4014-8ebd-2413ffb5a468",
   "metadata": {},
   "source": [
    "Why did we choose April 2019 and RGT 338? In Spring 2019, an airborne campaign called Operation IceBridge was flown across Jakobshavan as validation for ICESat-2. Onboard was the Airborne Topographic Mapper, a lidar that works at both 532 nm (like ICESat-2) and 1064 nm (near-infrared). More information about Operation IceBridge and ATM may be found here: https://nsidc.org/data/icebridge\n",
    "\n",
    "Here, we are going to try and co-register ATM spot measurements with ICESat-2. Because both data sets are rather large, this can be computationally expensive, so we will only consider one flight track with the ATM 532 nm beam.\n",
    "\n",
    "Operation IceBridge data is not available on the cloud, so this data was downloaded directly from NSIDC. If you are interested in using IceBridge data, NSIDC has a useful data portal here: https://nsidc.org/icebridge/portal/map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37608a-69c9-408b-9e8e-781edd93a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ATM data into a DataFrame\n",
    "atm_file = 'ILATM2_20190506_151600_smooth_nadir3seg_50pt.csv'\n",
    "atm_l2 = pd.read_csv(atm_file)\n",
    "\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec8435-262f-460b-ba30-e46feeb5cdb3",
   "metadata": {},
   "source": [
    "### Opening and manipulating data in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab7dbb-ad2a-4f23-b883-5ddc376ca131",
   "metadata": {},
   "source": [
    "Pandas excels at helping you explore, clean, and process tabular data, such as data stored in spreadsheets or databases. In pandas, a Series is a 1-D data table and the 2-D data table is called a DataFrame, which we saw just above.\n",
    "\n",
    "Read csv is the easiest way to open a csv data file into a pandas DataFrame. We can specify formating, data selection, indexing and much more when reading any data into a pandas DataFrame. Here I specify different headers, assign the first column as the index, and the first column as the header (even though we are renaming it, so that it doesn't get read in with data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ee715-1307-4520-8092-cb1344aeda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with specific headers\n",
    "headers = ['UTC', 'Lat', 'Lon',\n",
    "       'Height', 'South-to-North_Slope',\n",
    "       'West-to-East_Slope', 'RMS_Fit', 'Number_Measurements',\n",
    "       'Number_Measurements_Removed',\n",
    "       'Distance_Of_Block_To_The_Right_Of_Aircraft', 'Track_Identifier']\n",
    "atm_l2 = pd.read_csv(atm_file,names=headers,index_col=0,header=0)\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e62c0-c684-4607-bd35-4d8d73c9e482",
   "metadata": {},
   "source": [
    "Now we can explore the data and DataFrame functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878d354-6a04-464d-8073-7fc19032c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ada8f-28e7-4142-8f26-b987555bb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['Lat'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d061be-cbde-4d8a-871d-640454498b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.Lat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ba867-452c-484e-995a-3cb61f68db15",
   "metadata": {},
   "source": [
    "If we want something more intuitive as an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88611022-c007-450a-94de-6793349a12fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add datetime for index\n",
    "atm_l2 = atm_l2.reset_index()\n",
    "date = pd.to_datetime(atm_file[7:15])\n",
    "time = pd.to_timedelta(atm_l2.UTC, unit='s')\n",
    "atm_l2['DateTime'] = date + time\n",
    "atm_l2 = atm_l2.set_index('DateTime')\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06f88e-d7f3-46c3-a0a1-24e6e3af5993",
   "metadata": {},
   "source": [
    "Now we can easily slice data by date, month, day, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60247b-f5f3-4d85-877f-32c0b55b6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['2019-05-06'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2232f-c4a2-4942-b5df-ea1addc936fb",
   "metadata": {},
   "source": [
    "Or by a range of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437bbc1-9ee8-4747-8f28-c65c52e0f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['2019-05-06':'2019-05-07']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58c7e1-0bbf-4600-aa3d-89769145382e",
   "metadata": {},
   "source": [
    "#### Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a719fd6-d631-493b-b0d9-e870d8236364",
   "metadata": {},
   "source": [
    "Two methods for slicing:\n",
    "\n",
    "* .loc for label-based indexing\n",
    "* .iloc for positional indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9b6b-cc6e-45a0-860d-c9c7c5efa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use loc to slice specific indices\n",
    "atm_l2.loc[['2019-05-06 15:16:09.500', '2019-05-06 15:16:09.750']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d4c71-8dc3-4b2f-a3a7-7a05d81042a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select indices and columns\n",
    "atm_l2.loc[['2019-05-06 15:16:09.500', '2019-05-06 15:16:09.750'], ['Lat', 'Lon', 'Height']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538551c-689f-4506-8839-017a2b3459bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use iloc to select by row and column number\n",
    "atm_l2.iloc[[0,4],[0,1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478453b-0d95-41ab-a5ca-ae0f111c6f59",
   "metadata": {},
   "source": [
    "#### Statistical manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a429cc14-7e2b-4d37-944b-2956efca8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae523d-daf2-46de-9b15-36ae7de6791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2['Height'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc1908-f0cb-4856-bca5-4c7bf4a77fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rows together by a column and take the mean of each group\n",
    "atm_l2.groupby('Track_Identifier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef557f6b-e978-4196-8ed1-a2fe064bd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.resample('30S').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e5878-e444-4891-b290-7626275ee002",
   "metadata": {},
   "source": [
    "One alternative to using a loop to iterate over a DataFrame is to use the pandas __apply()__ method. This function acts as a map() function in Python. It takes a function as an input and applies this function to an entire DataFrame.If you are working with tabular data, you must specify an axis you want your function to act on (0 for columns; and 1 for rows). Much like the map() function, the apply() method can also be used with anonymous functions or lambda functions. Let's look at some apply() examples using baseball data.\n",
    "\n",
    "First, you will call the .apply() method on the atm_l2 dataframe. Then use the lambda function to iterate over the rows of the dataframe. For every row, we grab the Number_Measurements and Number_Measurements_Removed columns and pass them to the calc_total function. Finally, you will specify the axis=1 to tell the .apply() method that we want to apply it on the rows instead of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82ab1a-1d9d-40bd-8853-be51a5a3aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_total(a,b):\n",
    "    total = a + b\n",
    "    return total\n",
    "\n",
    "atm_l2['Total_Measurements'] = atm_l2.apply(lambda row: calc_total(row['Number_Measurements'], row['Number_Measurements_Removed']), axis=1)\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d66bfe-d1f5-47bf-b286-f583f2af560d",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31814149-471d-438e-a3cb-06f587e2ab1c",
   "metadata": {},
   "source": [
    "Try making apply work with this new function to create a new column, 'Distance_to_Jakobshavn', by using the Lat and Lon as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61273a29-9d07-4d2e-8272-aad6851e22ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "def distance(a,b):\n",
    "    '''\n",
    "    Calculate distance between a set point and a lat lon pair\n",
    "    a = lat\n",
    "    b = lon\n",
    "    '''\n",
    "    \n",
    "    jlat,jlon = 69.2330, -49.2434\n",
    "    \n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(jlat)\n",
    "    lon1 = radians(jlon)\n",
    "    lat2 = radians(a)\n",
    "    lon2 = radians(b)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "atm_l2['Distance_to_Jakobshavn'] \n",
    "# = atm_l2.apply(lambda row: calc_distance(row['Lat'], row['Lon']), axis=1)\n",
    "# atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c5439-b443-4ab0-bcb0-68811f4d12d6",
   "metadata": {},
   "source": [
    "Reset index to start fresh for multi-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e7d385-7b9d-4a88-9ebf-84e96711508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2 = atm_l2.reset_index()\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c7cfc-d241-49ca-ac2e-097916898711",
   "metadata": {},
   "source": [
    "#### Multi-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1facdde2-2694-450d-aed5-64c325c155b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2 = atm_l2.set_index(['DateTime','Track_Identifier']).sort_index()\n",
    "atm_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb491ac-8c5a-4561-a34a-926653255483",
   "metadata": {
    "tags": []
   },
   "source": [
    "To clear up some terminology, the levels of a MultiIndex are the former column names (UTC, Track_Identifier). The labels are the actual values in a level, (54969.50, 0-?, ...). Levels can be referred to by name or position, with 0 being the outermost level.\n",
    "\n",
    "Slicing the outermost index level is pretty easy, we just use our regular .loc[row_indexer, column_indexer]. We'll select the columns Lat and Long where the UTC was ---."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e93f75-5b55-4220-9e70-761c7451f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.loc[['2019-05-06 15:16:09.500','2019-05-06 15:16:09.750'],['Lat','Lon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864dfa9-af81-4916-8596-3387f1d060f4",
   "metadata": {},
   "source": [
    "If you wanted to select the rows whose track was 0 or 1, .loc wants [row_indexer, column_indexer] so let's wrap the two elements of our row indexer (the list of UTCs and the Tracks) in a tuple to make it a single unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc2e79-8ad6-405b-bf84-5a63d8e467db",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.loc[(['2019-05-06 15:16:09.500','2019-05-06 15:16:09.750'],[0,1]),['Lat','Lon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e280f-2324-44a2-9d26-cda88b787611",
   "metadata": {},
   "source": [
    "Now you want info from any UTC but for the specific tracks again (0,1). Below the : indicates all labels in this level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1574fd-5ac8-459e-87b9-851fdc8c8c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2.loc[pd.IndexSlice[:,[0,1]], ['Lat','Lon']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a7686-5ae6-472c-951e-87ed0ee2dadc",
   "metadata": {},
   "source": [
    "Many attributes and functions to explore with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4c68e-6710-42ef-b742-bcb4d8bd2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f3368-2574-41f8-aa58-14900138bdf9",
   "metadata": {},
   "source": [
    "## Co-register ICESat-2 with ATM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b3bc0-96f6-4f91-8d74-485ebe5840e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ATM data into a DataFrame\n",
    "atm_file = 'ILATM2_20190506_151600_smooth_nadir3seg_50pt.csv'\n",
    "atm_l2 = pd.read_csv(atm_file)\n",
    "\n",
    "atm_l2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af2d29-c176-4085-807f-edb9a76a712a",
   "metadata": {},
   "source": [
    "The ATM L2 file contains plenty of information, including surface height estimates and slope of the local topography. It also contains a track identifier - ATM takes measurements from multiple parts of the aircraft, namely starboard, port, and nadir. To keep things simple, we will filter the DataFrame to only look at the nadir track (Track_Identifier = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "atm_l2 = atm_l2[atm_l2['Track_Identifier']==0]\n",
    "\n",
    "# Change the longitudes to be consistent with ICESat-2\n",
    "atm_l2['Longitude(deg)'] -= 360\n",
    "\n",
    "print(atm_l2.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a21244-f406-472b-9143-28b8b6fd901e",
   "metadata": {},
   "source": [
    "Let's take a quick look at where ATM is relative to ICESat-2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3db3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the ICESat-2 data to the ATM latitudes\n",
    "is2_gt2r = is2_gt2r[(is2_gt2r['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2r['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "is2_gt2l = is2_gt2l[(is2_gt2l['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2l['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "\n",
    "\n",
    "# Set up a map with the flight tracks as overlays\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles, Polyline\n",
    "\n",
    "m = Map(\n",
    "    basemap=basemap_to_tiles(basemaps.Esri.WorldImagery),\n",
    "    center=(69.25, 310.35-360),\n",
    "    zoom=10\n",
    ")\n",
    "\n",
    "gt2r_line = Polyline(\n",
    "    locations=[\n",
    "        [is2_gt2r['lat'].min(), is2_gt2r['lon'].max()],\n",
    "        [is2_gt2r['lat'].max(), is2_gt2r['lon'].min()]\n",
    "    ],\n",
    "    color=\"green\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(gt2r_line)\n",
    "\n",
    "gt2l_line = Polyline(\n",
    "    locations=[\n",
    "        [is2_gt2l['lat'].min(), is2_gt2l['lon'].max()],\n",
    "        [is2_gt2l['lat'].max(), is2_gt2l['lon'].min()]\n",
    "    ],\n",
    "    color=\"green\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(gt2l_line)\n",
    "\n",
    "atm_line = Polyline(\n",
    "    locations=[\n",
    "        [atm_l2['Latitude(deg)'].min(), atm_l2['Longitude(deg)'].max()],\n",
    "        [atm_l2['Latitude(deg)'].max(), atm_l2['Longitude(deg)'].min()]\n",
    "    ],\n",
    "    color=\"orange\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(atm_line)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac64c19-e757-46a0-9a0b-4e779a17bc16",
   "metadata": {},
   "source": [
    "Looks like ATM aligns very closely with the left beam (GT2L), so hopefully the two beams will agree. The terrain over this region is quite rough, so we may expect some differences between ATM and GT2R. ICESat-2 also flew over Jakobshavan 16 days before ATM, so there might be slight differences due to ice movement.\n",
    "\n",
    "Let's take a look at how the data sets compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd13bbb8-0b6d-41d4-8cec-904241a95f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the IS-2 data to the max/min latitude of ATM\n",
    "#is2_gt2r = is2_gt2r[(is2_gt2r['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2r['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "#is2_gt2l = is2_gt2l[(is2_gt2l['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2l['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "\n",
    "print(is2_gt2r.size)\n",
    "print(is2_gt2l.size)\n",
    "\n",
    "plt.plot(is2_gt2r['lat'], is2_gt2r['elev'], label='gt2r')\n",
    "plt.plot(is2_gt2l['lat'], is2_gt2l['elev'], label='gt2l')\n",
    "plt.plot(atm_l2['Latitude(deg)'], atm_l2['WGS84_Ellipsoid_Height(m)'], label='atm')\n",
    "plt.xlabel('latitude')\n",
    "plt.ylabel('elevation [m]')\n",
    "plt.xlim([69.185, 69.275])\n",
    "plt.ylim([100, 550])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ce96c-090a-46f6-a0b8-ffeede8d1036",
   "metadata": {},
   "source": [
    "The comparison is pretty good! Sure enough, ATM appears to agree very well with GT2L! The southern reaches of the track have some disagreement, but we can also see that this region has some rough terrain that could cause some errors. Let's see if we can perform a bias assessment of GT2L relative to ATM now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0cad70-40db-4c76-9b35-837c4149cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downscale the ATM data to be of equal size to the IS-2 data\n",
    "from scipy.interpolate import splrep,splev\n",
    "\n",
    "spl = splrep(is2_gt2l['lat'], is2_gt2l['elev'], s=0)\n",
    "is2_spl = splev(atm_l2['Latitude(deg)'], spl, der=0)\n",
    "\n",
    "# Calculate GT2L bias and add it to the DataFrame\n",
    "atm_l2['bias'] = atm_l2['WGS84_Ellipsoid_Height(m)'] - is2_spl\n",
    "\n",
    "# Plot the bias curve\n",
    "plt.plot(is2_gt2l['lat'], is2_gt2l['bias'])\n",
    "#plt.plot(atm_l2['Latitude(deg)'], atm_l2['WGS84_Ellipsoid_Height(m)'])\n",
    "#plt.plot(atm_l2['Latitude(deg)'], is2_spl)\n",
    "plt.xlabel('latitude')\n",
    "plt.ylabel('bias [m]')\n",
    "plt.xlim([69.2, 69.26])\n",
    "plt.ylim([-40, 35])\n",
    "plt.show()\n",
    "\n",
    "print(atm_l2['bias'].mean())\n",
    "print(atm_l2['bias'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd35aa1-419f-44ce-afcc-90315a7149cc",
   "metadata": {},
   "source": [
    "It seems like there is a fair amount of variability in the area. Still, these values are fairly close to each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd1289-0941-4e82-b1ce-4e3c771ee665",
   "metadata": {},
   "source": [
    "## Download (Landsat) images from the cloud:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d6cdc-869b-4912-a2b8-9834ac76c48f",
   "metadata": {},
   "source": [
    "An Amazon Web Services (AWS) account is required to directly access Landsat data in\n",
    "the cloud housed within the usgs-landsat S3 requester pays bucket. Users can also find\n",
    "all objects within the Landsat record by using either the SpatioTemporal Asset Catalog\n",
    "(STAC) Browser (s3) or Satellite (SAT) Application Programing Interface (API) (https)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5164d2-005c-46dd-8dbf-fe5ad82ad9cc",
   "metadata": {},
   "source": [
    "Scene location - https://landsat-pds.s3.amazonaws.com/c1/L8/202/025/LC08_L1TP_202025_20190420_20190507_01_T1/ \\\n",
    "Scene band url for download - https://landsat-pds.s3.amazonaws.com/c1/L8/202/025/LC08_L1TP_202025_20190420_20190507_01_T1/LC08_L1TP_202025_20190420_20190507_01_T1_B4.TIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92589138-1d61-4bda-a26f-4d55909745d3",
   "metadata": {},
   "source": [
    "#### Using data in the cloud from S3 buckets:\n",
    "\n",
    "Landsat S3 User Manual: https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/atoms/files/LSDS-2032-Landsat-Commercial-Cloud-Direct-Access-Users-Guide-v2.pdf.pdf\n",
    "\n",
    "Scene band url for access - s3://usgs-landsat/collection02/level-2/standard/oli-tirs/2020/202/025/LC08_L1TP_202025_20190420_20190507_01_T1/LC08_L1TP_202025_20190420_20190507_01_T1_B4.TIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05076b32-9f21-42ea-b4bc-de8f6fc98f65",
   "metadata": {},
   "source": [
    "## Find and open Landsat images in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c5c58-d5d4-46d2-95be-53f3eb212878",
   "metadata": {},
   "source": [
    "But what if you want to search for Landsat 8 data over an area of interest? Browsing through lists of files is cumbersome. \n",
    "\n",
    "How can data formats enable high-speed access?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1ce5f-7b4a-4935-b234-a4b65ac3c109",
   "metadata": {},
   "source": [
    "### Web Object-based Storage (e.g., Simple Storage Service)\n",
    "Data storage architecture for handling large amounts of unstructured data (like geospatial datasets)\n",
    "* Inexpensive storage, BUT...\n",
    "* ...access is via HTTP1, not random disk reads\n",
    "* Does include HTTP “range-get” to retrieve a specific byte range\n",
    "\n",
    "### Cloud optimized approaches\n",
    "* Organize data as an aggregation of small, independently retrievable objects (e.g., zarr, HDF2, Geotiff in the Cloud)\n",
    "* Allow access to pieces of large objects (e.g., Cloud-Optimized GeoTIFF3,OPeNDAP4 in the Cloud)\n",
    " \n",
    "We will be working with Cloud Optimized GeoTIFF (COG). A COG is a GeoTIFF file with an internal organization that enables more efficient workflows in the cloud environment.  It does this by leveraging the ability of clients issuing ​HTTP GET range requests to ask for just the parts of a file they need instead of having to open the entire image or data set.\n",
    "\n",
    "To explore and access COG's easily we will use a SpatioTemporal Asset Catalog (STAC). The STAC specification provides a common, machine-readable (JSON) format for describing a wide range of geospatial datasets. STAC’s goal is to make it easier to index and discover any geospatial dataset that can be described by a spatial extent and time. STAC is the way geospatial asset metadata is structured and queried and it makes querrying S3 buckets easier. Learn more here: https://github.com/radiantearth/stac-spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0ca2b-8f04-4825-82e0-920ef67be0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up credentials for acquiring images through dask/xarray\n",
    "os.environ[\"AWS_REQUEST_PAYER\"] = \"requester\"\n",
    "\n",
    "# Need to set up proper credentials for acquiring data through rasterio\n",
    "aws_session = AWSSession(boto3.Session(), requester_pays=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853034d-165c-4f7d-9ed0-4fcd31d05c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract geometry bounds for Landsat spatial bbox\n",
    "geom = jk.geometry[0]\n",
    "print(geom.bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8462f9c-131a-46c2-8f97-7c7197010031",
   "metadata": {},
   "source": [
    "We will search for imagery in STAC catalog using satsearch: \n",
    "https://github.com/sat-utils/sat-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24141fa8-38eb-4ddb-a585-b8805c7cfd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search STAC API for Landsat images based on a bounding box, date and other metadata if desired\n",
    "\n",
    "bbox = (-51.3229, 68.840, -48.203, 69.616) #(west, south, east, north) \n",
    "\n",
    "timeRange = '2019-05-06/2019-05-07'\n",
    "    \n",
    "results = Search.search(url='https://ibhoyw8md9.execute-api.us-west-2.amazonaws.com/prod',\n",
    "                        collection='usgs-landsat/collection02/',\n",
    "                        datetime=timeRange,\n",
    "                        bbox=bbox,    \n",
    "                        # properties=properties,\n",
    "                        sort=['<datetime'])\n",
    "\n",
    "print('%s items' % results.found())\n",
    "items = results.items()\n",
    "\n",
    "# Save search to geojson file\n",
    "gjson_outfile = f'/home/jovyan/website2022/book/tutorials/DataIntegration/Landsat.geojson'\n",
    "items.save(gjson_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22daee1c-297a-4a0f-8e19-6dd2ad326b28",
   "metadata": {},
   "source": [
    "We are given a satsearch collection of items (images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f887b-8acd-4867-a675-100d9d2cdf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e4165-59ff-4127-a91b-cf34f0286566",
   "metadata": {},
   "source": [
    "Load the geojson file into geopandas and inspect the items we want to collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geojson file\n",
    "gf = gpd.read_file(gjson_outfile)\n",
    "gf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dcf7c-e970-417d-ad4c-8e8c9540b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot search area of interest and frames on a map using Holoviz Libraries (more on these later)\n",
    "cols = gf.loc[:,('id','landsat:wrs_path','landsat:wrs_row','geometry')]\n",
    "footprints = cols.hvplot(geo=True, line_color='k', hover_cols=['landsat:wrs_path','landsat:wrs_row'], alpha=0.2, title='Landsat 8 T1',tiles='ESRI')\n",
    "tiles = gv.tile_sources.CartoEco.options(width=700, height=500) \n",
    "labels = gv.tile_sources.StamenLabels.options(level='annotation')\n",
    "tiles * footprints * labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a6ed8-2743-45a6-82ed-eea895349d17",
   "metadata": {},
   "source": [
    "#### Intake all scenes using the intake-STAC library\n",
    "Intake-STAC facilitates discovering, exploring, and loading spatio-temporal datasets.\n",
    "\n",
    "Intake-STAC provides Intake Drivers for SpatioTemporal Asset Catalogs (STAC). By combining Intake and sat-stac, Intake-STAC provides a simple toolkit for working with STAC catalogs and for loading STAC assets as Xarray objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f38d9-f8cb-4e84-89d1-6428702f9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = intake.open_stac_item_collection(items)\n",
    "list(catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0991aa5-35bb-4c45-9899-87a127486cc0",
   "metadata": {},
   "source": [
    "\\\n",
    "Let's explore the metadata and keys for the first scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42faf5-cccf-47ff-9bf8-f70ebd78f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sceneids = list(catalog)\n",
    "item0 = catalog[sceneids[0]]\n",
    "# item0.metadata\n",
    "for keys in item0.keys():\n",
    "    print (keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a14425-8c85-482e-b6b6-ea0b246db046",
   "metadata": {},
   "source": [
    "We can explore the metadata for any of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957c5e3-ec7b-4b51-a6fc-0fdc8d1c0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item0['blue'].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb00dde-fc04-483d-b5ec-2e2e561af774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url needed to grab data from the S3 bucket\n",
    "# Using the satsearch catalog:\n",
    "items[0].asset('blue')['alternate']['s3']['href'] # can use item asset name or title (blue or B2)\n",
    "\n",
    "# Using the intake-STAC catalog\n",
    "item0.blue.metadata['alternate']['s3']['href'] # must use item asset name (blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d64cb1-76ce-4cec-884f-087c514ccbef",
   "metadata": {},
   "source": [
    "# Should I keep next cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a3220-bf2b-4034-99e3-4ff3c2888ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve first scene using rio\n",
    "item_url = item0.blue.metadata['alternate']['s3']['href']\n",
    "\n",
    "# Read and plot with grid coordinates \n",
    "with rio.Env(aws_session):\n",
    "    with rio.open(item_url) as src:\n",
    "        fig, ax = plt.subplots(figsize=(9,8))\n",
    "        \n",
    "        # To plot\n",
    "        show(src,1)\n",
    "        \n",
    "        # To open data into a numpy array\n",
    "        profile = src.profile\n",
    "        arr = src.read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913afed8-4113-47c7-bab1-5b45e11d8692",
   "metadata": {},
   "source": [
    "### Manipulating data in Xarray\n",
    "__Tasha__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32afc99-538f-48ba-b2a5-37870fb75b9e",
   "metadata": {},
   "source": [
    "Pandas and Xarray have very similar structures and ways of manipulating data, but Pandas excells with 2-D data and Xarray is ideal for more dimensions. Xarray introduces labels in the form of dimensions, coordinates and attributes on top of Pandas-like DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37b4de-4f47-4f3b-a7e1-528c7d197017",
   "metadata": {},
   "source": [
    "Xarray doesn’t just keep track of labels on arrays – it uses them to provide a powerful and concise interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee58074-466e-4933-8c35-330ac2030aa9",
   "metadata": {},
   "source": [
    "Read in 3 bands of a scene to xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1c642-9fe6-4381-899f-84972a432237",
   "metadata": {},
   "outputs": [],
   "source": [
    "sceneid = catalog[sceneids[0]]\n",
    "print (sceneid.name)\n",
    "\n",
    "band_names = ['red','green','blue']\n",
    "\n",
    "bands = []\n",
    "\n",
    "# Construct xarray for scene\n",
    "for band_name in band_names:\n",
    "    # Specify chunk size (x,y), Landsat COG is in 512 chunks so can use this or a multiple\n",
    "    band = sceneid[band_name](chunks=dict(band=1, x=512, y=512),urlpath=sceneid[band_name].metadata['alternate']['s3']['href']).to_dask()\n",
    "    band['band'] = [band_name]\n",
    "    bands.append(band)\n",
    "scene = xr.concat(bands, dim='band')\n",
    "scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3876e4f-4a64-498d-a6a0-47654bb3bb5b",
   "metadata": {},
   "source": [
    "We can choose not to specify chunk sizes and have everything read as one chunk. Though, when we load larger sets of imagery, like this one, we can change these chunk sizes to use dask. It’s best to align dask chunks with the way image chunks (typically called “tiles” are stored on disk or cloud storage buckets. The landsat data is stored on AWS S3 in a tiled Geotiff format where tiles are 512x512, so we should pick some multiple of that, and typically aim for chunk sizes of ~100Mb (although this is subjective). You can read more about dask chunks here: https://docs.dask.org/en/latest/array-best-practices.html\n",
    "\n",
    "Also check out this documentation about the Cloud-optimized Geotiff format, it is an excellent choice for putting satellite raster data on Cloud storage: https://www.cogeo.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c580e73-d076-47ac-9593-e995e46f84e6",
   "metadata": {},
   "source": [
    "Similarly as with pandas, we can explore variables easily. This time we can work with coordinates (equivalent to indices in pandas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7dc6f5-150c-410d-bc69-2cfd071cc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445764f1-155b-4cf8-b353-6e029a283627",
   "metadata": {},
   "source": [
    "Or keep track of arbitrary metadata (called attributes) in the form of a Python dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a99544-9ca6-4f6c-aa59-dcc5ee29b5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scene.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4549dcd-7346-4949-9221-5d13b346d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e1be2-f638-4913-b35f-93c76827a2d2",
   "metadata": {},
   "source": [
    "We can apply operations over dimensions by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382c443-237e-4d89-af5c-730290e26817",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.sel(band='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9371ce-bc08-4b9b-9efe-ba5c6db5f6be",
   "metadata": {},
   "source": [
    "Mathematical operations (e.g., x - y) vectorize across multiple dimensions (array broadcasting) based on dimension names, not shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d5c55-ea57-491b-882c-9f9c31d5de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.sel(band='blue').sum()#.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4407ba-4fc2-4f09-aea4-f670fa8fb8f8",
   "metadata": {},
   "source": [
    "Easily use the split-apply-combine paradigm with groupby:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1007e9f-d79f-4c60-99c3-573edb535eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene.groupby('band').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46115ce8-8e7d-4eaf-9681-7dbf85100f36",
   "metadata": {},
   "source": [
    "Database-like alignment based on coordinate labels that smoothly handles missing values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d5255-322f-42f3-a085-3a22520bb377",
   "metadata": {},
   "source": [
    "The N-dimensional nature of xarray’s data structures makes it suitable for dealing with multi-dimensional scientific data, and its use of dimension names instead of axis labels (dim='time' instead of axis=0) makes such arrays much more manageable than the raw numpy ndarray.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe58fc9-2fb3-4512-bbf1-356b19565596",
   "metadata": {},
   "source": [
    "\\\n",
    "Now let's open all the bands and multiple days together into an xarray. This will be a more complex xarray with an extra `'time'` dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae28dc-51f2-4041-ad5a-16f29437467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sceneids = list(catalog)\n",
    "sceneids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6a08f-260c-4a6c-a841-9e11b52a3fa0",
   "metadata": {},
   "source": [
    "\\\n",
    "Let's create the time variable first for the xarray time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92652324-3c6d-42e7-b531-f8e08df343d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time variable for time dim\n",
    "time_var = xr.Variable('time',gf.loc[gf.id.isin([sceneid for sceneid in sceneids if sceneid.endswith('_T1')])]['datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de262fa6-7c1e-4d19-95d5-894a8b89ecb2",
   "metadata": {},
   "source": [
    "Now we will search and collect band names for grabbing each desired band. We will just grab the bands that have 30 m pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f5945-62c0-4360-8930-9c2c24f4c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_names = []\n",
    "\n",
    "# Get band names\n",
    "sceneid = catalog[sceneids[0]]\n",
    "for k in sceneid.keys():\n",
    "    M = getattr(sceneid, k).metadata\n",
    "    if 'eo:bands' in M:\n",
    "        resol = M['eo:bands'][0]['gsd']\n",
    "        print(k, resol)\n",
    "        if resol >= 30: \n",
    "            band_names.append(k)\n",
    "            \n",
    "# Add qa band\n",
    "band_names.append('qa_pixel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18488506-2397-4a14-a732-a785cd279f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import to xarray\n",
    "# In xarray dataframe nans are in locations where concat of multiple scenes has expanded the grid\n",
    "\n",
    "scenes = []\n",
    "\n",
    "for sceneid in sceneids:\n",
    "    if sceneid.endswith('_T1'):\n",
    "        sceneid = catalog[sceneid]\n",
    "        print (sceneid.name)\n",
    "        \n",
    "        bands = []\n",
    "        \n",
    "        # Construct xarray for scene\n",
    "        for band_name in band_names:\n",
    "            band = sceneid[band_name](chunks=dict(band=1, x=2048, y=2048),urlpath=sceneid[band_name].metadata['alternate']['s3']['href']).to_dask() # Specify chunk size, landsat is prob in 512 chunks so used multiple\n",
    "            band['band'] = [band_name]\n",
    "            bands.append(band)\n",
    "        scene = xr.concat(bands, dim='band')\n",
    "        scenes.append(scene)\n",
    "\n",
    "# Concatenate scenes with time variable ***This is the slowest\n",
    "ls_scenes = xr.concat(scenes, dim=time_var)\n",
    "\n",
    "pix = ls_scenes.transform[0]\n",
    "epsg = ls_scenes.crs\n",
    "\n",
    "ls_scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b592a6-e14e-4ef2-b7a9-016c719788f7",
   "metadata": {},
   "source": [
    "We now have 2 Landsat scenes with all of the bands we are interested in stored in an xarray. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab467d-8b99-445a-b6de-feb236423c0c",
   "metadata": {},
   "source": [
    "We can also easily subset and visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71507c0f-b8a9-48ad-bf17-e3ed84dbd0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbands = ['blue', 'nir08', 'swir16','cirrus', 'lwir11']\n",
    "sub_box = 1\n",
    "\n",
    "# for t in thw.time.values:\n",
    "t = ls_scenes.time.values[0]\n",
    "\n",
    "# Further subset to polynya location if desired \n",
    "pulx = 500000\n",
    "puly = 7650000\n",
    "plrx = 600000\n",
    "plry = 7550000\n",
    "\n",
    "image = ls_scenes.sel(time=t,band=sbands,y=slice(plry,puly),x=slice(pulx,plrx))\n",
    "\n",
    "# For use at the end for coordinates\n",
    "pol_y = image.y\n",
    "pol_x = image.x\n",
    "\n",
    "image = np.array(image.where(image.notnull(),0))\n",
    "image = np.moveaxis(image, 0, -1)\n",
    "\n",
    "n_band = image.shape[2]\n",
    "print (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7f1bb-890e-4d5f-9d79-1be56ee7d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.imshow(image[:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-amateur",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have seen how we can search for non-ICESat_2 datasets, open them into pandas and xarray dataframes, and manipulate/explore the dataframes.\n",
    " \n",
    "\n",
    "```{note}\n",
    "You may have noticed Jupyter Book adds some extra formatting features that do not necessarily render as you might expect when *executing* a noteook in Jupyter Lab. This \"admonition\" note is one such example.\n",
    "```\n",
    "\n",
    ":::{warning}\n",
    "Jupyter Book is very particular about [Markdown header ordering](https://jupyterbook.org/structure/sections-headers.html?highlight=headers#how-headers-and-sections-map-onto-to-book-structure) to automatically create table of contents on the website. In this tutorial we are careful to use a single main header (#) and sequential subheaders (#, ##, ###, etc.)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749f5c79-19d7-48ac-9421-852c3c83cc5f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "To further explore the topics of this tutorial see the following detailed documentation:\n",
    "\n",
    "* [Jupyter Book rendering of .ipynb notebooks](https://jupyterbook.org/file-types/notebooks.html)\n",
    "* [Jupyter Book guide on writing narrative content](https://jupyterbook.org/content/index.html)\n",
    "* [ipyleaflet documentation](https://ipyleaflet.readthedocs.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
