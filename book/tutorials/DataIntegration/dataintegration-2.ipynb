{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data integration with ICESat-2 - Part II\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "**Goals**\n",
    "- Access NSIDC data sets and acquire IS-2 using icepyx\n",
    "- Analyze point and raster data together with IS-2\n",
    "- Advanced visualizations of multiple datasets\n",
    "```\n",
    "☝️ This formatting is a Jupyter Book [admonition](https://jupyterbook.org/content/content-blocks.html#notes-warnings-and-other-admonitions), that uses a custom version of Markdown called {term}`MyST`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries need to add:\n",
    "* nisardev\n",
    "* grimpfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Flowines Using Remote Greenland Ice Mapping Project Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how Greenland Ice Mapping Project can be remotely accessed to create plots along flowlines from [Felikson et al., 2020](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112), which are archived on [Zenodo](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112). The copies of the shapefiles included in this repository were downloaded in late January 2022. The notebook works with a specific set of glacier ids but is easily modified to plot results for other glaciers in the flowline data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import nisardev as nisar\n",
    "import os\n",
    "import matplotlib.colors as mcolors\n",
    "import grimpfunc as grimp\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import importlib\n",
    "import requests\n",
    "import pyproj\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import panel\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "panel.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSIDC Login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For remote access to the velocity data at NSIDC, run these cells to login with your NASA EarthData Login (see  [NSIDCLoginNotebook](https://github.com/fastice/GRiMPNotebooks/blob/master/NSIDCLoginNotebook.ipynb) for further details). These cells can skipped if all data are being accessed locally. First define where the cookie files need for login are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dict(GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/.grimp_download_cookiejar.txt'),\n",
    "            GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/.grimp_download_cookiejar.txt'))\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now enter credentials. If previous valid cookie exists, no user input is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myLogin = grimp.NASALogin()\n",
    "myLogin.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples presented here we will use glaciers 1 & 2  in the Felikson data base, [Felikson et al., 2020](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112), which were retrieved from [Zenodo](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112). \n",
    "\n",
    "Each glacier's flowlines are used to create `grimp.Flowlines` instances, which are saved in a dictionary, `myFlowlines` with glacier id: '0001' and  '0002'. Each `Flowlines` read a set of flowlines for each glacier and stores in a dictionary of `myFlowlines.flowlines`. The code to do this looks something like:\n",
    "\n",
    "```\n",
    "    flowlines = {}\n",
    "    shapeTable = gpd.read_file(shapefile)\n",
    "    for index, row in shapeTable.iterrows():  # loop over features\n",
    "        fl = {}  # New Flowline\n",
    "        fl['x'], fl['y'] = np.array([c for c in row['geometry'].coords]).transpose()\n",
    "        fl['d'] = computeDistance(fl['x'], fl['y'])\n",
    "        flowlines[row['flowline']] = fl\n",
    " ```\n",
    "For further detail, see the full [class defintion](https://github.com/fastice/grimpfunc/blob/master/grimpfunc/Flowlines.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit the plots to the downstream regions, the flowlines are all truncated to a `length` of 50km. \n",
    "\n",
    "Within each myFlowines entry (a `grimp.Flowlines` instance), the individual flowlines are maintained as a dictionary `myFlowlines['glacierId'].flowlines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myShapeFiles = [f'./shpfiles/glacier000{i}.shp' for i in range(1, 3)] # Build list of shape file names\n",
    "myFlowlines = {x[-8:-4]: grimp.Flowlines(shapefile=x, name=x[-8:-4], length=50e3) for x in myShapeFiles} \n",
    "myFlowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each flowline is indexed as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFlowlines['0001'].flowlines.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the flow line is simple, just `x`, `y` polar stereographic coordinates (EPSG=3413) and the distance, `d`, from the start of the flowline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFlowlines['0001'].flowlines['03'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coordinates for a given index can be return as `myFlowlines['0001'].xym(index='03')` or `myFlowlines['0001'].xykm(index='03')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area of interest can be defined as the union of the bounds for all of the flowlines computed as shown below along with the unique set of flowline IDs across all glaciers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myBounds = {'minx': 1e9, 'miny': 1e9, 'maxx': -1e9, 'maxy': -1e9}  # Initial bounds to force reset\n",
    "flowlineIDs = []  # \n",
    "for myKey in myFlowlines:\n",
    "    # Get bounding box for flowlines\n",
    "    flowlineBounds = myFlowlines[myKey].bounds\n",
    "    # Merge with prior bounds\n",
    "    myBounds = myFlowlines[myKey].mergeBounds(myBounds, flowlineBounds)\n",
    "    # Get the flowline ids\n",
    "    flowlineIDs.append(myFlowlines[myKey].flowlineIDs())\n",
    "# Get the unique list of flowlines ids (used for legends later)\n",
    "flowlineIDs = np.unique(flowlineIDs)\n",
    "print(myBounds)\n",
    "print(flowlineIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Catalog for Velocity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to locate velocity data from the GrIMP data set. For this excercise, we will focus on the annual velocity maps of Greenland. To do this, we will use the `grimp.cmrUrls` tool, which will do a GUI based search of NASA's Common Metadata Repository ([CMR](https://earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/cmr)). Search parameters can be passe directly to `initialSearch` method to perform the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myUrls = grimp.cmrUrls(mode='nisar', verbose=True)  # nisar mode excludes image and tsx products and allows only one product type at a time\n",
    "myUrls.initialSearch(product='NSIDC-0725')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `verbose` flag causes the CMR search string to be printed. The search basically works by a) reading the parameters from the search panel (e.g., product, date, etc) and creating a search string, which returns the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]='\n",
    "                        'producer_granule_id&scroll=false&page_size=2000&page_num=1&short_name=NSIDC-0725&version=3&temporal[]='\n",
    "                        '2000-01-01T00:00:01Z,2022-03-10T00:23:59&bounding_box[]=-75.00,60.00,-5.00,82.00&producer_granule_id[]='\n",
    "                        '*&options[producer_granule_id][pattern]=true')\n",
    "search_results = response.json()\n",
    "search_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the `cmrUrls` code can filter the json to get a list of urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myUrls.getURLS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Velocity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GrIMP produces full Greenland velocity maps. Collectively, there are more than 400 full Greenland maps, totalling several hundred GB of data, which may be more than a user interested in a few glaciers wants to download and store on their laptop.  Fortunately using Cloud Optimized Geotiffs, which have the following properties:\n",
    "\n",
    "- All the metadata is at the beginning of the file, allowing a single read to obtain the layout. \n",
    "- The data are tiled (i.e., stored as a series of blocks like a checkerboard) rather than as a line-by-line raster.  \n",
    "- A consistent set of overview images (pyramids) are stored with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the velocity data are stored as multiple files at NSIDC, they can all be combined into a single `nisarVelSeries` instance, which has the following properties:\n",
    "\n",
    "- Built on Xarray,\n",
    "- Dask (arallel operations),\n",
    "- Local and remote subsetting (Lazy Opens), and\n",
    "- Subsets can be saved for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the data, we must setup the filename template. Specifically, we must put a '\\*' where the band identifier would go and remove the trailing '.tif' extention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlNames = [x.replace('vv','*').replace('.tif','') for x in myUrls.getCogs()] # getCogs filters to ensure tif products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries = nisar.nisarVelSeries() # Create Series\n",
    "myVelSeries.readSeriesFromTiff(urlNames, url=True, readSpeed=False)  # readSpeed=False computes speed from vx, vy rather than downloading\n",
    "myVelSeries.xr  # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the annual data set, this step produces a ~7GB data sets, which expands to 370GB for the full 6-12-day data set. To avoid downloading unnessary data, the data can be subsetted using the bounding box from the flowlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries.subSetVel(myBounds) # Apply subset\n",
    "myVelSeries.subset # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volume of the data set is now a far more manageable ~15MB, which is still located in the archive.  Operations can continue without downloading, but if lots of operations are going to occur, it is best to download the data upfront. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries.loadRemote() # Load the data to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlNames[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myOverview = nisar.nisarVelSeries() # Create Series\n",
    "myOverview.readSeriesFromTiff([urlNames[-1]], url=True, readSpeed=False, overviewLevel=3)  # readSpeed=False computes speed from vx, vy rather than downloading\n",
    "myOverview.xr  # Add semicolon after to suppress output\n",
    "myOverview.loadRemote()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Flowlines and Velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flowlines over one of the velocity layers can be displayed as with the following block of code. In addition to plotting the flowline, a point 10 km along each flowline is plotted and saved for subsequent plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up figure and axis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(21, 12))\n",
    "# Create a dictionary for accumulating glacier points\n",
    "glacierPoints = {}\n",
    "# generate a color dict that spans all flowline ids, using method from a flowline instance\n",
    "flowlineColors = list(myFlowlines.values())[0].genColorDict(flowlineIDs=flowlineIDs)\n",
    "# Plot velocity maps\n",
    "# Saturate at 2000 m/yr to preserve slow detail\n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=axes[0], labelFontSize=12, plotFontSize=9, titleFontSize=14, \n",
    "                              vmin=0, vmax=2000, units='km', scale='linear', colorBarSize='3%') \n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=axes[1], labelFontSize=12, plotFontSize=9, titleFontSize=14, \n",
    "                              vmin=1, vmax=3000, units='km', scale='log', midDate=False, colorBarSize='3%')\n",
    "# Plot location inset\n",
    "height = 3\n",
    "axInset = inset_axes(axes[0], width=height * myOverview.sx/myOverview.sy, height=height, loc=1)\n",
    "myOverview.displayVelForDate(None, ax=axInset, vmin=1, vmax=3000, colorBar=False, scale='log', title='')\n",
    "axInset.plot(*myVelSeries.outline(), color='r')\n",
    "axInset.axis('off')\n",
    "#\n",
    "# Loop over each glacier and plot the flowlines\n",
    "for glacierId in myFlowlines:\n",
    "    # Plot the flowline Match units to the map\n",
    "    myFlowlines[glacierId].plotFlowlineLocations(ax=axes[0], units='km', colorDict=flowlineColors)\n",
    "    myFlowlines[glacierId].plotFlowlineLocations(ax=axes[1], units='km', colorDict=flowlineColors)\n",
    "    # \n",
    "    myFlowlines[glacierId].plotGlacierName(ax=axes[0], units='km', color='w', fontsize=12,fontweight='bold', first=False)\n",
    "    myFlowlines[glacierId].plotGlacierName(ax=axes[1], units='km', color='w', fontsize=12,fontweight='bold', first=False)\n",
    "    # Generates points 10km from downstream end of each flowline\n",
    "    points10km = myFlowlines[glacierId].extractPoints(10, None, units='km')\n",
    "    glacierPoints[glacierId] = points10km\n",
    "    for key in points10km:\n",
    "        axes[0].plot(*points10km[key], 'r.')\n",
    "        axes[1].plot(*points10km[key], 'r.')\n",
    "\n",
    "\n",
    "# Add legend\n",
    "for ax in axes:\n",
    "    # Create a dict of unique labels for legend\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(l, h)) # will overwrite identical entries to produce uniqe values\n",
    "    ax.legend(by_label.values(), by_label.keys(), title='Flowline ID', ncol=2, loc='lower left', fontsize=14)\n",
    "#fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common function with the velocity date is interpolating data for plotting points or profiles, which can be easily done with the `nisarVelSeries.interp` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using km\n",
    "vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "# or units of meters\n",
    "vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xym(), units='m')\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "# or entirely different coordinate system\n",
    "xytoll = pyproj.Transformer.from_crs(3413, 4326)\n",
    "lat, lon = xytoll.transform(*myFlowlines[glacierId].xym())\n",
    "vx, vy, vv = myVelSeries.interp(lat, lon, sourceEPSG=4326)\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "# Or would prefer an xarray rather than nparray\n",
    "result = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km', returnXR=True)\n",
    "result;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Central Flowlines at Different Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example will demonstrate plotting the nominally central flowline ('06') for each of the six years for which there are currently data. This example works for annual data. Minor modifications are needed for the legend if more frequent data are selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowlineId ='06'  # Flowline id to plot\n",
    "fig, axes = plt.subplots(np.ceil(len(myFlowlines)/4).astype(int), 2, figsize=(16, 8))  # Setup plot\n",
    "# Loop over glaciers\n",
    "for glacierId, ax in zip(myFlowlines, axes.flatten()):\n",
    "    # return interpolated values as vx(time index, distance index)\n",
    "    vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "    # loop over each profile by time\n",
    "    for speed, myDate in zip(vv, myVelSeries.time):\n",
    "        ax.plot(myFlowlines[glacierId].distancekm(), speed, label=myDate.year)\n",
    "    # pretty up plot\n",
    "    ax.legend(ncol=2, loc='upper right', fontsize=15)\n",
    "    ax.set_xlabel('Distance (km)', fontsize=18)\n",
    "    ax.set_ylabel('Speed (m/yr)', fontsize=18)\n",
    "    ax.set_title(f'Glacier {glacierId}', fontsize=20)\n",
    "# For other combinations could have \n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Points Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the map plots were generated above, a set of points 10-k from the start of each flowline was extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "# Loop over glaciers\n",
    "for glacierId, ax in zip(glacierPoints, axes.flatten()):\n",
    "    # Loop over flowlines\n",
    "    for flowlineId in glacierPoints[glacierId]:\n",
    "        # interpolate to get results vx(time index) for each point\n",
    "        vx, vy, v = myVelSeries.interp(*glacierPoints[glacierId][flowlineId], units='km')\n",
    "        ax.plot(myVelSeries.time, v, marker='o',  linestyle='-', color=flowlineColors[flowlineId],label=f'{flowlineId}')\n",
    "    # pretty up plot\n",
    "    ax.legend(ncol=3, loc='upper right', title='Flowline ID')\n",
    "    ax.set_xlabel('year', fontsize=13)\n",
    "    ax.set_ylabel('Speed (m/yr)', fontsize=13)\n",
    "    ax.set_title(f'Glacier {glacierId}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is convenient to work the date remotely, its nice to be be able to save the data for further processing.\n",
    "\n",
    "The downloaded subset can be saved in a netcdf and reloaded for to `velSeries` instance for later analysis. \n",
    "\n",
    "Note makes sure the data have been subsetted so only the the subset will be saved (~15MB in this example). If not, the entire Greeland data set will be saved (370GB). \n",
    "\n",
    "Change `saveData` and `reloadData` below to test this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData = False # Set to True to save data\n",
    "if saveData:\n",
    "    myVelSeries.toNetCDF('Glaciers1-2example.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open open the file and redo a plot from above with the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloadData = False  # Set to True to reload the saved data\n",
    "if reloadData:\n",
    "    fig, axes = plt.subplots(np.ceil(len(myFlowlines)/2).astype(int), 2, figsize=(16, 8))  # Setup plot\n",
    "    myVelCDF = nisar.nisarVelSeries() # Create Series\n",
    "    myVelCDF.readSeriesFromNetCDF('Glaciers1-2example.nc')\n",
    "    #\n",
    "    for glacierId, ax in zip(myFlowlines, axes.flatten()):\n",
    "        # return interpolated values as vx(time index, distance index)\n",
    "        vx, vy, vv = myVelCDF.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "        # loop over each profile by time\n",
    "        for speed, myDate in zip(vv, myVelSeries.time):\n",
    "            ax.plot(myFlowlines[glacierId].distancekm(), speed, label=myDate.year)\n",
    "        # pretty up plot\n",
    "        ax.legend(ncol=2, loc='upper right', fontsize=15)\n",
    "        ax.set_xlabel('Distance (km)', fontsize=18)\n",
    "        ax.set_ylabel('Speed (m/yr)', fontsize=18)\n",
    "        ax.set_title(f'Glacier {glacierId}', fontsize=20)\n",
    "# For other combinations could have \n",
    "    for ax in axes.flatten():\n",
    "        ax.tick_params(axis='both', labelsize=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing with widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dotson - requires a shapefile to plot that has been formated to lists of lat/lon pairs\n",
    "# formatting for shapefile: Wcoords2 = [[Wlat2[i],Wlon2[i]] for i in range(Wlat2.shape[0])]\n",
    "\n",
    "add\n",
    "\n",
    "center = [-74.23, -112.00]\n",
    "zoom = 8\n",
    "\n",
    "mapdt1 = '2022-02-01'\n",
    "\n",
    "global dc, start, end, file, lon_l, lat_l, lon_r, lat_r, dt\n",
    "\n",
    "# Pick date\n",
    "start = widgets.DatePicker(disabled=False)\n",
    "end = widgets.DatePicker(disabled=False)\n",
    "# dt = widgets.DatePicker(disabled=False)\n",
    "\n",
    "# Select from map\n",
    "draw_control = DrawControl(rectangle={}, polyline={'shapeOptions': {'color': '#fca45d','weight': 1,'opacity': 1.0}}, \n",
    "                 polygon={}, circlemarker={})\n",
    "\n",
    "feature_collection = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': []}\n",
    "\n",
    "def handle_draw(self, action, geo_json):\n",
    "    \"\"\"Do something with the GeoJSON when it's drawn on the map\"\"\"    \n",
    "    feature_collection['features'].append(geo_json)\n",
    "\n",
    "draw_control.on_draw(handle_draw)\n",
    "\n",
    "m = Map(basemap=basemap_to_tiles(basemaps.NASAGIBS.ModisAquaTrueColorCR, mapdt1),center=center,zoom=zoom)\n",
    "\n",
    "line1 = Polyline(locations=Wcoords1, weight=1,color='red' , fill=False)\n",
    "line2 = Polyline(locations=Wcoords2, weight=1,color='red' , fill=False)\n",
    "m.add_layer(line1)\n",
    "m.add_layer(line2)\n",
    "\n",
    "# Provides drawing control - can add something that keeps what is drawn...\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# Can upload a shapefile\n",
    "file = widgets.FileUpload(accept='.shp', multiple=False)\n",
    "\n",
    "# Specify a bounding box\n",
    "lon_l = widgets.FloatText(description=\"lon\")\n",
    "lat_l = widgets.FloatText(description=\"lat\")\n",
    "lon_r = widgets.FloatText(description=\"lon\")\n",
    "lat_r = widgets.FloatText(description=\"lat\")\n",
    "\n",
    "AppLayout(header = VBox([HTML(\"<h1>Select area (time and space)</h1>\"),\n",
    "                 HBox([Label(\"Start Date:\"), start, \n",
    "                       Label(\"End Date:\"), end]),\n",
    "                 # HBox([Label(\"Map Date:\"), mapdt1])\n",
    "                        ]),\n",
    "             center = m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    " 🎉 Congratulations! You've completely this tutorial and have seen how we can add  notebook can be formatted, and how to create interactive map visualization with ipyleaflet.\n",
    " \n",
    "\n",
    "```{note}\n",
    "You may have noticed Jupyter Book adds some extra formatting features that do not necessarily render as you might expect when *executing* a noteook in Jupyter Lab. This \"admonition\" note is one such example.\n",
    "```\n",
    "\n",
    ":::{warning}\n",
    "Jupyter Book is very particular about [Markdown header ordering](https://jupyterbook.org/structure/sections-headers.html?highlight=headers#how-headers-and-sections-map-onto-to-book-structure) to automatically create table of contents on the website. In this tutorial we are careful to use a single main header (#) and sequential subheaders (#, ##, ###, etc.)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "To further explore the topics of this tutorial see the following detailed documentation:\n",
    "\n",
    "* [Jupyter Book rendering of .ipynb notebooks](https://jupyterbook.org/file-types/notebooks.html)\n",
    "* [Jupyter Book guide on writing narrative content](https://jupyterbook.org/content/index.html)\n",
    "* [ipyleaflet documentation](https://ipyleaflet.readthedocs.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forge",
   "language": "python",
   "name": "forge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
