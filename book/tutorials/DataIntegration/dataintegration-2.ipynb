{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74079890",
   "metadata": {},
   "source": [
    "# Data integration with ICESat-2 - Part II\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "**Goals**\n",
    "- Access NSIDC data sets and acquire IS-2 using icepyx\n",
    "- Analyze point and raster data together with IS-2\n",
    "- Advanced visualizations of multiple datasets\n",
    "```\n",
    "‚òùÔ∏è This formatting is a Jupyter Book [admonition](https://jupyterbook.org/content/content-blocks.html#notes-warnings-and-other-admonitions), that uses a custom version of Markdown called {term}`MyST`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d87b1",
   "metadata": {},
   "source": [
    "## Libraries need to add:\n",
    "* nisardev\n",
    "* grimpfunc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5a84f-4efc-4c4b-882d-789488577688",
   "metadata": {},
   "source": [
    "# Comparing ICESat-2 Data with Other Datasets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732376d-8430-4e30-91c5-b8a9650cba7c",
   "metadata": {},
   "source": [
    "Last time, we did a bit of work to add ICESat-2 and Operation Icebridge data to Pandas Dataframes. We only covered the basic operations that you can do with Pandas, so today we are going to do a more thorough analysis of the data here.\n",
    "\n",
    "Since we already downloaded the ICESat-2/ATM files of interest, we are not going to use icepyx just yet - we will go ahead and reload the data from yesterday.\n",
    "\n",
    "(Prompt) I forgot how to load the ICESat-2 data from a .h5 file. What do I need to do?\n",
    "\n",
    "(Prompt) I also forgot how to load the ATM data. How do I read the CSV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd55a71-4dd2-4314-ad09-9d01405b0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2907df4-e054-4d9f-8efb-f37090480601",
   "metadata": {},
   "source": [
    "We established last time that ATM aligns best with the central ICESat-2 beams, particularly the central strong beam (GT2L). Let's see if that is reflected in the elevation profiles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50174c5-b3a4-4123-b714-b2b20e6d9591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ICESat-2 data\n",
    "is2_file = 'processed_ATL06_20190420093051_03380303_005_01_full.h5'\n",
    "with h5py.File(is2_file, 'r') as f:\n",
    "    is2_gt2r = pd.DataFrame(data={'lat': f['gt2r/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2r/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2r/land_ice_segments/h_li'][:]}) # Central weak beam\n",
    "    is2_gt2l = pd.DataFrame(data={'lat': f['gt2l/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2l/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2l/land_ice_segments/h_li'][:]}) # Central strong beam\n",
    "    \n",
    "# Load the ATM data\n",
    "atm_file = 'ILATM2_20190506_151600_smooth_nadir3seg_50pt.csv'\n",
    "atm_l2 = pd.read_csv(atm_file)\n",
    "\n",
    "# Look only at the nadir track\n",
    "atm_l2 = atm_l2[atm_l2['Track_Identifier']==0]\n",
    "\n",
    "# Change the longitudes to be consistent with ICESat-2\n",
    "atm_l2['Longitude(deg)'] -= 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784b935-431f-459e-acd4-8cbe3450ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the ICESat-2 data to the ATM latitudes\n",
    "is2_gt2r = is2_gt2r[(is2_gt2r['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2r['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "is2_gt2l = is2_gt2l[(is2_gt2l['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2l['lat']>atm_l2['Latitude(deg)'].min())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70385d23-32a1-487b-ab4a-10254ac803ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 2D plot of along-track surface height\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "plt.plot(is2_gt2r['lat'], is2_gt2r['elev'], label='gt2r')\n",
    "plt.plot(is2_gt2l['lat'], is2_gt2l['elev'], label='gt2l')\n",
    "plt.plot(atm_l2['Latitude(deg)'], atm_l2['WGS84_Ellipsoid_Height(m)'], label='atm')\n",
    "plt.xlabel('latitude')\n",
    "plt.ylabel('elevation [m]')\n",
    "plt.xlim([69.185, 69.275])\n",
    "plt.ylim([100, 550])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba608b9-c2c3-4b11-8796-005dc1ffe35a",
   "metadata": {},
   "source": [
    "Sure enough, GT2L and ATM match very well! Since they are very close to each other, we can do a quick accuracy assessment between the two.\n",
    "\n",
    "The ATM DataFrame is larger than the ICESat-2 dataframe, so we're going to apply a simple spline interpolant to downscale the ICESat-2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d3520-d999-463f-ae11-5302a2f2d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splrep,splev\n",
    "\n",
    "# Apply a spline interpolant to the ICESat-2 data\n",
    "spl = splrep(is2_gt2l['lat'], is2_gt2l['elev'], s=0)\n",
    "is2_spl = splev(atm_l2['Latitude(deg)'], spl, der=0)\n",
    "\n",
    "# Calculate GT2L bias and add it to the ATM DataFrame\n",
    "atm_l2['bias'] = atm_l2['WGS84_Ellipsoid_Height(m)'] - is2_spl\n",
    "\n",
    "# Plot the bias curve\n",
    "plt.plot(atm_l2['Latitude(deg)'], atm_l2['bias'])\n",
    "#plt.plot(atm_l2['Latitude(deg)'], atm_l2['WGS84_Ellipsoid_Height(m)'])\n",
    "#plt.plot(atm_l2['Latitude(deg)'], is2_spl)\n",
    "plt.xlabel('latitude')\n",
    "plt.ylabel('bias [m]')\n",
    "plt.xlim([69.2, 69.26])\n",
    "plt.ylim([-20, 20])\n",
    "plt.show()\n",
    "\n",
    "print('Mean bias: %s m' %(atm_l2['bias'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254be85c-af8e-4013-8e52-001d7173e9e4",
   "metadata": {},
   "source": [
    "Through some relatively simple operations, we found that ATM and ICESat-2 differ by ~0.33 m on average. Between this plot and the elevation plot above, what do you think might be causing some of the differences?\n",
    "\n",
    "We will revisit ICESat-2 and ATM near the end of this tutorial. Now, we are going to look at ice velocities and flow lines from the GRIMP project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebc076",
   "metadata": {},
   "source": [
    "# Plot Flowines Using Remote Greenland Ice Mapping Project Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5a0ae",
   "metadata": {},
   "source": [
    "This notebook demonstrates how Greenland Ice Mapping Project can be remotely accessed to create plots along flowlines from [Felikson et al., 2020](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112), which are archived on [Zenodo](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112). The copies of the shapefiles included in this repository were downloaded in late January 2022. The notebook works with a specific set of glacier ids but is easily modified to plot results for other glaciers in the flowline data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import nisardev as nisar\n",
    "import os\n",
    "import matplotlib.colors as mcolors\n",
    "import grimpfunc as grimp\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import importlib\n",
    "import requests\n",
    "import pyproj\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import panel\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "panel.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00471c8-5bd7-4277-82e0-f3d8aefff622",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  git+https://github.com/fastice/grimpfunc.git@master;\n",
    "!pip install git+https://github.com/fastice/nisardev.git@main;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e0668",
   "metadata": {},
   "source": [
    "## NSIDC Login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5832bd",
   "metadata": {},
   "source": [
    "For remote access to the velocity data at NSIDC, run these cells to login with your NASA EarthData Login (see  [NSIDCLoginNotebook](https://github.com/fastice/GRiMPNotebooks/blob/master/NSIDCLoginNotebook.ipynb) for further details). These cells can skipped if all data are being accessed locally. First define where the cookie files need for login are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b903d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dict(GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/.grimp_download_cookiejar.txt'),\n",
    "            GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/.grimp_download_cookiejar.txt'))\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37b0b5",
   "metadata": {},
   "source": [
    "Now enter credentials. If previous valid cookie exists, no user input is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myLogin = grimp.NASALogin()\n",
    "myLogin.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143e344",
   "metadata": {},
   "source": [
    "## Read Shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beeaa2f",
   "metadata": {},
   "source": [
    "In the examples presented here we will use glaciers 1 & 2  in the Felikson data base, [Felikson et al., 2020](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112), which were retrieved from [Zenodo](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112). \n",
    "\n",
    "Each glacier's flowlines are used to create `grimp.Flowlines` instances, which are saved in a dictionary, `myFlowlines` with glacier id: '0001' and  '0002'. \n",
    "\n",
    "Each `Flowlines` read a set of flowlines for each glacier and stores in a dictionary of `myFlowlines.flowlines`. The code to do this looks something like:\n",
    "\n",
    "```\n",
    "    flowlines = {}\n",
    "    shapeTable = gpd.read_file(shapefile)\n",
    "    for index, row in shapeTable.iterrows():  # loop over features\n",
    "        fl = {}  # New Flowline\n",
    "        fl['x'], fl['y'] = np.array([c for c in row['geometry'].coords]).transpose()\n",
    "        fl['d'] = computeDistance(fl['x'], fl['y'])\n",
    "        flowlines[row['flowline']] = fl\n",
    " ```\n",
    "For further detail, see the full [class defintion](https://github.com/fastice/grimpfunc/blob/master/grimpfunc/Flowlines.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32b76b",
   "metadata": {},
   "source": [
    "To limit the plots to the downstream regions, the flowlines are all truncated to a `length` of 50km. \n",
    "\n",
    "Within each myFlowines entry (a `grimp.Flowlines` instance), the individual flowlines are maintained as a dictionary `myFlowlines['glacierId'].flowlines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "myShapeFiles = [f'./shpfiles/glacier000{i}.shp' for i in range(1, 3)] # Build list of shape file names\n",
    "myFlowlines = {x[-8:-4]: grimp.Flowlines(shapefile=x, name=x[-8:-4], length=50e3) for x in myShapeFiles} \n",
    "myFlowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d1864",
   "metadata": {},
   "source": [
    "Each flowline is indexed as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6399d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFlowlines['0001'].flowlines.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46f028",
   "metadata": {},
   "source": [
    "The data for the flow line is simple, just `x`, `y` polar stereographic coordinates (EPSG=3413) and the distance, `d`, from the start of the flowline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00465a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFlowlines['0001'].flowlines['03'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34833585",
   "metadata": {},
   "source": [
    "These coordinates for a given index can be returned as `myFlowlines['0001'].xym(index='03')` or `myFlowlines['0001'].xykm(index='03')`, depending on whether m or km are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8544a83",
   "metadata": {},
   "source": [
    "The area of interest can be defined as the union of the bounds for all of the flowlines computed as shown below along with the unique set of flowline IDs across all glaciers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myBounds = {'minx': 1e9, 'miny': 1e9, 'maxx': -1e9, 'maxy': -1e9}  # Initial bounds to force reset\n",
    "flowlineIDs = []  # \n",
    "for myKey in myFlowlines:\n",
    "    # Get bounding box for flowlines\n",
    "    flowlineBounds = myFlowlines[myKey].bounds\n",
    "    # Merge with prior bounds\n",
    "    myBounds = myFlowlines[myKey].mergeBounds(myBounds, flowlineBounds)\n",
    "    # Get the flowline ids\n",
    "    flowlineIDs.append(myFlowlines[myKey].flowlineIDs())\n",
    "# Get the unique list of flowlines ids (used for legends later)\n",
    "flowlineIDs = np.unique(flowlineIDs)\n",
    "print(myBounds)\n",
    "print(flowlineIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2982f1",
   "metadata": {},
   "source": [
    "## Search Catalog for Velocity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8791ad4",
   "metadata": {},
   "source": [
    "We now need to locate velocity data from the GrIMP data set. For this excercise, we will focus on the annual velocity maps of Greenland. To do this, we will use the `grimp.cmrUrls` tool, which will do a GUI based search of NASA's Common Metadata Repository ([CMR](https://earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/cmr)). Search parameters can be passe directly to `initialSearch` method to perform the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "myUrls = grimp.cmrUrls(mode='nisar', verbose=True)  # nisar mode excludes image and tsx products and allows only one product type at a time\n",
    "myUrls.initialSearch(product='NSIDC-0725')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c5810",
   "metadata": {},
   "source": [
    "The `verbose` flag causes the CMR search string to be printed. The search basically works by a) reading the parameters from the search panel (e.g., product, date, etc) and creating a search string, which returns the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccff8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]='\n",
    "                        'producer_granule_id&scroll=false&page_size=2000&page_num=1&short_name=NSIDC-0725&version=3&temporal[]='\n",
    "                        '2000-01-01T00:00:01Z,2022-03-10T00:23:59&bounding_box[]=-75.00,60.00,-5.00,82.00&producer_granule_id[]='\n",
    "                        '*&options[producer_granule_id][pattern]=true')\n",
    "search_results = response.json()\n",
    "search_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd07a87",
   "metadata": {},
   "source": [
    "Under the hood, the `cmrUrls` code can filter the json to get a list of urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06088dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "myUrls.getURLS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182d472",
   "metadata": {},
   "source": [
    "## Load the Velocity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735892d",
   "metadata": {},
   "source": [
    "GrIMP produces full Greenland velocity maps. Collectively, there are more than 400 full Greenland maps, totalling several hundred GB of data, which may be more than a user interested in a few glaciers wants to download and store on their laptop.  Fortunately using Cloud Optimized Geotiffs, which have the following properties:\n",
    "\n",
    "- All the metadata is at the beginning of the file, allowing a single read to obtain the layout. \n",
    "- The data are tiled (i.e., stored as a series of blocks like a checkerboard) rather than as a line-by-line raster.  \n",
    "- A consistent set of overview images (pyramids) are stored with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd18231",
   "metadata": {},
   "source": [
    "While the velocity data are stored as multiple files at NSIDC, they can all be combined into a single `nisarVelSeries` instance, which has the following properties:\n",
    "\n",
    "- Built on Xarray,\n",
    "- Dask (arallel operations),\n",
    "- Local and remote subsetting (Lazy Opens), and\n",
    "- Subsets can be saved for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e20a8",
   "metadata": {},
   "source": [
    "Before loading the data, we must setup the filename template. Specifically, we must put a '\\*' where the band identifier would go and remove the trailing '.tif' extention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14078020",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlNames = [x.replace('vv','*').replace('.tif','') for x in myUrls.getCogs()] # getCogs filters to ensure tif products\n",
    "urlNames[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcca923-0a62-422a-b379-bca8e9f6cbc6",
   "metadata": {},
   "source": [
    "We can now create a `nisarVelocitySeries` object, which will create a large times series stack with all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03441cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries = nisar.nisarVelSeries() # Create Series\n",
    "myVelSeries.readSeriesFromTiff(urlNames, url=True, readSpeed=False)  # readSpeed=False computes speed from vx, vy rather than downloading\n",
    "myVelSeries.xr  # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57083baf",
   "metadata": {},
   "source": [
    "For the annual data set, this step produces a ~7GB data sets, which expands to 370GB for the full 6-12-day data set. \n",
    "\n",
    "To avoid downloading unnessary data, the data can be subsetted using the bounding box from the flowlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries.subSetVel(myBounds) # Apply subset\n",
    "myVelSeries.subset # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06753a08",
   "metadata": {},
   "source": [
    "The volume of the data set is now a far more manageable ~15MB, which is still located in the archive.  \n",
    "\n",
    "With dask, operations can continue without downloading, until the data are finally needed to do something (e.g., create a plot).\n",
    "\n",
    "If lots of operations are going to occur, however, it is best to download the data upfront. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries.loadRemote() # Load the data to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7dfc9",
   "metadata": {},
   "source": [
    "## Overview Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646099c0-2554-47e8-b85c-e62f5455d269",
   "metadata": {},
   "source": [
    "In the above, we picked out a small region for Greenland and downloaded a full-res data series. But in some cases, we may want the full image at reduced resolution (e.g., for an overview map). \n",
    "\n",
    "Here we can take advantage of the overviews to pull a single velocity map at reduced resolution (`overviewLevel=3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlNames[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "myOverview = nisar.nisarVelSeries() # Create Series\n",
    "myOverview.readSeriesFromTiff([urlNames[-1]], url=True, readSpeed=False, overviewLevel=3)  # readSpeed=False computes speed from vx, vy rather than downloading\n",
    "myOverview.loadRemote()\n",
    "myOverview.xr  # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddffbac1",
   "metadata": {},
   "source": [
    "## Display Flowlines and Velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a7d16",
   "metadata": {},
   "source": [
    "In the next cell, we put the above pieces together.\n",
    "\n",
    "Specifically, the flowlines over one of the velocity layers can be displayed as with the following block of code. \n",
    "\n",
    "In addition to plotting the flowline, a point 10 km along each flowline is plotted and saved for subsequent plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3962581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up figure and axis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16,8))\n",
    "#\n",
    "# Create a dictionary for accumulating glacier points\n",
    "glacierPoints = {}\n",
    "# generate a color dict that spans all flowline ids, using method from a flowline instance\n",
    "flowlineColors = list(myFlowlines.values())[0].genColorDict(flowlineIDs=flowlineIDs)\n",
    "# Plot velocity maps\n",
    "# Saturate at 2000 m/yr to preserve slow detail\n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=axes[0], labelFontSize=12, plotFontSize=9, titleFontSize=14, \n",
    "                              vmin=0, vmax=2000, units='km', scale='linear', colorBarSize='3%') \n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=axes[1], labelFontSize=12, plotFontSize=9, titleFontSize=14, \n",
    "                              vmin=1, vmax=3000, units='km', scale='log', midDate=False, colorBarSize='3%')\n",
    "# Plot location inset\n",
    "height = 3\n",
    "axInset = inset_axes(axes[0], width=height * myOverview.sx/myOverview.sy, height=height, loc=1)\n",
    "myOverview.displayVelForDate(None, ax=axInset, vmin=1, vmax=3000, colorBar=False, scale='log', title='')\n",
    "axInset.plot(*myVelSeries.outline(), color='r')\n",
    "axInset.axis('off')\n",
    "#\n",
    "# Loop over each glacier and plot the flowlines\n",
    "for glacierId in myFlowlines:\n",
    "    # Plot the flowline Match units to the map\n",
    "    myFlowlines[glacierId].plotFlowlineLocations(ax=axes[0], units='km', colorDict=flowlineColors)\n",
    "    myFlowlines[glacierId].plotFlowlineLocations(ax=axes[1], units='km', colorDict=flowlineColors)\n",
    "    # \n",
    "    myFlowlines[glacierId].plotGlacierName(ax=axes[0], units='km', color='w', fontsize=12,fontweight='bold', first=False)\n",
    "    myFlowlines[glacierId].plotGlacierName(ax=axes[1], units='km', color='w', fontsize=12,fontweight='bold', first=False)\n",
    "    #\n",
    "    # Generates points 10km from downstream end of each flowline\n",
    "    points10km = myFlowlines[glacierId].extractPoints(10, None, units='km')\n",
    "    glacierPoints[glacierId] = points10km\n",
    "    for key in points10km:\n",
    "        axes[0].plot(*points10km[key], 'r.')\n",
    "        axes[1].plot(*points10km[key], 'r.')\n",
    "#\n",
    "# Add legend\n",
    "for ax in axes:\n",
    "    # Create a dict of unique labels for legend\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(l, h)) # will overwrite identical entries to produce uniqe values\n",
    "    ax.legend(by_label.values(), by_label.keys(), title='Flowline ID', ncol=2, loc='lower left', fontsize=14)\n",
    "#fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a023145",
   "metadata": {},
   "source": [
    "## Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da823b9",
   "metadata": {},
   "source": [
    "A common function with the velocity date is interpolating data for plotting points or profiles, which can be easily done with the `nisarVelSeries.interp` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Using km\n",
    "vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "#\n",
    "# or units of meters\n",
    "vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xym(), units='m')\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "#\n",
    "# or entirely different coordinate system\n",
    "xytoll = pyproj.Transformer.from_crs(3413, 4326)\n",
    "lat, lon = xytoll.transform(*myFlowlines[glacierId].xym())\n",
    "vx, vy, vv = myVelSeries.interp(lat, lon, sourceEPSG=4326)\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "# Or would prefer an xarray rather than nparray\n",
    "result = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km', returnXR=True)\n",
    "result;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f417c9",
   "metadata": {},
   "source": [
    "## Plot Central Flowlines at Different Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbea223",
   "metadata": {},
   "source": [
    "This example will demonstrate plotting the nominally central flowline ('06') for each of the six years for which there are currently data. This example works for annual data. Minor modifications are needed for the legend if more frequent data are selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowlineId ='06'  # Flowline id to plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))  # Setup plot\n",
    "# Loop over glaciers\n",
    "for glacierId, ax in zip(myFlowlines, axes.flatten()):\n",
    "    # return interpolated values as vx(time index, distance index)\n",
    "    vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "    # loop over each profile by time\n",
    "    for speed, myDate in zip(vv, myVelSeries.time):\n",
    "        ax.plot(myFlowlines[glacierId].distancekm(), speed, label=myDate.year)\n",
    "    # pretty up plot\n",
    "    ax.legend(ncol=2, loc='upper right', fontsize=15)\n",
    "    ax.set_xlabel('Distance (km)', fontsize=18)\n",
    "    ax.set_ylabel('Speed (m/yr)', fontsize=18)\n",
    "    ax.set_title(f'Glacier {glacierId}', fontsize=20)\n",
    "# For other combinations could have \n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648e88f",
   "metadata": {},
   "source": [
    "## Plot Points Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85662a",
   "metadata": {},
   "source": [
    "When the map plots were generated above, a set of points 10-k from the start of each flowline was extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7714a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a01e8b-f64b-48d9-b0de-767a42a22fe2",
   "metadata": {},
   "source": [
    "The time series for each set of points can be plotted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "# Loop over glaciers\n",
    "for glacierId, ax in zip(glacierPoints, axes.flatten()):\n",
    "    # Loop over flowlines\n",
    "    for flowlineId in glacierPoints[glacierId]:\n",
    "        # interpolate to get results vx(time index) for each point\n",
    "        vx, vy, v = myVelSeries.interp(*glacierPoints[glacierId][flowlineId], units='km')\n",
    "        ax.plot(myVelSeries.time, v, marker='o',  linestyle='-', color=flowlineColors[flowlineId],label=f'{flowlineId}')\n",
    "    # pretty up plot\n",
    "    ax.legend(ncol=3, loc='upper right', title='Flowline ID')\n",
    "    ax.set_xlabel('year', fontsize=13)\n",
    "    ax.set_ylabel('Speed (m/yr)', fontsize=13)\n",
    "    ax.set_title(f'Glacier {glacierId}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ff3a3",
   "metadata": {},
   "source": [
    "## Save the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25bbee5",
   "metadata": {},
   "source": [
    "While it is convenient to work the date remotely, its nice to be be able to save the data for further processing.\n",
    "\n",
    "The downloaded subset can be saved in a netcdf and reloaded for to `velSeries` instance for later analysis. \n",
    "\n",
    "Note makes sure the data have been subsetted so only the the subset will be saved (~15MB in this example). If not, the entire Greeland data set will be saved (370GB). \n",
    "\n",
    "Change `saveData` and `reloadData` below to test this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData = True # Set to True to save data\n",
    "if saveData:\n",
    "    myVelSeries.toNetCDF('Glaciers1-2example.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f39f8",
   "metadata": {},
   "source": [
    "Now open open the file and redo a plot from above with the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloadData = True  # Set to True to reload the saved data\n",
    "if reloadData:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))  # Setup plot\n",
    "    myVelCDF = nisar.nisarVelSeries() # Create Series\n",
    "    myVelCDF.readSeriesFromNetCDF('Glaciers1-2example.nc')\n",
    "    #\n",
    "    for glacierId, ax in zip(myFlowlines, axes.flatten()):\n",
    "        # return interpolated values as vx(time index, distance index)\n",
    "        vx, vy, vv = myVelCDF.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "        # loop over each profile by time\n",
    "        for speed, myDate in zip(vv, myVelSeries.time):\n",
    "            ax.plot(myFlowlines[glacierId].distancekm(), speed, label=myDate.year)\n",
    "        # pretty up plot\n",
    "        ax.legend(ncol=2, loc='upper right', fontsize=15)\n",
    "        ax.set_xlabel('Distance (km)', fontsize=18)\n",
    "        ax.set_ylabel('Speed (m/yr)', fontsize=18)\n",
    "        ax.set_title(f'Glacier {glacierId}', fontsize=20)\n",
    "# For other combinations could have \n",
    "    for ax in axes.flatten():\n",
    "        ax.tick_params(axis='both', labelsize=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f8d185",
   "metadata": {},
   "source": [
    "# Visualizing with widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9527a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dotson - requires a shapefile to plot that has been formated to lists of lat/lon pairs\n",
    "# formatting for shapefile: Wcoords2 = [[Wlat2[i],Wlon2[i]] for i in range(Wlat2.shape[0])]\n",
    "\n",
    "add\n",
    "\n",
    "center = [-74.23, -112.00]\n",
    "zoom = 8\n",
    "\n",
    "mapdt1 = '2022-02-01'\n",
    "\n",
    "global dc, start, end, file, lon_l, lat_l, lon_r, lat_r, dt\n",
    "\n",
    "# Pick date\n",
    "start = widgets.DatePicker(disabled=False)\n",
    "end = widgets.DatePicker(disabled=False)\n",
    "# dt = widgets.DatePicker(disabled=False)\n",
    "\n",
    "# Select from map\n",
    "draw_control = DrawControl(rectangle={}, polyline={'shapeOptions': {'color': '#fca45d','weight': 1,'opacity': 1.0}}, \n",
    "                 polygon={}, circlemarker={})\n",
    "\n",
    "feature_collection = {\n",
    "    'type': 'FeatureCollection',\n",
    "    'features': []}\n",
    "\n",
    "def handle_draw(self, action, geo_json):\n",
    "    \"\"\"Do something with the GeoJSON when it's drawn on the map\"\"\"    \n",
    "    feature_collection['features'].append(geo_json)\n",
    "\n",
    "draw_control.on_draw(handle_draw)\n",
    "\n",
    "m = Map(basemap=basemap_to_tiles(basemaps.NASAGIBS.ModisAquaTrueColorCR, mapdt1),center=center,zoom=zoom)\n",
    "\n",
    "line1 = Polyline(locations=Wcoords1, weight=1,color='red' , fill=False)\n",
    "line2 = Polyline(locations=Wcoords2, weight=1,color='red' , fill=False)\n",
    "m.add_layer(line1)\n",
    "m.add_layer(line2)\n",
    "\n",
    "# Provides drawing control - can add something that keeps what is drawn...\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# Can upload a shapefile\n",
    "file = widgets.FileUpload(accept='.shp', multiple=False)\n",
    "\n",
    "# Specify a bounding box\n",
    "lon_l = widgets.FloatText(description=\"lon\")\n",
    "lat_l = widgets.FloatText(description=\"lat\")\n",
    "lon_r = widgets.FloatText(description=\"lon\")\n",
    "lat_r = widgets.FloatText(description=\"lat\")\n",
    "\n",
    "AppLayout(header = VBox([HTML(\"<h1>Select area (time and space)</h1>\"),\n",
    "                 HBox([Label(\"Start Date:\"), start, \n",
    "                       Label(\"End Date:\"), end]),\n",
    "                 # HBox([Label(\"Map Date:\"), mapdt1])\n",
    "                        ]),\n",
    "             center = m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dfcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88fbd5c5",
   "metadata": {},
   "source": [
    "We are now going to revisit ATM and ICESat-2 one last time. We have conducted a bias assessment between the two lidars, so now we are going to look at how the land ice heights change over time.\n",
    "\n",
    "First let's take a look at ATM data from previous years. The CSV file we are going to use is pre-processed L2 data for 2011-2018, much like the data from 2019. These flights are slightly east of the 2019 flight, which was adjusted to better align with ICESat-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79713ddb-f8a6-4384-ad0c-5656f0078aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# Read in the ATM CSV file\n",
    "atm_2011_2018 = pd.read_csv('ILATM2_2011_2019_v3.csv')\n",
    "lltoxy = pyproj.Transformer.from_crs(4326, 3413)\n",
    "\n",
    "#%matplotlib widget\n",
    "# Loop through the valid years and plot surface height\n",
    "years = ['2011', '2013', '2014', '2015', '2016', '2018']\n",
    "for i,year in enumerate(years):\n",
    "    lat = atm_2011_2018['Latitude_'+year]\n",
    "    elev = atm_2011_2018['elev_'+year]\n",
    "    axes[1].plot(lat, elev, label=year)\n",
    "#\n",
    "#\n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=axes[0], labelFontSize=12, plotFontSize=9, titleFontSize=14, \n",
    "                              vmin=1, vmax=3000, units='m', scale='log', midDate=False, colorBarSize='3%', colorBarPosition='bottom')\n",
    "axes[0].axis('off')\n",
    "lltoxy = pyproj.Transformer.from_crs(4326, 3413)\n",
    "for i, year in enumerate(years[3:]):\n",
    "    lat, lon = atm_2011_2018['Latitude_'+year], atm_2011_2018['Longitude_'+year]\n",
    "    x, y = lltoxy.transform(lat, lon)\n",
    "    axes[0].plot(x, y, 'k', linewidth=2)\n",
    "    v =  myVelSeries.interp(lat, lon, sourceEPSG=4326, returnXR=True).sel(time=datetime(int(year), 6, 1), method='nearest').sel(band='vv').data\n",
    "    axes[2].plot(lat, v, label=year)\n",
    "#\n",
    "axes[1].set_xlabel('latitude')\n",
    "axes[1].set_ylabel('elevation [m]')\n",
    "axes[1].legend()\n",
    "axes[2].set_xlabel('latitude')\n",
    "axes[2].set_ylabel('Speed [m/yr]')\n",
    "axes[2].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71a480-3be8-4d6c-b998-a61fd8fe90b7",
   "metadata": {},
   "source": [
    "It's obvious that there was significant ice loss between 2011 and 2013. The other years appear to be much more variable - let's focus in on the trough at 69.203N..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bae601-65a8-4ddb-b67f-742318e4fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the latitude bounds of the surface trough\n",
    "lat_bounds = [69.1982, 69.2113]\n",
    "\n",
    "# 2013 has the longest streak of data over this region. We are going to downscale the other years to its length.\n",
    "lat_2013 = atm_2011_2018['Latitude_2013'][(atm_2011_2018['Latitude_2013']>lat_bounds[0]) & (atm_2011_2018['Latitude_2013']<lat_bounds[1])]\n",
    "\n",
    "# First, downscale the 2011 data to 2013 resolution\n",
    "lat = atm_2011_2018['Latitude_2011']\n",
    "elev = atm_2011_2018['elev_2011'][(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "        \n",
    "lat = lat[(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "spl = splrep(lat[::-1], elev[::-1], s=0)\n",
    "slp_2011 = splev(lat_2013, spl, der=0)\n",
    "\n",
    "# Calculate ice loss relative to 2011\n",
    "delta_h = [0]\n",
    "std_h = [0]\n",
    "for i,year in enumerate(years[1:]): # Start loop at 2013 (2012 has no data)\n",
    "    if year != 2013: # Downscale other years to 2013 resolution\n",
    "        lat = atm_2011_2018['Latitude_'+year]\n",
    "        elev = atm_2011_2018['elev_'+year][(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "        \n",
    "        # Downscale the data with splines\n",
    "        lat = lat[(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "        spl = splrep(lat[::-1], elev[::-1], s=0)\n",
    "        spl_year = splev(lat_2013, spl, der=0)\n",
    "        \n",
    "        # Now calculate the difference relative to 2011\n",
    "        delta_h.append((spl_year - slp_2011).mean())\n",
    "        std_h.append((spl_year - slp_2011).std())\n",
    "    else:\n",
    "        lat = atm_2011_2018['Latitude_'+year]\n",
    "        elev = atm_2011_2018['elev_'+year][(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "        \n",
    "        # Calculate the difference relative to 2011\n",
    "        delta_h.append((elev[::-1] - slp_2011).mean())\n",
    "        std_h.append((spl_year - slp_2011).std())\n",
    "    \n",
    "%matplotlib widget\n",
    "plt.errorbar(years, delta_h, yerr=std_h, marker='.', markersize=12, capsize=4)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('$\\Delta$ h [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dc30c-d807-441b-804f-0c4ab1bd07f7",
   "metadata": {},
   "source": [
    "Ta-da!! Using a few operations, we were able to use ATM data to derive a rough time series of ice sheet elevation change over Jakobshavan.\n",
    "\n",
    "We're going to try and do the same thing, but for ICESat-2. Because it was launched in late-2018, we are going to try and grab interseasonal measurements from RGT 338 for 2019-2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f1417-d575-4c6c-affc-0834425df49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to go through the icepyx routine again!\n",
    "import icepyx as ipx\n",
    "\n",
    "# Specifying the necessary icepyx parameters\n",
    "short_name = 'ATL06'\n",
    "lat_bounds = [69.1982, 69.2113]\n",
    "spatial_extent = [-50, 69.1982, -48.5, 69.2113] # KML polygon centered on Jakobshavan\n",
    "date_range = ['2019-04-01', '2021-12-30']\n",
    "rgts = ['338'] # IS-2 RGT of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e314d-f9c4-4014-824a-88ac238e70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Query object\n",
    "region = ipx.Query(short_name, spatial_extent, date_range, tracks=rgts)\n",
    "\n",
    "# Show the available granules\n",
    "region.avail_granules(ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24552df8-9bba-4754-9e16-7aa70ee99aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Earthdata credentials\n",
    "uid = 'zhfair'\n",
    "email = 'zhfair@umich.edu'\n",
    "region.earthdata_login(uid, email)\n",
    "\n",
    "# Order the granules\n",
    "region.order_granules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00221419-857c-43a9-a1f6-589691d250ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jovyan/website2022/book/tutorials/DataIntegration/'\n",
    "region.download_granules(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86405a-c311-4264-828a-d99edf2cf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "# Iterate through the files to grab elevation, and derive elevation differences relative to April 2019\n",
    "files = ['processed_' + granule for granule in region.avail_granules(ids=True)[0]]\n",
    "\n",
    "# Get the initial data from April 2019\n",
    "with h5py.File(files[0]) as f:\n",
    "    elev_42019 = f['gt2l/land_ice_segments/h_li'][:]\n",
    "    lat_42019 = f['gt2l/land_ice_segments/latitude'][:]\n",
    "    plt.plot(lat_42019, elev_42019, label=files[0][16:24])\n",
    "\n",
    "delta_h = [0]\n",
    "std_h = [0]\n",
    "for file in files[1:]:\n",
    "    try:\n",
    "        with h5py.File(file) as f:\n",
    "            cloud_flag = np.mean(f['gt2l/land_ice_segments/geophysical/cloud_flg_asr'][:])\n",
    "            # Filter out cloudy scenes\n",
    "            if cloud_flag < 2:\n",
    "                lat = f['gt2l/land_ice_segments/latitude'][:]\n",
    "                elev = f['gt2l/land_ice_segments/h_li'][:]\n",
    "                date = file[16:24] # Get date of IS-2 overpass\n",
    "                \n",
    "                # Find the difference relative to April 2019\n",
    "                delta_h.append(np.mean(elev - elev_42019))\n",
    "                std_h.append(np.std(elev - elev_42019))\n",
    "                \n",
    "                # Plot the elevation data\n",
    "                plt.plot(lat, elev, label=date)\n",
    "    except:\n",
    "            print('no data')\n",
    "            \n",
    "plt.xlabel('latitude')\n",
    "plt.ylabel('elevation [m]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2b388-06c2-4587-bd45-2eb96d99972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ice sheet change time series\n",
    "dates = ['20190420','20200117', '20200717', '202110115']\n",
    "%matplotlib widget\n",
    "plt.errorbar(dates, delta_h, yerr=std_h, marker='.', markersize=12, capsize=4)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('$\\Delta$ h [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fedff-0cb4-4a5b-9633-1ed4d7b83d56",
   "metadata": {},
   "source": [
    "There we go! We lost some data due to cloud cover, and the mean change has some spread, but the ICESat-2 data continues to show a downward trend that was suggested by ATM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef0873",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    " üéâ Congratulations! You've completely this tutorial and have seen how we can add  notebook can be formatted, and how to create interactive map visualization with ipyleaflet.\n",
    " \n",
    "\n",
    "```{note}\n",
    "You may have noticed Jupyter Book adds some extra formatting features that do not necessarily render as you might expect when *executing* a noteook in Jupyter Lab. This \"admonition\" note is one such example.\n",
    "```\n",
    "\n",
    ":::{warning}\n",
    "Jupyter Book is very particular about [Markdown header ordering](https://jupyterbook.org/structure/sections-headers.html?highlight=headers#how-headers-and-sections-map-onto-to-book-structure) to automatically create table of contents on the website. In this tutorial we are careful to use a single main header (#) and sequential subheaders (#, ##, ###, etc.)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4ec44",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "To further explore the topics of this tutorial see the following detailed documentation:\n",
    "\n",
    "* [Jupyter Book rendering of .ipynb notebooks](https://jupyterbook.org/file-types/notebooks.html)\n",
    "* [Jupyter Book guide on writing narrative content](https://jupyterbook.org/content/index.html)\n",
    "* [ipyleaflet documentation](https://ipyleaflet.readthedocs.io)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
