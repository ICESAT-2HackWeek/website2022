{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74079890",
   "metadata": {},
   "source": [
    "# Data integration with ICESat-2 - Part II\n",
    "\n",
    "__Credits__\n",
    "* Zach Fair\n",
    "* Ian Joughin\n",
    "* Tasha Snow\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "**Goals**\n",
    "- Access NSIDC data sets and acquire IS-2 using icepyx\n",
    "- Analyze point and raster data together with IS-2\n",
    "- Advanced visualizations of multiple datasets\n",
    "```\n",
    "\n",
    "For this tutorial, feel free to run the code along with us as we live code by downsizing the zoom window and splitting your screen (or using two screens). Or you can simply watch the zoom walkthrough. Don't worry if you fall behind on the code. The notebook is standalone and you can easily run the code at your own pace another time to catch anything you missed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a785896-0259-4265-b83f-8071063008c4",
   "metadata": {},
   "source": [
    "## Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d87b1",
   "metadata": {},
   "source": [
    "### GrIMP libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999cdb66-e0be-4594-9e83-a311f2653254",
   "metadata": {},
   "source": [
    "This notebook makes use of two packages for working with data from the Greenland Ice Mapping Project (GrIMP) that are stored remotely at NSIDC. These packages are:\n",
    "- [grimpfunc](https://github.com/fastice/grimpfunc): Code for searching NISDC catalog for GrIMP data, subsetting the data, and working with flowlines.\n",
    "- [nisardev](https://github.com/fastice/nisardev): Classes for working with velocity and image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nisardev as nisar\n",
    "import os\n",
    "import matplotlib.colors as mcolors\n",
    "import grimpfunc as grimp\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import importlib\n",
    "import requests\n",
    "import pyproj\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import panel\n",
    "from dask.diagnostics import ProgressBar\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import ipyleaflet\n",
    "from ipyleaflet import Map,GeoData,LegendControl,LayersControl,Rectangle,basemaps,basemap_to_tiles,TileLayer,SplitMapControl,Polygon,Polyline\n",
    "\n",
    "import ipywidgets\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "ProgressBar().register()\n",
    "panel.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f0147-ae79-41ae-aa23-d9f4850ff90c",
   "metadata": {},
   "source": [
    "Sometimes the above cell will return an error about a missing module. If this happens, try restarting the kernel and re-running the above cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e0668",
   "metadata": {},
   "source": [
    "## NSIDC Login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5832bd",
   "metadata": {},
   "source": [
    "For remote access to the velocity data at NSIDC, run these cells to login with your NASA EarthData Login (see  [NSIDCLoginNotebook](https://github.com/fastice/GRiMPNotebooks/blob/master/NSIDCLoginNotebook.ipynb) for further details). These cells can skipped if all data are being accessed locally. First define where the cookie files need for login are saved. \n",
    "\n",
    "These environment variables are used by GDAL for remote data access via [vsicurl](https://gdal.org/user/virtual_file_systems.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b903d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dict(GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/.grimp_download_cookiejar.txt'),\n",
    "            GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/.grimp_download_cookiejar.txt'))\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37b0b5",
   "metadata": {},
   "source": [
    "Now enter credentials, which will create the cookie files above as well as *.netrc* file with the credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e6cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm ~/.netrc\n",
    "myLogin = grimp.NASALogin()\n",
    "myLogin.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbfeb9-ca2a-4cc2-a1c3-fe6f28d85fc0",
   "metadata": {},
   "source": [
    "## Load glacier termini\n",
    "In this section we will read shapefiles stored remotely at NSIDC. \n",
    "\n",
    "The first step is to get the *urls* for the files in the NSDIC catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5ce9b-ed75-44d8-b594-61c2ed6447fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "myTerminusUrls = grimp.cmrUrls(mode='terminus')  # mode image restricts search to the image products\n",
    "myTerminusUrls.initialSearch();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17c0fc-7f84-49eb-820d-4f856a7170d5",
   "metadata": {},
   "source": [
    "Using the ```myTerminusUrls.getURLS()``` method to return the urls for the shape files, read in termini and store in a dict, `myTermini`, by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8cfa3-dcdb-4cb0-b093-0dacfb25a0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myTermini = {}\n",
    "for url in myTerminusUrls.getURLS():\n",
    "    year = os.path.basename(url).split('_')[1]  # Extract year from name\n",
    "    myTermini[year] = gpd.read_file(f'/vsicurl/&url={url}')  # Add terminus to data frame\n",
    "    print(f'/vsicurl/&url={url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045402e-7160-4347-a2b6-57b13c971c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "myTermini['2000'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a64576-9d47-480e-afd1-e35a8778dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in myTermini:\n",
    "    myTermini[year] = myTermini[year].to_crs('EPSG:4326') # to lat/lon\n",
    "myTermini['2000'].plot()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a51801-29e3-4b7e-9329-2a51f5702e8e",
   "metadata": {},
   "source": [
    "## Flowlines\n",
    "In this section we will work with a two collections of flowlines from [Felikson et al., 2020](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112). The full set of flowines for all of Greenland can be downloaded from [Zenodo](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112).  \n",
    "\n",
    "The data are stored in the subdirectory *shpfiles* and can be read as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66b747-ae39-4f9f-981f-92a15edc7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "glaciers = {}\n",
    "for i in range(1, 3):\n",
    "    glaciers[f'000{i}'] = gpd.read_file(f'shpfiles/glacier000{i}.shp').to_crs('EPSG:4326');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a03d41-d3c1-4cdc-935f-a1fa99d6f706",
   "metadata": {},
   "source": [
    "Note this is the same procedure as for the termini, except we are using a filename instead of a url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f1e1b-ebfa-45f9-88fa-7de697ebaab1",
   "metadata": {},
   "source": [
    "## ICESat-2 ATL06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bebe3-2f84-4a7d-883e-983dac068524",
   "metadata": {},
   "source": [
    "Now that we have the flowlines and termini, we are going to plot these alongside the ICESat-2 and ATM tracks. Remember the data we worked with yesterday? Here we are going to use that again for this mappinp project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7456d9-e420-488c-b75d-4e4813cb2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ICESat-2 data\n",
    "is2_file = 'processed_ATL06_20190420093051_03380303_005_01_full.h5'\n",
    "with h5py.File(is2_file, 'r') as f:\n",
    "    is2_gt2r = pd.DataFrame(data={'lat': f['gt2r/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2r/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2r/land_ice_segments/h_li'][:]}) # Central weak beam\n",
    "    is2_gt2l = pd.DataFrame(data={'lat': f['gt2l/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2l/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2l/land_ice_segments/h_li'][:]}) # Central strong beam\n",
    "    \n",
    "# Load the ATM data\n",
    "atm_file = 'ILATM2_20190506_151600_smooth_nadir3seg_50pt.csv'\n",
    "atm_l2 = pd.read_csv(atm_file)\n",
    "\n",
    "# Look only at the nadir track\n",
    "atm_l2 = atm_l2[atm_l2['Track_Identifier']==0]\n",
    "\n",
    "# Change the longitudes to be consistent with ICESat-2\n",
    "atm_l2['Longitude(deg)'] -= 360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f05370-cace-4bb1-bbed-125d3b4c3e77",
   "metadata": {},
   "source": [
    "Next the data are subsetted to the range of ATM latitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d944aec-7519-48d3-af04-5bb4e6d88fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the ICESat-2 data to the ATM latitudes\n",
    "is2_gt2r = is2_gt2r[(is2_gt2r['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2r['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "is2_gt2l = is2_gt2l[(is2_gt2l['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2l['lat']>atm_l2['Latitude(deg)'].min())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72b472-bcda-4c1b-b912-076221144020",
   "metadata": {},
   "source": [
    "## Plot lidar tracks, flowlines and termini\n",
    "Checking everything looks good before analysis by plotting the data over imagery rendered via ipyleaflet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c22d9-6b94-4980-bc83-7eb9fe6c2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "center = [69.2, -50]\n",
    "zoom = 8\n",
    "\n",
    "mapdt1 = '2019-05-06'\n",
    "\n",
    "m = Map(basemap=basemap_to_tiles(basemaps.NASAGIBS.ModisAquaTrueColorCR, mapdt1),center=center,zoom=zoom)\n",
    "\n",
    "gt2r_line = Polyline(\n",
    "    locations=[\n",
    "        [is2_gt2r['lat'].min(), is2_gt2r['lon'].max()],\n",
    "        [is2_gt2r['lat'].max(), is2_gt2r['lon'].min()]\n",
    "    ],\n",
    "    color=\"green\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(gt2r_line)\n",
    "\n",
    "gt2l_line = Polyline(\n",
    "    locations=[\n",
    "        [is2_gt2l['lat'].min(), is2_gt2l['lon'].max()],\n",
    "        [is2_gt2l['lat'].max(), is2_gt2l['lon'].min()]\n",
    "    ],\n",
    "    color=\"green\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(gt2l_line)\n",
    "\n",
    "atm_line = Polyline(\n",
    "    locations=[\n",
    "        [atm_l2['Latitude(deg)'].min(), atm_l2['Longitude(deg)'].max()],\n",
    "        [atm_l2['Latitude(deg)'].max(), atm_l2['Longitude(deg)'].min()]\n",
    "    ],\n",
    "    color=\"orange\" ,\n",
    "    fill=False\n",
    ")\n",
    "m.add_layer(atm_line)\n",
    "legend = LegendControl({'ICESat-2':'green','ATM':'orange'}, name = 'Lidar', position=\"topleft\")\n",
    "m.add_control(legend)\n",
    "\n",
    "tLegend = {}\n",
    "for i in range(3, 5):\n",
    "    for key in myTermini:\n",
    "        # Create list of lat/lon pairs\n",
    "        r = lambda: random.randint(0,255)\n",
    "        cr = '#%02X%02X%02X' % (r(),r(),r())\n",
    "        term_coords = [[[xy[1],xy[0]] for xy in geom.coords] for geom in myTermini[key].loc[myTermini[key]['Glacier_ID'] == i].geometry]\n",
    "        term_data = Polyline(locations=term_coords, weight=2, color=cr, fill=False)\n",
    "        m.add_layer(term_data)\n",
    "        tLegend[key] = cr\n",
    "legend = LegendControl(tLegend, name=\"Terminus\", position=\"topright\")\n",
    "m.add_control(legend)\n",
    "\n",
    "for glacier in glaciers:\n",
    "    gl_data = GeoData(geo_dataframe = glaciers[glacier],\n",
    "                   style={'color': 'black', 'weight':1.0},\n",
    "                   name = f'{glacier}')\n",
    "    m.add_layer(gl_data)\n",
    "    \n",
    "m.add_control(LayersControl())\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebc076",
   "metadata": {},
   "source": [
    "## Plot Flowines Using Remote Greenland Ice Mapping Project Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5a0ae",
   "metadata": {},
   "source": [
    "ICESat measures thinning and thickening, which often is driven by changes in the flow of the glacier. \n",
    "\n",
    "Thus, to understand whether elevation change is driven by ice dynamics or changes in surface mass balance (net melting and snowfall), we need to look at how the flow velocity is evolving with time.\n",
    "\n",
    "This section demonstrates how Greenland Ice Mapping Project (GrIMP) [data](https://nsidc.org/data/measures/grimp) can be remotely accessed. As an example, will  used flowlines from [Felikson et al., 2020](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112) distributed via [Zenodo](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112).  \n",
    "\n",
    "Here we will use:\n",
    "- ```grimp.Flowlines``` to read, manipulate, and store the flowlines data;\n",
    "- ```grimp.cmrUrls``` to search the NISDC catalog; and\n",
    "- ```nisar.nisarVelSeries``` to build a time-dependent stack of velocity data, which be plotted, interpolated etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143e344",
   "metadata": {},
   "source": [
    "### Read Shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beeaa2f",
   "metadata": {},
   "source": [
    "In the examples presented here we will use glaciers 1 & 2  in the Felikson data base, [Felikson et al., 2020](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112), which were retrieved from [Zenodo](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112). \n",
    "\n",
    "Each glacier's flowlines are used to create `grimp.Flowlines` instances, which are saved in a dictionary, `myFlowlines` with glacier id: '0001' and  '0002'. \n",
    "\n",
    "Each `Flowlines` read a set of flowlines for each glacier and stores in a dictionary of `myFlowlines.flowlines`. The code to do this looks something like:\n",
    "\n",
    "```\n",
    "    flowlines = {}\n",
    "    shapeTable = gpd.read_file(shapefile)\n",
    "    for index, row in shapeTable.iterrows():  # loop over features\n",
    "        fl = {}  # New Flowline\n",
    "        fl['x'], fl['y'] = np.array([c for c in row['geometry'].coords]).transpose()\n",
    "        fl['d'] = computeDistance(fl['x'], fl['y'])\n",
    "        flowlines[row['flowline']] = fl\n",
    " ```\n",
    "For further detail, see the full [class definition](https://github.com/fastice/grimpfunc/blob/master/grimpfunc/Flowlines.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32b76b",
   "metadata": {},
   "source": [
    "To limit the plots to the downstream regions, the flowlines are all truncated to a `length` of 50km. \n",
    "\n",
    "Within each myFlowines entry (a `grimp.Flowlines` instance), the individual flowlines are maintained as a dictionary `myFlowlines['glacierId'].flowlines`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea6570",
   "metadata": {},
   "outputs": [],
   "source": [
    "myShapeFiles = [f'./shpfiles/glacier000{i}.shp' for i in range(1, 3)] # Build list of shape file names\n",
    "myFlowlines = {x[-8:-4]: grimp.Flowlines(shapefile=x, name=x[-8:-4], length=50e3) for x in myShapeFiles} \n",
    "myFlowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d1864",
   "metadata": {},
   "source": [
    "Each flowline is indexed as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6399d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFlowlines['0001'].flowlines.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46f028",
   "metadata": {},
   "source": [
    "The data for the flow line is simple, just `x`, `y` polar stereographic coordinates (EPSG=3413) and the distance, `d`, from the start of the flowline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00465a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFlowlines['0001'].flowlines['03'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34833585",
   "metadata": {},
   "source": [
    "These coordinates for a given index can be returned as `myFlowlines['0001'].xym(index='03')` or `myFlowlines['0001'].xykm(index='03')` depending on whether m or km are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8544a83",
   "metadata": {},
   "source": [
    "<a id='bounds'></a>The area of interest can be defined as the union of the bounds for all of the flowlines computed as shown below along with the unique set of flowline IDs across all glaciers. We will use the bounding box [below](#subsettext) to subset the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myBounds = {'minx': 1e9, 'miny': 1e9, 'maxx': -1e9, 'maxy': -1e9}  # Initial bounds to force reset\n",
    "flowlineIDs = []  # \n",
    "for myKey in myFlowlines:\n",
    "    # Get bounding box for flowlines\n",
    "    flowlineBounds = myFlowlines[myKey].bounds\n",
    "    # Merge with prior bounds\n",
    "    myBounds = myFlowlines[myKey].mergeBounds(myBounds, flowlineBounds)\n",
    "    # Get the flowline ids\n",
    "    flowlineIDs.append(myFlowlines[myKey].flowlineIDs())\n",
    "# Get the unique list of flowlines ids (used for legends later)\n",
    "flowlineIDs = np.unique(flowlineIDs)\n",
    "print(myBounds)\n",
    "print(flowlineIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2982f1",
   "metadata": {},
   "source": [
    "### Search Catalog for Velocity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8791ad4",
   "metadata": {},
   "source": [
    "We now need to locate velocity data from the GrIMP data set. For this exercise, we will focus on the annual velocity maps of Greenland. To do this, we will use the [grimp.cmrUrls](https://github.com/fastice/grimpfunc/blob/master/grimpfunc/cmrUrls.py) tool, which will do a GUI based search of NASA's Common Metadata Repository ([CMR](https://earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/cmr)). Search parameters can be passe directly to `initialSearch` method to perform the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b0f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "myUrls = grimp.cmrUrls(mode='subsetter', verbose=True)  # nisar mode excludes image and tsx products and allows only one product type at a time\n",
    "myUrls.initialSearch(product='NSIDC-0725')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c5810",
   "metadata": {},
   "source": [
    "The `verbose` flag causes the CMR search string to be printed. The search basically works by a) reading the parameters from the search panel (e.g., product, date, etc) and creating a search string, which returns the search result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccff8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://cmr.earthdata.nasa.gov/search/granules.json?provider=NSIDC_ECS&sort_key[]=start_date&sort_key[]='\n",
    "                        'producer_granule_id&scroll=false&page_size=2000&page_num=1&short_name=NSIDC-0725&version=3&temporal[]='\n",
    "                        '2000-01-01T00:00:01Z,2022-03-10T00:23:59&bounding_box[]=-75.00,60.00,-5.00,82.00&producer_granule_id[]='\n",
    "                        '*&options[producer_granule_id][pattern]=true')\n",
    "search_results = response.json()\n",
    "search_results;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd07a87",
   "metadata": {},
   "source": [
    "Under the hood, the `cmrUrls` code can filter the json to get a list of urls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06088dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "myUrls.getURLS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4182d472",
   "metadata": {},
   "source": [
    "### Load the Velocity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735892d",
   "metadata": {},
   "source": [
    "GrIMP produces full Greenland velocity maps. Collectively, there are more than 400 full Greenland maps, totalling several hundred GB of data, which may be more than a user interested in a few glaciers wants to download and store on their laptop.  Fortunately using Cloud Optimized Geotiffs only the data are actually needed are downloaded. As a quick review, COGs have the following properties:\n",
    "\n",
    "- All the metadata is at the beginning of the file, allowing a single read to obtain the layout. \n",
    "- The data are tiled (i.e., stored as a series of blocks like a checkerboard) rather than as a line-by-line raster.  \n",
    "- A consistent set of overview images (pyramids) are stored with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd18231",
   "metadata": {},
   "source": [
    "While the velocity data are stored as multiple files at NSIDC, they can all be combined into a single [nisarVelSeries](https://github.com/fastice/nisardev/blob/main/nisardev/nisarVelSeries.py) instance, which has the following properties:\n",
    "\n",
    "- Built on Xarray,\n",
    "- Dask (parallel operations),\n",
    "- Local and remote subsetting (Lazy Opens), and\n",
    "- Subsets can be saved for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e20a8",
   "metadata": {},
   "source": [
    "Before loading the data, we must setup the filename template for the multi-band velocity products. \n",
    "\n",
    "Specifically, we must put a '\\*' where the band identifier would go and remove the trailing '.tif' extension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14078020",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlNames = [x.replace('vv','*').replace('.tif','') for x in myUrls.getCogs()] # getCogs filters to ensure tif products\n",
    "urlNames[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97746b5c-0b34-4a34-84c4-8c0fdb4747eb",
   "metadata": {},
   "source": [
    "We can now create a `nisarVelocitySeries` object, which will create a large time series stack with all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03441cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries = nisar.nisarVelSeries() # Create Series\n",
    "myVelSeries.readSeriesFromTiff(urlNames, url=True, readSpeed=False)  # readSpeed=False computes speed from vx, vy rather than downloading\n",
    "myVelSeries.xr  # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57083baf",
   "metadata": {},
   "source": [
    "For the annual data set, this step produces a ~7GB data sets, which expands to 370GB for the full 6-12-day data set. \n",
    "\n",
    "<a id='subsettext'></a>To avoid downloading unnessary data, the data can be subsetted using the bounding box we created [above](#bounds) from the flowlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries.subSetVel(myBounds) # Apply subset\n",
    "myVelSeries.subset # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06753a08",
   "metadata": {},
   "source": [
    "The volume of the data set is now a far more manageable ~15MB, which is still located in the archive.  \n",
    "\n",
    "With dask, operations can continue without downloading, until the data are finally needed to do something (e.g., create a plot). \n",
    "\n",
    "If lots of operations are going to occur, however, it is best to download the data upfront. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries.loadRemote() # Load the data to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7dfc9",
   "metadata": {},
   "source": [
    "### Overview Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3526f28-bfe8-475a-b28b-2f2573cab62d",
   "metadata": {},
   "source": [
    "In the above, we picked out a small region for Greenland and downloaded a full-res data series. But in some cases, we may want the full image at reduced resolution (e.g., for an overview map). \n",
    "\n",
    "Here we can take advantage of the overviews to pull a single velocity map at reduced resolution (`overviewLevel=3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "urlNames[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "myOverview = nisar.nisarVelSeries() # Create Series\n",
    "myOverview.readSeriesFromTiff([urlNames[-1]], url=True, readSpeed=False, overviewLevel=3)  # readSpeed=False computes speed from vx, vy rather than downloading\n",
    "myOverview.loadRemote()\n",
    "myOverview.xr  # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddffbac1",
   "metadata": {},
   "source": [
    "### Display Flowlines and Velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a7d16",
   "metadata": {},
   "source": [
    "<a id='mapplot'></a>In the next cell, we put the above pieces together:\n",
    "- Display speed with linear and log color bars,\n",
    "- Use the overview image for an inset.\n",
    "- Plot the flowline line locations.\n",
    "- For a later [plot](#points), extract a point 10-km along each flowline using `myFlowlines[glacierId].extractPoints(10, None, units='km')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3962581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up figure and axis\n",
    "#%matplotlib inline\n",
    "fig, axes = plt.subplots(1, 2, figsize=(21, 12))\n",
    "\n",
    "# Create a dictionary for accumulating glacier points\n",
    "glacierPoints = {}\n",
    "# generate a color dict that spans all flowline ids, using method from a flowline instance\n",
    "flowlineColors = list(myFlowlines.values())[0].genColorDict(flowlineIDs=flowlineIDs)\n",
    "# Plot velocity maps\n",
    "# Saturate at 2000 m/yr to preserve slow detail\n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=axes[0], labelFontSize=12, plotFontSize=9, titleFontSize=14, \n",
    "                              vmin=0, vmax=2000, units='km', scale='linear', colorBarSize='3%') \n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=axes[1], labelFontSize=12, plotFontSize=9, titleFontSize=14, \n",
    "                              vmin=1, vmax=3000, units='m', scale='log', midDate=False, colorBarSize='3%')\n",
    "# Plot location inset\n",
    "height = 3\n",
    "axInset = inset_axes(axes[0], width=height * myOverview.sx/myOverview.sy, height=height, loc=1)\n",
    "myOverview.displayVelForDate(None, ax=axInset, vmin=1, vmax=3000, colorBar=False, scale='log', title='')\n",
    "axInset.plot(*myVelSeries.outline(), color='r')\n",
    "axInset.axis('off')\n",
    "#\n",
    "# Loop over each glacier and plot the flowlines\n",
    "for glacierId in myFlowlines:\n",
    "    # Plot the flowline Match units to the map\n",
    "    myFlowlines[glacierId].plotFlowlineLocations(ax=axes[0], units='km', colorDict=flowlineColors)\n",
    "    myFlowlines[glacierId].plotFlowlineLocations(ax=axes[1], units='m', colorDict=flowlineColors)\n",
    "    # \n",
    "    myFlowlines[glacierId].plotGlacierName(ax=axes[0], units='km', color='w', fontsize=12,fontweight='bold', first=False)\n",
    "    myFlowlines[glacierId].plotGlacierName(ax=axes[1], units='m', color='w', fontsize=12,fontweight='bold', first=False)\n",
    "    # Generates points 10km from downstream end of each flowline\n",
    "    points10km = myFlowlines[glacierId].extractPoints(10, None, units='km')\n",
    "    glacierPoints[glacierId] = points10km\n",
    "    for key in points10km:\n",
    "        axes[0].plot(*points10km[key], 'r.')\n",
    "#\n",
    "# Add legend\n",
    "for ax in axes:\n",
    "    # Create a dict of unique labels for legend\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(l, h)) # will overwrite identical entries to produce unique values\n",
    "    ax.legend(by_label.values(), by_label.keys(), title='Flowline ID', ncol=2, loc='lower left', fontsize=14)\n",
    "#fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a023145",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da823b9",
   "metadata": {},
   "source": [
    "A common function with the velocity date is interpolating data for plotting points or profiles, which can be easily done with the `nisarVelSeries.interp` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6343d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using km\n",
    "vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "\n",
    "# or units of meters\n",
    "vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xym(), units='m')\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "# or entirely different coordinate system\n",
    "xytoll = pyproj.Transformer.from_crs(3413, 4326)\n",
    "lat, lon = xytoll.transform(*myFlowlines[glacierId].xym())\n",
    "vx, vy, vv = myVelSeries.interp(lat, lon, sourceEPSG=4326)\n",
    "print(vx.shape, vx[0, 100], vy[0, 100], vv[0, 100])\n",
    "# Or would prefer an xarray rather than nparray\n",
    "result = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km', returnXR=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f417c9",
   "metadata": {},
   "source": [
    "### Plot Central Flowlines at Different Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbea223",
   "metadata": {},
   "source": [
    "This example will demonstrate plotting the nominally central flowline ('06') for each of the six years for which there are currently data. While we are using flow lines here, any profile data could be used (e.g., a flux gate). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowlineId ='06'  # Flowline id to plot\n",
    "fig, axes = plt.subplots(np.ceil(len(myFlowlines)/4).astype(int), 2, figsize=(16, 8))  # Setup plot\n",
    "# Loop over glaciers\n",
    "for glacierId, ax in zip(myFlowlines, axes.flatten()):\n",
    "    #\n",
    "    # return interpolated values as vx(time index, distance index)\n",
    "    vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "    #\n",
    "    # loop over each profile by time\n",
    "    for speed, myDate in zip(vv, myVelSeries.time):\n",
    "        ax.plot(myFlowlines[glacierId].distancekm(), speed, label=myDate.year)\n",
    "    #\n",
    "    # pretty up plot\n",
    "    ax.legend(ncol=2, loc='upper right', fontsize=15)\n",
    "    ax.set_xlabel('Distance (km)', fontsize=18)\n",
    "    ax.set_ylabel('Speed (m/yr)', fontsize=18)\n",
    "    ax.set_title(f'Glacier {glacierId}', fontsize=20)\n",
    "#\n",
    "# Resize tick labels \n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648e88f",
   "metadata": {},
   "source": [
    "### Plot Points Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85662a",
   "metadata": {},
   "source": [
    "<a id='points'></a>When the map plots were generated [above](#mapplot), a set of points 10-k from the start of each flowline was extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7714a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "glacierPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ab529-c9ff-4817-bc36-229a8fa60786",
   "metadata": {},
   "source": [
    "The time series for each set of. points can be plotted as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "# Loop over glaciers\n",
    "for glacierId, ax in zip(glacierPoints, axes.flatten()):\n",
    "    # Loop over flowlines\n",
    "    for flowlineId in glacierPoints[glacierId]:\n",
    "        #\n",
    "        # interpolate to get results vx(time index) for each point\n",
    "        vx, vy, v = myVelSeries.interp(*glacierPoints[glacierId][flowlineId], units='km')\n",
    "        ax.plot(myVelSeries.time, v, marker='o',  linestyle='-', color=flowlineColors[flowlineId],label=f'{flowlineId}')\n",
    "    #\n",
    "    # pretty up plot\n",
    "    ax.legend(ncol=3, loc='upper right', title='Flowline ID')\n",
    "    ax.set_xlabel('year', fontsize=18)\n",
    "    ax.set_ylabel('Speed (m/yr)', fontsize=18)\n",
    "    ax.set_title(f'Glacier {glacierId}', fontsize=20)\n",
    "#\n",
    "# Resize tick labels \n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ff3a3",
   "metadata": {},
   "source": [
    "### Save the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25bbee5",
   "metadata": {},
   "source": [
    "While it is convenient to work the date remotely, its nice to be be able to save the data for further processing.\n",
    "\n",
    "The downloaded subset can be saved in a netcdf and reloaded for to `velSeries` instance for later analysis. \n",
    "\n",
    "Note makes sure the data have been subsetted so only the the subset will be saved (~15MB in this example). If not, the entire Greeland data set will be saved (370GB). \n",
    "\n",
    "Change `saveData` and `reloadData` below to test this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData = True # Set to True to save data\n",
    "if saveData:\n",
    "    myVelSeries.toNetCDF('Glaciers1-2example.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f39f8",
   "metadata": {},
   "source": [
    "Now open open the file and redo a plot from above with the saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloadData = True  # Set to True to reload the saved data\n",
    "if reloadData:\n",
    "    fig, axes = plt.subplots(np.ceil(len(myFlowlines)/2).astype(int), 2, figsize=(16, 8))  # Setup plot\n",
    "    myVelCDF = nisar.nisarVelSeries() # Create Series\n",
    "    myVelCDF.readSeriesFromNetCDF('Glaciers1-2example.nc')\n",
    "    #\n",
    "    for glacierId, ax in zip(myFlowlines, axes.flatten()):\n",
    "        # return interpolated values as vx(time index, distance index)\n",
    "        vx, vy, vv = myVelCDF.interp(*myFlowlines[glacierId].xykm(), units='km')\n",
    "        # loop over each profile by time\n",
    "        for speed, myDate in zip(vv, myVelSeries.time):\n",
    "            ax.plot(myFlowlines[glacierId].distancekm(), speed, label=myDate.year)\n",
    "        # pretty up plot\n",
    "        ax.legend(ncol=2, loc='upper right', fontsize=15)\n",
    "        ax.set_xlabel('Distance (km)', fontsize=18)\n",
    "        ax.set_ylabel('Speed (m/yr)', fontsize=18)\n",
    "        ax.set_title(f'Glacier {glacierId}', fontsize=20)\n",
    "# For other combinations could have \n",
    "    for ax in axes.flatten():\n",
    "        ax.tick_params(axis='both', labelsize=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28906f6a-ab41-4a64-bb2b-a0d607d9b8a6",
   "metadata": {},
   "source": [
    "### Summary for GrIMP Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855908e-61d9-4654-ac06-d3bfc12c389f",
   "metadata": {},
   "source": [
    "Using the [nisardev](https://github.com/fastice/nisardev) and [grimp](https://github.com/fastice/grimpfunc) we were easily able to perform many of the typical functions needed for the analysis of glaciers by accesssing remote [GrIMP data](https://nsidc.org/data/measures/grimp) such as:\n",
    "- Accessing stacks of velocity data;\n",
    "- Display velocity maps; and\n",
    "- Interpolating data to points or lines;\n",
    "\n",
    "When working with larger data sets (e.g., the 300+ [6/12 day velocity maps](https://nsidc.org/data/measures/grimp) at NSIDC), downloads can take longer (several minutes), but are still 2 to 3 orders of magnitude faster than downloading the full data set. \n",
    "\n",
    "Once downloaded, the data are easily saved for later use.\n",
    "\n",
    "Other notebooks demonstrated the use of these tools are available through [GrIMPNotebooks](https://github.com/fastice/GrIMPNotebooks) repo at github.\n",
    "\n",
    "As mentioned above, velocity data can help provide context for elevation change measurements. Next we look at elevation change for the Jakobshavn region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2364c3-8ced-4aba-9d67-710ba2d1bc2c",
   "metadata": {},
   "source": [
    "## Comparing ICESat-2 Data with Other Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737490d5-7748-44fa-a2e7-412d87f11ae4",
   "metadata": {},
   "source": [
    "Last time, we did a bit of work to add ICESat-2 and Operation Icebridge data to Pandas Dataframes. We only covered the basic operations that you can do with Pandas, so today we are going to do a more thorough analysis of the data here.\n",
    "\n",
    "Since we already downloaded the ICESat-2/ATM files of interest, we are not going to use icepyx just yet - we will go ahead and reload the data from yesterday.\n",
    "\n",
    "(Prompt) I forgot how to load the ICESat-2 data from a .h5 file. What do I need to do?\n",
    "\n",
    "(Prompt) I also forgot how to load the ATM data. How do I read the CSV?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcd82f-dfc7-4a41-baea-1e74821e5315",
   "metadata": {},
   "source": [
    "We established last time that ATM aligns best with the central ICESat-2 beams, particularly the central strong beam (GT2L). Let's see if that is reflected in the elevation profiles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50174c5-b3a4-4123-b714-b2b20e6d9591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ICESat-2 data\n",
    "is2_file = 'processed_ATL06_20190420093051_03380303_005_01_full.h5'\n",
    "with h5py.File(is2_file, 'r') as f:\n",
    "    is2_gt2r = pd.DataFrame(data={'lat': f['gt2r/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2r/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2r/land_ice_segments/h_li'][:]}) # Central weak beam\n",
    "    is2_gt2l = pd.DataFrame(data={'lat': f['gt2l/land_ice_segments/latitude'][:],\n",
    "                                  'lon': f['gt2l/land_ice_segments/longitude'][:],\n",
    "                                  'elev': f['gt2l/land_ice_segments/h_li'][:]}) # Central strong beam\n",
    "    \n",
    "# Load the ATM data\n",
    "atm_file = 'ILATM2_20190506_151600_smooth_nadir3seg_50pt.csv'\n",
    "atm_l2 = pd.read_csv(atm_file)\n",
    "\n",
    "# Look only at the nadir track\n",
    "atm_l2 = atm_l2[atm_l2['Track_Identifier']==0]\n",
    "\n",
    "# Change the longitudes to be consistent with ICESat-2\n",
    "atm_l2['Longitude(deg)'] -= 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784b935-431f-459e-acd4-8cbe3450ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the ICESat-2 data to the ATM latitudes\n",
    "is2_gt2r = is2_gt2r[(is2_gt2r['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2r['lat']>atm_l2['Latitude(deg)'].min())]\n",
    "is2_gt2l = is2_gt2l[(is2_gt2l['lat']<atm_l2['Latitude(deg)'].max()) & (is2_gt2l['lat']>atm_l2['Latitude(deg)'].min())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70385d23-32a1-487b-ab4a-10254ac803ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 2D plot of along-track surface height\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib widget\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.plot(is2_gt2r['lat'], is2_gt2r['elev'], label='gt2r')\n",
    "plt.plot(is2_gt2l['lat'], is2_gt2l['elev'], label='gt2l')\n",
    "plt.plot(atm_l2['Latitude(deg)'], atm_l2['WGS84_Ellipsoid_Height(m)'], label='atm')\n",
    "plt.xlabel('latitude')\n",
    "plt.ylabel('elevation [m]')\n",
    "plt.xlim([69.185, 69.275])\n",
    "plt.ylim([100, 550])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc3a1d-167e-472e-b6ab-dfb33307aa22",
   "metadata": {},
   "source": [
    "Sure enough, GT2L and ATM match very well! Since they are very close to each other, we can do a quick accuracy assessment between the two.\n",
    "\n",
    "The ATM DataFrame is larger than the ICESat-2 dataframe, so we're going to apply a simple spline interpolant to downscale the ICESat-2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d3520-d999-463f-ae11-5302a2f2d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import splrep,splev\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "# Apply a spline interpolant to the ICESat-2 data\n",
    "spl = splrep(is2_gt2l['lat'], is2_gt2l['elev'], s=0)\n",
    "is2_spl = splev(atm_l2['Latitude(deg)'], spl, der=0)\n",
    "\n",
    "# Calculate GT2L bias and add it to the ATM DataFrame\n",
    "atm_l2['bias'] = atm_l2['WGS84_Ellipsoid_Height(m)'] - is2_spl\n",
    "# Plot the bias curve\n",
    "plt.plot(atm_l2['Latitude(deg)'], atm_l2['bias'])\n",
    "#plt.plot(atm_l2['Latitude(deg)'], atm_l2['WGS84_Ellipsoid_Height(m)'])\n",
    "#plt.plot(atm_l2['Latitude(deg)'], is2_spl)\n",
    "plt.xlabel('latitude')\n",
    "plt.ylabel('bias [m]')\n",
    "plt.xlim([69.2, 69.26])\n",
    "plt.ylim([-20, 20])\n",
    "plt.show()\n",
    "\n",
    "print('Mean bias: %s m' %(atm_l2['bias'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b572c-8743-4a0c-b417-1c6105c77b45",
   "metadata": {},
   "source": [
    "Through some relatively simple operations, we found that ATM and ICESat-2 differ by ~0.33 m on average. Between this plot and the elevation plot above, what do you think might be causing some of the differences?\n",
    "\n",
    "We will revisit ICESat-2 and ATM near the end of this tutorial. Now, we are going to look at ice velocities and flow lines from the GRIMP project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af2a62-5445-494d-a18c-6f2a5abe899b",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fbd5c5",
   "metadata": {},
   "source": [
    "We are now going to revisit the GRIMP data one last time to visualize all of the data together. We have conducted a bias assessment between the two lidars, so now we are going to look at how the land ice heights change over time.\n",
    "\n",
    "First let's take a look at ATM data from previous years. The CSV file we are going to use is pre-processed L2 data for 2011-2018, much like the data from 2019. These flights are slightly east of the 2019 flight, which was adjusted to better align with ICESat-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79713ddb-f8a6-4384-ad0c-5656f0078aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# Read in the ATM CSV file\n",
    "atm_2011_2018 = pd.read_csv('ILATM2_2011_2019_v3.csv')\n",
    "lltoxy = pyproj.Transformer.from_crs(4326, 3413)\n",
    "\n",
    "#%matplotlib widget\n",
    "# Loop through the valid years and plot surface height\n",
    "years = ['2011', '2013', '2014', '2015', '2016', '2018']\n",
    "for i,year in enumerate(years):\n",
    "    lat = atm_2011_2018['Latitude_'+year]\n",
    "    elev = atm_2011_2018['elev_'+year]\n",
    "    axes[1].plot(lat, elev, label=year)\n",
    "#\n",
    "#\n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=axes[0], labelFontSize=12, plotFontSize=9, titleFontSize=14, \n",
    "                              vmin=1, vmax=3000, units='m', scale='log', midDate=False, colorBarSize='3%', colorBarPosition='bottom')\n",
    "axes[0].axis('off')\n",
    "lltoxy = pyproj.Transformer.from_crs(4326, 3413)\n",
    "for i, year in enumerate(years[3:]):\n",
    "    lat, lon = atm_2011_2018['Latitude_'+year], atm_2011_2018['Longitude_'+year]\n",
    "    x, y = lltoxy.transform(lat, lon)\n",
    "    axes[0].plot(x, y, 'w', linewidth=2)\n",
    "    v =  myVelSeries.interp(lat, lon, sourceEPSG=4326, returnXR=True).sel(time=datetime.datetime(int(year), 6, 1), \n",
    "                                                                          method='nearest').sel(band='vv')\n",
    "    axes[2].plot(lat, v, label=year)\n",
    "#\n",
    "axes[1].set_xlabel('latitude')\n",
    "axes[1].set_ylabel('elevation [m]')\n",
    "axes[1].legend()\n",
    "axes[2].set_xlabel('latitude')\n",
    "axes[2].set_ylabel('Speed [m/yr]')\n",
    "axes[2].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71a480-3be8-4d6c-b998-a61fd8fe90b7",
   "metadata": {},
   "source": [
    "Across these three figures, we can compare the ATM surface heights with ice velocities over the region. It's obvious that the greatest ice velocities are at the lower elevations, and vice-versa.\n",
    "\n",
    "We can also see a distinct decrease in ice velocity for 2018 - let's make a time series to see the ice height changes observed by ATM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bae601-65a8-4ddb-b67f-742318e4fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the latitude bounds of the surface trough\n",
    "lat_bounds = [69.1982, 69.2113]\n",
    "\n",
    "# 2013 has the longest streak of data over this region. We are going to downscale the other years to its length.\n",
    "lat_2013 = atm_2011_2018['Latitude_2013'][(atm_2011_2018['Latitude_2013']>lat_bounds[0]) & (atm_2011_2018['Latitude_2013']<lat_bounds[1])]\n",
    "\n",
    "# First, downscale the 2011 data to 2013 resolution\n",
    "lat = atm_2011_2018['Latitude_2011']\n",
    "elev = atm_2011_2018['elev_2011'][(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "        \n",
    "lat = lat[(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "spl = splrep(lat[::-1], elev[::-1], s=0)\n",
    "slp_2011 = splev(lat_2013, spl, der=0)\n",
    "\n",
    "# Calculate ice loss relative to 2011\n",
    "delta_h = [0]\n",
    "std_h = [0]\n",
    "for i,year in enumerate(years[1:]): # Start loop at 2013 (2012 has no data)\n",
    "    if year != 2013: # Downscale other years to 2013 resolution\n",
    "        lat = atm_2011_2018['Latitude_'+year]\n",
    "        elev = atm_2011_2018['elev_'+year][(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "        \n",
    "        # Downscale the data with splines\n",
    "        lat = lat[(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "        spl = splrep(lat[::-1], elev[::-1], s=0)\n",
    "        spl_year = splev(lat_2013, spl, der=0)\n",
    "        \n",
    "        # Now calculate the difference relative to 2011\n",
    "        delta_h.append((spl_year - slp_2011).mean())\n",
    "        std_h.append((spl_year - slp_2011).std())\n",
    "    else:\n",
    "        lat = atm_2011_2018['Latitude_'+year]\n",
    "        elev = atm_2011_2018['elev_'+year][(lat>lat_bounds[0]) & (lat<lat_bounds[1])].reset_index(drop=True)\n",
    "        \n",
    "        # Calculate the difference relative to 2011\n",
    "        delta_h.append((elev[::-1] - slp_2011).mean())\n",
    "        std_h.append((spl_year - slp_2011).std())\n",
    "    \n",
    "#%matplotlib widget\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.errorbar(years, delta_h, yerr=std_h, marker='.', markersize=12, capsize=4)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('$\\Delta$ h [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dc30c-d807-441b-804f-0c4ab1bd07f7",
   "metadata": {},
   "source": [
    "Ta-da!! Using a few operations, we were able to use ATM data to derive a rough time series of ice sheet elevation change over Jakobshavan. We can see that there is a significant loss in ice between 2011 and 2013, followed by a gradual decrease up through 2016. Interestingly, there is a non-negligible increase in ice height in 2018, which may explain the decrease in ice velocity for the same year.\n",
    "\n",
    "We're going to try and do the same thing, but for ICESat-2. Because it was launched in late-2018, we are going to try and grab interseasonal measurements from RGT 338 for 2019-2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4f1417-d575-4c6c-affc-0834425df49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to go through the icepyx routine again!\n",
    "import icepyx as ipx\n",
    "\n",
    "# Specifying the necessary icepyx parameters\n",
    "short_name = 'ATL06'\n",
    "lat_bounds = [69.1982, 69.2113]\n",
    "spatial_extent = [-50, 69.1982, -48.5, 69.2113] # KML polygon centered on Jakobshavan\n",
    "date_range = ['2019-04-01', '2021-12-30']\n",
    "rgts = ['338'] # IS-2 RGT of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e314d-f9c4-4014-824a-88ac238e70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Query object\n",
    "region = ipx.Query(short_name, spatial_extent, date_range, tracks=rgts)\n",
    "\n",
    "# Show the available granules\n",
    "region.avail_granules(ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24552df8-9bba-4754-9e16-7aa70ee99aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Earthdata credentials\n",
    "uid = 'uwhackweek'\n",
    "email = 'hackweekadmin@gmail.com'\n",
    "region.earthdata_login(uid, email)\n",
    "\n",
    "# Order the granules\n",
    "region.order_granules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00221419-857c-43a9-a1f6-589691d250ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/tmp/DataIntegration/'\n",
    "region.download_granules(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86405a-c311-4264-828a-d99edf2cf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Iterate through the files to grab elevation, and derive elevation differences relative to April 2019\n",
    "files = ['processed_' + granule for granule in region.avail_granules(ids=True)[0]]\n",
    "\n",
    "# Get the initial data from April 2019\n",
    "with h5py.File(files[0]) as f:\n",
    "    elev_42019 = f['gt2l/land_ice_segments/h_li'][:]\n",
    "    lat_42019 = f['gt2l/land_ice_segments/latitude'][:]\n",
    "    plt.plot(lat_42019, elev_42019, label=files[0][16:24])\n",
    "\n",
    "delta_h = [0]\n",
    "std_h = [0]\n",
    "for file in files[1:]:\n",
    "    try:\n",
    "        with h5py.File(file) as f:\n",
    "            cloud_flag = np.mean(f['gt2l/land_ice_segments/geophysical/cloud_flg_asr'][:])\n",
    "            # Filter out cloudy scenes\n",
    "            if cloud_flag < 2:\n",
    "                lat = f['gt2l/land_ice_segments/latitude'][:]\n",
    "                elev = f['gt2l/land_ice_segments/h_li'][:]\n",
    "                date = file[16:24] # Get date of IS-2 overpass\n",
    "                \n",
    "                # Find the difference relative to April 2019\n",
    "                delta_h.append(np.mean(elev - elev_42019))\n",
    "                std_h.append(np.std(elev - elev_42019))\n",
    "                \n",
    "                # Plot the elevation data\n",
    "                plt.plot(lat, elev, label=date)\n",
    "            else:\n",
    "                print('Cloudy scene - no data loaded')\n",
    "    except:\n",
    "            print('Cloudy scene - no data loaded')\n",
    "            \n",
    "plt.xlabel('latitude')\n",
    "plt.ylabel('elevation [m]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2b388-06c2-4587-bd45-2eb96d99972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ice sheet change time series\n",
    "dates = ['20190420','20200117', '20200717', '202110115']\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.errorbar(dates, delta_h, yerr=std_h, marker='.', markersize=12, capsize=4)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('$\\Delta$ h [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2fedff-0cb4-4a5b-9633-1ed4d7b83d56",
   "metadata": {},
   "source": [
    "There we go! We lost some data due to cloud cover, and the mean change has some spread, but the ICESat-2 data continues to show a downward trend that was suggested by ATM. Note that these changes are relative to an ICESat-2 observation - if we plotted these on the previous figure, the trend would be even more pronounced!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef0873",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "  Congratulations! You've completed this tutorial and have learned how to:\n",
    "* Access and plot glacier velocity using GrIMP data and python packages\n",
    "* Compared ICESat-2 elevations with ATM elevations, and \n",
    "* Integrated velocities and elevation changes for Jakobshavn into one plot;\n",
    "\n",
    "These are advanced methods for integrating, analyzing, and visualizing multiple kinds of data sets with ICESat-2, which can be adopted for other kinds of data and analyses.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
