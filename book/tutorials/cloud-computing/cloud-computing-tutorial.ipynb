{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297618f9-eaa6-44cd-abd1-ce102d3c98e9",
   "metadata": {},
   "source": [
    "# Cloud Computing Tutorial\n",
    "\n",
    "For ICESat-2 UW Hackweek 2022\n",
    "\n",
    "<img src='https://media0.giphy.com/media/r3Yeh3aAjsyYGObizC/giphy.gif' alt='cloud bouncing and smiling' style='width:350px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdece32-de39-4c13-950e-b7640171d2ff",
   "metadata": {},
   "source": [
    "# Icebreaker questions\n",
    "\n",
    "Enter your answers in chat or in your own text editor of choice.\n",
    "\n",
    "* Is everyone who wants to be logged into their jupyterhub and have the notebook open?\n",
    "* When you hear the term \"cloud computing\" what's the first thing that comes to mind?\n",
    "* What concepts or tools are you hoping to learn more about in this tutorial?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd657e4-dc4a-43ea-bcf5-70c9c868bef7",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Learning Objectives\n",
    "\n",
    "1. The difference between code running on your local machine vs a remote environment.\n",
    "2. The difference between data hosted by cloud providers, like AWS, and on-prem data centers\n",
    "3. The difference between how to access data hosted by NASA DAACs, on-prem, cloud/s3\n",
    "4. Cloud computing tools for scaling your science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b3082-ce41-44dc-96e3-f32193838948",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "* At least one tutorial (or tool) to try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de703c-b6e0-47a0-9ca1-a25684ca233b",
   "metadata": {},
   "source": [
    "***\n",
    "# Sections\n",
    "\n",
    "1. Local vs Remote Resources\n",
    "2. Data on the Cloud vs On-premise\n",
    "3. How to access NASA data\n",
    "4. Tools for cloud computing: Brief introduction to Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e598bf2-0d5b-48bd-a461-943a52d951ed",
   "metadata": {},
   "source": [
    "***\n",
    "# Setup: Getting prepared for what's coming!!!\n",
    "\n",
    "* Each section includes some key learning points and at least 1 exercise.\n",
    "* Configure your screens so you can see both the tutorial and your jupyterhub to follow along.\n",
    "* Let's all login into https://urs.earthdata.nasa.gov/home before we get started.\n",
    "* The bottom lists many other references for revisiting.\n",
    "* Let's install some libraries for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c2021-0927-42e2-a067-b0a5f5ffb8e3",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 1. Local vs Remote Resources\n",
    "\n",
    "## ‚ùìü§î‚ùì Question for the group:\n",
    "\n",
    "What's the difference between running code on your local machine this remote jupyterhub?\n",
    "\n",
    "As you are probably aware, this code is running on machine somewhere in Amazon Web Services (AWS) land.\n",
    "\n",
    "<img src='images/aws-data-centers.png' alt='aws data centers' style='width:400px' />\n",
    "\n",
    "## What types of resources are available on this machine?\n",
    "\n",
    "### CPUs\n",
    "\n",
    "The central processing unit (CPU) or processor, is the unit which performs most of the processing inside a computer. It processes all instructions received by software running on the PC and by other hardware components, and acts as a powerful calculator. [Source: techopedia.com](https://www.techopedia.com/definition/2851/central-processing-unit-cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff873453-0114-4b90-acc4-3d3c189659cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many CPUs are running on this machine?\n",
    "!lscpu | grep ^CPU\\(s\\):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd865127-441d-4044-b0b1-e4a71b790674",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "Computer random access memory (RAM) is one of the most important components in determining your system‚Äôs performance. RAM gives applications a place to store and access data on a short-term basis. It stores the information your computer is actively using so that it can be accessed quickly. [Source: crucial.com](https://www.crucial.com/articles/about-memory/support-what-does-computer-memory-do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d84bc-a048-445a-9106-6acf1d5f82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much memory is available?\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb2b1a-7477-4501-a37c-b0a4b77b8e4c",
   "metadata": {},
   "source": [
    "If you're curious about the difference between free and available memory: https://haydenjames.io/free-vs-available-memory-in-linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadd2da-7db1-49a8-9aaa-d91063b8de0e",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Exercise: How many CPUs does your machine have\n",
    "\n",
    "Unless you are using a linux machine, the above commands probably won't give you what you need.\n",
    "\n",
    "1. For MAC users: `sysctl -a | grep cpu | grep hw` or https://www.linux.com/training-tutorials/5-commands-checking-memory-usage-linux/\n",
    "2. For Windows users: https://www.top-password.com/blog/find-number-of-cores-in-your-cpu-on-windows-10/ (Not tested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb611300-3b96-4b9c-9fd2-c394296d418f",
   "metadata": {},
   "source": [
    "## ‚ùìü§î‚ùìQuestion for the group\n",
    "\n",
    "When might you want to use a remote machine and when might you want to use your local machine?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0ea5b-a750-430d-a108-9dfe9bcf621f",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 2. Data on the Cloud vs On-premise\n",
    "\n",
    "## What's the difference between data hosted on the cloud and on-prem data centers?\n",
    "\n",
    "<img alt='NASA Distributed Active Archive Centers (DAACs)' src='https://pbs.twimg.com/media/Dj7jIwwUUAAnskf?format=jpg&name=small' />\n",
    "\n",
    "NASA DAACs are in the process of migrating their collections to the \"Earthdata Cloud\". At this time, most datasets are still located and accessible \"on-premise\" from NASA DAACs, while high priority and new datasets are being stored on AWS Simple Storage Service (S3). Given different use cases, you will need to access datasets from NASA DAAC's as well as on NASA's Earthdata Cloud (AWS S3).\n",
    "\n",
    "* Datasets are still managed by the DAAC, but the DAAC stores files on AWS S3.\n",
    "* The DAACs' services will be collocoated in the cloud with the data.\n",
    "* Users are encouraged to access the data collocated in the cloud through AWS computer services (like this jupyterhub!)\n",
    "\n",
    "\n",
    "## üèãÔ∏è Exercise\n",
    "\n",
    "Navigate search.earthdata.nasa.gov and search for ICESat-2 and answer the following questions:\n",
    "\n",
    "1. Which DAAC hosts ICESat-2 datasets?\n",
    "2. Which ICESat-2 datasets are hosted on the AWS Cloud and how can you tell?\n",
    "\n",
    "## What did we learn?\n",
    "\n",
    "NASA has a new cloud paradigm, which includes data stored both on-premise as well as on the cloud. NASA DAACs are providing services also on AWS.\n",
    "\n",
    "PO.DAAC has a great diagram for this new paradigm, source https://podaac.jpl.nasa.gov/cloud-datasets/about\n",
    "\n",
    "![Cloud_ecosystem_diagram](https://podaac.jpl.nasa.gov/sites/default/files/content/CLOUD_Data_Pages/Cloud_ecosystem_diagram.jpg)\n",
    "\n",
    "## Final thought: Other cloud data providers\n",
    "\n",
    "AWS is of course not the only cloud provider and Earthdata can be found on other popular cloud providers.\n",
    "\n",
    "* [Google Earth Engine Data Catalog](https://developers.google.com/earth-engine/datasets)\n",
    "* [Microsoft Planetary Computer Data Catalog](https://planetarycomputer.microsoft.com/catalog)\n",
    "* AWS also has its public data registry [Open Data on AWS](https://aws.amazon.com/opendata/) and its sustainability data initiative with its [Registry of Open Data on AWS: Sustainability Data Initiative](https://registry.opendata.aws/collab/asdi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8ab2b-c839-42f8-8634-3beadb56a669",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# 3. How to access NASA data\n",
    "\n",
    "How do we access data hosted on-prem and on the cloud? What are some tools we can use?\n",
    "\n",
    "## Earthdata Login for Access\n",
    "\n",
    "NASA uses Earthdata Login to authenticate users requesting data to track usage. You must supply EDL credentials to access all data hosted by NASA. Some data is available to all EDL users and some is restricted.\n",
    "\n",
    "You can access data from NASA using ~/.netrc files locally which store your EDL credentials.\n",
    "\n",
    "## üèãÔ∏è Exercise 1: Access via Earthdata Login using `~/.netrc`\n",
    "\n",
    "The Openscapes Tutorial [04. Authentication for NASA Earthdata](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html) offers an excellent quick tutorial on how to create a ~/.netrc file. \n",
    "\n",
    "* Exercise: Review the tutorial and answer the following question: Why might you want to be careful running this code in a shared jupyterhub environment?\n",
    "\n",
    "* Takehome exercise: Run through the code on your local machine\n",
    "\n",
    "## üèãÔ∏è Exercise 2: Use the `earthdata` library to access ICESat-2 data \"on-premise\" at NSIDC\n",
    "\n",
    "Programmatic access of NSIDC data can happen in 2 ways:\n",
    "\n",
    "```text\n",
    "Search -> Download -> Process -> Research\n",
    "```\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NASA-Openscapes/earthdata-cloud-cookbook/main/examples/NSIDC/img/download-model.png\" width=\"35%\"/>\n",
    "\n",
    "```text\n",
    "Search -> Process in the cloud -> Research\n",
    "```\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NASA-Openscapes/earthdata-cloud-cookbook/main/examples/NSIDC/img/cloud-model.png\" width=\"35%\"/>\n",
    "\n",
    "> **Credit**: Open Architecture for scalable cloud-based data analytics. From Abernathey, Ryan (2020): Data Access Modes in Science.\n",
    "\n",
    "For this exercise, we are going to use [NSIDC's earthdata python library](https://github.com/nsidc/earthdata/) to find and download ATL08 files from NSIDC DAAC via HTTPS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f394471-3b89-40d2-ac16-42dd933c8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login using earthdata\n",
    "from earthdata import Auth, DataGranules, DataCollections, Store\n",
    "import os.path\n",
    "\n",
    "auth = Auth()\n",
    "\n",
    "# For Githhub CI, we can use ~/.netrc\n",
    "if os.path.isfile(os.path.expanduser('~/.netrc')):\n",
    "    auth.login(strategy='netrc')\n",
    "else:\n",
    "    auth.login(strategy='interactive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b330e96-acc4-4275-839c-536176eb50aa",
   "metadata": {},
   "source": [
    "Earthdata library uses a [session](https://github.com/nsidc/earthdata/blob/cb54f645a9fced3ce28d6312cd57aa4d7b68683e/earthdata/auth.py#L91) so credentials are not stored in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8b829-bb6e-4591-a344-7a64f5408bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth._session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b742fee-4e79-4cc9-9e58-12cdd1a593e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find some ICESat-2 ATL08 granules and display them\n",
    "granules = DataGranules().short_name('ATL08').bounding_box(-10,20,10,50).get(5)\n",
    "[display(g) for g in granules[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204ef7f-9fab-4f2a-b837-d79b2b43e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if these are hosted on the cloud\n",
    "granules[0].cloud_hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be42d3f-a228-41f1-953c-7a6b9b8c05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "## Download some files\n",
    "atl08_dir = 'data/demo-atl08'\n",
    "store = Store(auth)\n",
    "store.get(granules[0:3], atl08_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527eb06-b7ef-4817-b1bc-3e84a91f6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "## Open one of them\n",
    "files = glob.glob(f'{atl08_dir}/*.h5')\n",
    "ds = h5py.File(files[0], 'r')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f984e8b-4bc8-40de-b5ac-40d9d2a36d40",
   "metadata": {},
   "source": [
    "## ‚ùìü§î‚ùì Question for the group\n",
    "\n",
    "**Which NSIDC data access paradigm does the above code fit into?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58bda6b-0f90-4eb5-b521-e58b129d9c10",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Exercise 3: Access COG data from S3 using Earthdata Search\n",
    "\n",
    "First I will demonstrate how to navigate to search.earthdata.nasa.gov and search for GEDI data \"Available from AWS Cloud\". Select a granule and click the download icon and then AWS S3 Access and \"Get AWS S3 Credentials\"\n",
    "\n",
    "To generate S3 credentials programmatically, you can follow the tutorials this was based on:\n",
    "\n",
    "* [Accessing Cloud Optimized GeoTIFF (COG) - HTTPS Example](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/how-tos/Earthdata_Cloud__Single_File__HTTPS_Access_COG_Example.html)\n",
    "* [Accessing Cloud Optimized GeoTIFF (COG) - S3 Direct Access](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_COG_Example.html)\n",
    "* [Accessing a NetCDF4/HDF5 File - S3 Direct Access](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/how-tos/Earthdata_Cloud__Single_File__Direct_S3_Access_NetCDF4_Example.html)\n",
    "* [Accessing Multiple NetCDF4/HDF5 Files - S3 Direct Access](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/how-tos/Multi-File_Direct_S3_Access_NetCDF_Example.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287450b6-6e76-46d8-bfdf-64a5ddfd97d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import rasterio as rio\n",
    "from rasterio.session import AWSSession\n",
    "import requests\n",
    "import rioxarray\n",
    "import os\n",
    "\n",
    "def get_temp_creds(provider):\n",
    "    return requests.get(s3_cred_endpoint[provider]).json()\n",
    "\n",
    "s3_cred_endpoint = {\n",
    "    'podaac':'https://archive.podaac.earthdata.nasa.gov/s3credentials',\n",
    "    'gesdisc': 'https://data.gesdisc.earthdata.nasa.gov/s3credentials',\n",
    "    'lpdaac':'https://data.lpdaac.earthdatacloud.nasa.gov/s3credentials',\n",
    "    'ornldaac': 'https://data.ornldaac.earthdata.nasa.gov/s3credentials',\n",
    "    'ghrcdaac': 'https://data.ghrc.earthdata.nasa.gov/s3credentials'\n",
    "}\n",
    "\n",
    "if os.path.isfile(os.path.expanduser('~/.netrc')):\n",
    "    # For Githhub CI, we can use ~/.netrc \n",
    "    temp_creds_req = get_temp_creds('lpdaac')\n",
    "else:\n",
    "    # ADD temporary credentials here\n",
    "    temp_creds_req = {}\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=temp_creds_req['accessKeyId'], \n",
    "                        aws_secret_access_key=temp_creds_req['secretAccessKey'],\n",
    "                        aws_session_token=temp_creds_req['sessionToken'],\n",
    "                        region_name='us-west-2')\n",
    "\n",
    "# ADD S3 URL from Earthdata Search here\n",
    "# Note you want to pick a GeoTIFF for the rioxarray code to work\n",
    "s3_url = 's3://lp-prod-protected/HLSL30.020/HLS.L30.T58KEB.2022077T225645.v2.0/HLS.L30.T58KEB.2022077T225645.v2.0.SZA.tif'\n",
    "\n",
    "# NOTE: Using rioxarray assumes you are accessing a GeoTIFF\n",
    "rio_env = rio.Env(AWSSession(session),\n",
    "                  GDAL_DISABLE_READDIR_ON_OPEN='TRUE',\n",
    "                  GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                  GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'))\n",
    "rio_env.__enter__()\n",
    "\n",
    "da = rioxarray.open_rasterio(s3_url)\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7716b2e3-4766-4b90-ae0b-6bdec1ebf9bc",
   "metadata": {},
   "source": [
    "## What did we learn?\n",
    "\n",
    "* How to use earthdata library to access datasets\n",
    "* How to use earthdata search to generate credentials\n",
    "\n",
    "## ‚ùìü§î‚ùì Question for the Group\n",
    "\n",
    "When making a request to Earthdata file URLs, how do we know if the data are coming from Earthdata Cloud or on-premise?\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "* There are a lot of other examples of NASA data access, here are a few examples\n",
    "  * https://github.com/NASA-Openscapes/earthdata-cloud-cookbook/blob/main/examples/NSIDC/ICESat2-CMR-OnPrem-vs-Cloud.ipynb\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4771aa-2e38-4774-b30b-c6d8e4b31f53",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "# 4. Tools for cloud computing\n",
    "\n",
    "We've spent a lot of time on how to access the data because that's the first step to taking advantage of cloud compute resources. Once you have developed a way to access the data on the cloud or on-premise, you may be ready to take the next step and scale your workload using a tool like Dask.\n",
    "\n",
    ">Dask is a flexible library for parallel computing in Python. Dask is composed of two parts: Dynamic task scheduling optimized for computation.\n",
    "\n",
    "https://docs.dask.org/en/stable/\n",
    "\n",
    "<img alt='Dask is composed of three parts. \"Collections\" create \"Task Graphs\" which are then sent to the \"Scheduler\" for execution. There are two types of schedulers that are described in more detail below.' src='./images/dask-overview.png' style='width: 600px'/>\n",
    "\n",
    "Dask is composed of three parts. \"Collections\" create \"Task Graphs\" which are then sent to the \"Scheduler\" for execution. \n",
    "\n",
    "\n",
    "## üèãÔ∏è Exercise - Using Dask to parallelize computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd75b7-cc2f-4208-b6c7-eec9a2ebbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4,\n",
    "                local_directory=\"/tmp/dask\" # local scratch disk space\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5299182d-d6af-40e1-b238-08847f20d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801a245-8dc0-4d7c-b993-ec4807325308",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "First let‚Äôs make some toy functions, inc and add, that sleep for a while to simulate work. We‚Äôll then time running these functions normally.\n",
    "\n",
    "In the next section we‚Äôll parallelize this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223f33a-e3c6-4aaf-8b49-8b774cb5b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6a2b3-63b6-4424-ae9c-86696618a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes three seconds to run because we call each\n",
    "# function sequentially, one after the other\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74516ea0-0c8f-4b73-8ae5-4623fa513784",
   "metadata": {},
   "source": [
    "## Parallelize with the dask.delayed decorator\n",
    "\n",
    "Those two increment calls could be called in parallel, because they are totally independent of one-another.\n",
    "\n",
    "We‚Äôll transform the inc and add functions using the dask.delayed function. When we call the delayed version by passing the arguments, exactly as before, the original function isn‚Äôt actually called yet - which is why the cell execution finishes very quickly. Instead, a delayed object is made, which keeps track of the function to call and the arguments to pass to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f3653-42f2-4713-82e4-46dcc702585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b07d2d-2cb4-48d5-b93e-5e092d831c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This runs immediately, all it does is build a graph\n",
    "\n",
    "x = delayed(inc)(1)\n",
    "y = delayed(inc)(2)\n",
    "z = delayed(add)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f19a88-2f2e-4b84-8989-c120285e1ea9",
   "metadata": {},
   "source": [
    "This ran immediately, since nothing has really happened yet.\n",
    "\n",
    "To get the result, call compute. Notice that this runs faster than the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533da77-d7eb-4ad4-a03a-e55f57f43380",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This actually runs our computation using a local thread pool\n",
    "\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62d379-3745-4cf0-ac24-731ff2e5c23c",
   "metadata": {},
   "source": [
    "## What just happened?\n",
    "The z object is a lazy Delayed object. This object holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another. We can evaluate the result with .compute() as above or we can visualize the task graph for this value with .visualize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51214b-94ed-44ab-a784-817008d80ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc8911-3193-4fa7-a768-4d3eca591fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a952d-9b81-4bc7-8864-8a75509eb058",
   "metadata": {},
   "source": [
    "Some questions to consider:\n",
    "\n",
    "* Why did we go from 3s to 2s? Why weren‚Äôt we able to parallelize down to 1s?\n",
    "* What would have happened if the inc and add functions didn‚Äôt include the sleep(1)? Would Dask still be able to speed up this code?\n",
    "* What if we have multiple outputs or also want to get access to x or y?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60decf-f08b-493b-bee6-b0e0bf5f359e",
   "metadata": {},
   "source": [
    "## Demonstration - Dask\n",
    "\n",
    "We just used local threads to parallelize this operation. What are some other options for running our code?\n",
    "\n",
    "[Dask Gateway](https://gateway.dask.org/) provides a way to connect to more than one machine running dask workers.\n",
    "\n",
    "> Dask Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed cluster environment, without requiring users to have direct access to the underlying cluster backend (e.g. Kubernetes, Hadoop/YARN, HPC Job queues, etc...).\n",
    "\n",
    "We can see this in action using the [ESIP QHub deployment](https://jupyter.qhub.esipfed.org/) or a Pangeo Hub.\n",
    "\n",
    "![esip-qhub-dask](images/esip-qhub-dask.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e93cd-c10f-4b0e-99d2-717ae5525260",
   "metadata": {},
   "source": [
    "## ‚ùìü§î‚ùì Question for the group\n",
    "\n",
    "**What makes running code on a dask cluster different from running it in this notebook?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519b783-6322-41d1-bdc2-1d301338bca6",
   "metadata": {},
   "source": [
    "## üéÅ Bonus\n",
    "\n",
    "If we have time, go through Dask Tutorial from the [Pangeo Tutorial Gallery](http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b0254-428a-42e9-826a-f587180aaf1d",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "# Want to learn more?\n",
    "\n",
    "Join the Pangeo and ESIP Cloud Computing Cluster communities\n",
    "\n",
    "* [Pangeo](https://pangeo.io/) is first and foremost a community promoting open, reproducible, and scalable science. This community provides documentation, develops and maintains software, and deploys computing infrastructure to make scientific research and programming easier. \n",
    "  * Visit the website or joing the community meetings: [Pangeo Community Meetings](studio)\n",
    "* Join the ESIP Cloud Computing Cluster for our next Knowledge Sharing Session on Apache Beam for Geospatial data\n",
    "  * Cloud computing cluster email list: https://lists.esipfed.org/mailman/listinfo/esip-cloud\n",
    "  * ESIP Slack and cloud computing cluster channel: https://bit.ly/3FtX1HV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1e3a9-41f0-4ea9-8fc2-65e650ecee75",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Resources\n",
    "\n",
    "## Examples + Tutorials of cloud computing\n",
    "\n",
    "* [Notebook exploring the Landsat Collection 2 data on AWS using a Dask cluster via Coiled.io](https://github.com/Element84/geo-notebooks)\n",
    "* [Pangeo Tutorial Gallery](http://gallery.pangeo.io/)\n",
    "\n",
    "## ICESat-2\n",
    "\n",
    "* [ICESat-2 AWS cloud data access (BETA ONLY)](https://icepyx.readthedocs.io/en/latest/example_notebooks/IS2_cloud_data_access.html)\n",
    "\n",
    "## Collections of tutorials\n",
    "\n",
    "* [NASA Openscapes 2021 Cloud Hackathon Tutorials](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials)\n",
    "* [NASA Openscapes 2021 Cloud Workshop at AGU](https://nasa-openscapes.github.io/2021-Cloud-Workshop-AGU/)\n",
    "\n",
    "I believe those notebooks will eventually be migrated to [Earthdata Cloud Cookbook: Supporting NASA Earth science research teams‚Äô migration to the cloud](https://nasa-openscapes.github.io/earthdata-cloud-cookbook) but that website has placeholders for the content.\n",
    "\n",
    "## Introduction to NASA Earthdata Cloud from PO.DAAC\n",
    "\n",
    "* [Introduction: Access to PO.DAAC datasets in the cloud](https://podaac.jpl.nasa.gov/cloud-datasets/about)\n",
    "\n",
    "## Tools for subsetting\n",
    "\n",
    "* [rioxarray for Cloud-Optimized GeoTIFFs](https://corteva.github.io/rioxarray/stable/examples/COG.html)\n",
    "* [Data Subsetting and Transformation Services in the Cloud: Using the Harmony-Py library to access customized data from NASA Earthdata](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/07_Harmony_Subsetting.html)\n",
    "* [xarray for Zarr](https://xarray.pydata.org/en/stable/generated/xarray.open_zarr.html)\n",
    "\n",
    "## About COGs\n",
    "\n",
    "* https://developmentseed.org/blog/2019-05-03-cog-talk-part-1-whats-new\n",
    "* https://developers.planet.com/planetschool/an-introduction-to-cloud-optimized-geotiffs-cogs-part-1-overview/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
