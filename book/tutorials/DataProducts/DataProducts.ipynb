{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d9a0f2-2fcd-44f8-8d44-9187144a44bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ICESat-2 data products\n",
    "\n",
    "This tutorial is meant to show some examples of the standard land-ice data products put out by the ICESat-2 project.  It is not a comprehensive look at all the products, nor does it document all the features available in the product is shows.  Rather, it presents some tools for looking at the data, and shows how the information flows between land-ice products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef84b42-7348-4e2c-94cf-dd377ae9741f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "1.  Demonstrate basic HDF-5 product structures of ICESat-2 standard data products \n",
    "2.  Demonstrate the flow of information from the lowest-level products to the highest-level products (or vice versa)\n",
    "3.  Data search using geographic regions and file names using icePyx\n",
    "4.  File-level data access from NSIDC\n",
    "5.  Photon data access using SlideRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b69f6-a47b-4ef0-8346-136186813cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages:\n",
    "import numpy as np                   # Numeric Python\n",
    "import matplotlib.pyplot as plt      # Plotting routines\n",
    "import h5py                          # general HDF5 reading/writing library\n",
    "import rioxarray as rx               # Package to read raster data from hdf5 files\n",
    "from pyproj import Transformer, CRS  # libraries to allow coordinate transforms\n",
    "import glob                          # Package to locate files on disk\n",
    "import os                            # File-level utilities\n",
    "import re                            # regular expressions for string interpretation\n",
    "import icepyx as ipx                 # Package to interact with ICESat-2 online resources\n",
    "from sliderule import icesat2        # Package for online ICESat-2 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d39813-4585-4e10-9b6c-fce421499562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please note: This tutorial is best run in interactive mode,\n",
    "# but is archived with the interactive section disabled.\n",
    "# To enable interactive mode, comment the next line, and uncomment the second line:\n",
    "%matplotlib inline  \n",
    "#%matplotlib widget \n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab9a47-7837-4d87-846c-e72ea1af3c3a",
   "metadata": {},
   "source": [
    "## 0.  Get access to earthdata using a .netrc file.  You'll need to do this to run the notebook!\n",
    "\n",
    "We will need to generate a (hidden) .netrc file in our local directory.  The [NASA Openscapes 2021 Cloud Hackathon](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/04_NASA_Earthdata_Authentication.html) came up with a function that does this painlessly.  You'll only have to run this once on each computer you use for this tutorial.  If you're running interactively, you'll have to uncomment two lines in the next cell and run that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45aa35-a50e-435e-8ca3-47990db56631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFAULT: to generate the rendered tutorial for the book. Comment these two lines if you are running interactively\n",
    "HOST = 'https://urs.earthdata.nasa.gov'\n",
    "ipx.core.Earthdata.Earthdata('uwhackweek','hackweekadmin@gmail.com', HOST).login()\n",
    "\n",
    "# If you are running interactively, uncomment these two lines, and run this cell!\n",
    "#from setup_earthdata_netrc import setup_earthdata_netrc\n",
    "#setup_earthdata_netrc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5732d0-a8a7-4dde-aef0-9e5c19c15f34",
   "metadata": {},
   "source": [
    "## 1. Location\n",
    "We will be looking at Svalbard in the Norwegian Arctic, focusing on the massive surge from the Austfonna ice cap.  This started in 2010, and the ice cap is still adjusting to the rapid loss of ice, so we expect to see large thinning rates in the area affected by the surge.  We will use the ICESat-2 ATL15 product for a look at the mass-loss pattern over the last three years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9493dc5-b60b-4bab-a362-7965baf89fe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Svalbard](images/feart-08-00156-g001_small.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd718c-d78c-4112-868e-4c9703399cbf",
   "metadata": {},
   "source": [
    "Photo credit: Schuler et. al, Front. Earth Sci., 27 May 2020 | https://doi.org/10.3389/feart.2020.00156"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5122ec-13a6-4914-bd27-6495bd1059db",
   "metadata": {},
   "source": [
    "## 2. ATL15 data.\n",
    "\n",
    "ATL15 provides a high-level map of how ice-surface heights have changed.  It supplies grids of surface-height differences relative to a January-2020 reference surface, at quarter-year, 1-km resolution.  It comes in subsets for different polar glaciated regions:\n",
    "\n",
    "The Antarctic and Greenland full-resolution granules are large, but the smaller regions () are compact files that are quick to download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4588e9-2a2a-4a62-aeb8-deea2c703b78",
   "metadata": {
    "tags": []
   },
   "source": [
    "We'll download these files directly from NSIDC using the unix 'wget' command.  You'll need to have your login credentials set up for this to work.  Please check near the top of this tutorial (section zero) for help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b1802-332f-4b6c-aa7c-91eb08626af0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once our credentials are set up, we can use wget to download the ATL15 Svalbard granule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ac3a1-3f55-45dc-923c-e7f41d244cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (-nc = \"no clobber\" if it already exists\n",
    "ds_url='https://n5eil01u.ecs.nsidc.org/ATLAS/ATL15.001/2019.03.29/ATL15_SV_0311_01km_001_01.nc'\n",
    "! wget  -nc {ds_url} -O /tmp/ATL15_SV_0311_01km_001_01.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416dfa6-61ed-4554-8ef1-66d0858ef354",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "source": [
    "### A note on data access:  \n",
    "If the previous slide did not download data, you may need to revisit the commented cell just after section zero (0. Get access to earthdata using a .netrc file.) You'll need to do this to run the notebook!\n",
    "                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de767eb9-92d8-4076-9073-424c4581c413",
   "metadata": {
    "tags": []
   },
   "source": [
    "The data we want to look at are in the delta_h group, and we can see the structure of the product by opening it with _rioxarray_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c1b5d-73cb-4364-a8c7-f2d6618d1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=rx.open_rasterio('/tmp/ATL15_SV_0311_01km_001_01.nc', group='delta_h', masked=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfaef44-2a3e-4274-b014-951ca2000c47",
   "metadata": {},
   "source": [
    "The datasets here represent surface-height differences relative to 2020.0.  For a quick view of what is going on, we can make a map of the mean height change rate from the start of the mission to the present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c864815-953a-4ab9-b104-8cdcc534c6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note that time is in days after Jan 1 2018\n",
    "dhdt = (ds['delta_h'][-1,:,:]-ds['delta_h'][0,:,:])/(ds['time'][-1]-ds['time'][0])*365.25\n",
    "extent=np.array([np.min(ds['x'])-500, np.max(ds['x'])+500, np.min(ds['y'])-500, np.max(ds['y'])+500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a33c0-5049-4c01-870a-ef2ecbf7311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "hax=plt.gca()\n",
    "h_im=hax.imshow(dhdt, cmap='Spectral',  extent=extent)\n",
    "hax.set_aspect(1)\n",
    "hax.set_ylim([-0.8e6, -0.2e6])\n",
    "hax.set_xlabel('polar-stereographic x')\n",
    "hax.set_ylabel('polar-stereographic y')\n",
    "plt.colorbar(h_im, label='height-change rate, m/yr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0efa58-6f60-44ff-b6b5-d75e845057a3",
   "metadata": {},
   "source": [
    "If you are looking at the map in interactive mode (%matplotlib widget in the first cell) you can use the box-zoom button to zoom in on the edge of the surging glacier, or you can run the next cell to zoom to pre-selected region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e467e-95e7-49cd-b3d8-3c4f5df88020",
   "metadata": {},
   "outputs": [],
   "source": [
    "YR= np.array([-4.2e5, -3.85e5])\n",
    "XR= np.array([1.015e6, 1.060e6])\n",
    "hax.plot(XR[[0, 1, 1, 0, 0]], YR[[0, 0, 1, 1, 0]],'k')\n",
    "\n",
    "plt.figure(); \n",
    "ax1=plt.gca()\n",
    "h_im=ax1.imshow(dhdt, cmap='Spectral',  extent=extent)\n",
    "ax1.set_aspect(1)\n",
    "ax1.set_xlim(XR)\n",
    "ax1.set_ylim(YR)\n",
    "plt.colorbar(h_im, label='dh/dt, m/yr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d9481-16c7-408e-9a68-324047e7269c",
   "metadata": {
    "tags": []
   },
   "source": [
    "If we want to see the height change for a point, we can find the x and y coordinates that are closest to that point, and plot the delta_h variable for those coordinates.  Let's do that for the point a the center of the current zoomed view, and for another point 12 km to the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4a598-53de-4843-a76d-46a9f151bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the limits of the current axes, mark the center\n",
    "XR=np.array(ax1.get_xlim())  #<----------In interactive mode, try zooming in on a different location\n",
    "YR=np.array(ax1.get_ylim())\n",
    "xc=XR.mean()\n",
    "yc=YR.mean()\n",
    "\n",
    "# find the point closest to the axes center:\n",
    "col=np.argmin(np.abs(np.array(ds.x)-xc))\n",
    "row=np.argmin(np.abs(np.array(ds.y)-yc))\n",
    "ax1.plot(ds.x[col], ds.y[row],'k*')\n",
    "\n",
    "plt.figure()\n",
    "ax2=plt.gca()\n",
    "ax2.plot(ds.time/365.25+2018, ds.delta_h[:, row, col],'k', marker='*')\n",
    "\n",
    "# find another point 12 km to the left of the center:\n",
    "col=np.argmin(np.abs(np.array(ds.x)-(xc-12000)))\n",
    "row=np.argmin(np.abs(np.array(ds.y)-yc))\n",
    "ax1.plot(ds.x[col], ds.y[row],'g*')\n",
    "ax2.plot(ds.time/365.25+2018, ds.delta_h[:, row, col],'g', marker='*')\n",
    "ax2.set_xlabel('time'); ax2.set_ylabel('height WRT 2020.0, m');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf7ea6-bb87-4e94-990a-efdcd6ef6470",
   "metadata": {},
   "source": [
    "## 3. ATL11 along-track height data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da073d7e-d986-455c-8a15-94e287d5c432",
   "metadata": {},
   "source": [
    "We might be interested to see where these height changes come from.  ATL15 is derived from the ATL11 (along-track height change) product, which maps height changes for individual ICESat-2 reference tracks.  We don't necessarily know what track has contributed to the height change at any given point, so we need to bring in some more data-discovery tools to get there.\n",
    "\n",
    "### 3.1 Finding ATL11 data using IcePyx and CMR\n",
    "\n",
    "We can use a CMR (Common Metadata Repository) query through icepyx to find what ATL11 granules have contributed to the height changes we see above.  CMR queries are conducted in latitude and longitude, rather than projected coordinates, so we'll need to find the latitude and longitude for the zoomed-in axes using pyproj:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c809a-7887-4bda-87a5-8a37130191a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare coordinate transformations between lat/lon and the ATL15 coordinate system\n",
    "crs=CRS.from_epsg(3413)\n",
    "to_xy_crs=Transformer.from_crs(crs.geodetic_crs, crs)\n",
    "to_geo_crs=Transformer.from_crs(crs, crs.geodetic_crs)\n",
    "\n",
    "corners_lat, corners_lon=to_geo_crs.transform(np.array(XR)[[0, 1, 1, 0, 0]], np.array(YR)[[0, 0, 1, 1, 0]])\n",
    "latlims=[np.min(corners_lat), np.max(corners_lat)]\n",
    "lonlims=[np.min(corners_lon), np.max(corners_lon)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f1a84c-95d5-4b42-9f84-4a00f658acca",
   "metadata": {},
   "source": [
    "We'll use these limits for a CMR query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc0b2a-c8a5-4c3d-922e-e4dd51ba17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a = ipx.Query('ATL11', [lonlims[0], latlims[0], lonlims[1], latlims[1]], ['2018-01-01','2022-06-01'], \\\n",
    "                          start_time='00:00:00', end_time='23:59:59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6a1051-86e4-4755-92b7-a1fd2a57746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_a.avail_granules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9a5e4-5809-4ed4-9704-b7e7cca2f072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for count, granule in enumerate(region_a.granules.avail[0:10]):\n",
    "    print(f' {count} {granule[\"producer_granule_id\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3be12-7e8d-4c29-bd3c-30f833c8a136",
   "metadata": {},
   "source": [
    "There are a total of 55 granules that intersect our region. This is too many files to download during a tutorial, although it's not a huge amount of data.  Let's take a peek at the ATL11 filenames to help decide what to download.  These filenames follow a specific format:\n",
    "\n",
    "**ATL11_054803_0311_004_01.h5**\n",
    "\n",
    "This name is made up of :\n",
    "\n",
    "**ATL11_ttttrr_c0c1_RRR_VV.h5**\n",
    "\n",
    "where\n",
    "- ATL11 is the product name\n",
    "- tttt is the reference ground track (0548)\n",
    "- rr is the subregion (03 indicates as ascending track in the northern mid latitudes)\n",
    "- c0c1 are the first and last repeat-track cycles in the granule\n",
    "- RRR is the release\n",
    "- VV is the version\n",
    "\n",
    "The subregion is particularly useful here: region 03 are upper-northern-hemisphere ascending tracks, and region 04 are mid-northern-hemisphere descending tracks.  Region 04 crosses the pole.  Let's start by just downloading region 03.  We'll use the shell program 'wget' to do the downloads, and a regular expression to capture the region number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f43ac0-9b01-4787-8819-c4fc15449d79",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "ATL11_re=re.compile('ATL11_(\\d\\d\\d\\d)(\\d\\d)_')\n",
    "\n",
    "for granule in region_a.granules.avail:\n",
    "    this_name = granule['producer_granule_id']\n",
    "    # check if each granule has been downloaded already\n",
    "    if os.path.isfile('/tmp/'+this_name):\n",
    "        continue\n",
    "    # pull out the subregion number, skip subregion 4\n",
    "    subregion = ATL11_re.search(this_name).group(2)\n",
    "    if not subregion == '03':\n",
    "        continue\n",
    "    print(this_name)\n",
    "    ! wget {granule['links'][0]['href']} -q -O /tmp/{this_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c08f1-1547-435e-9a7c-7c95166922f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's see what these files look like.  An easy way to see what is in these granules is to use the 'h5ls' utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68eb97-7dcd-4d84-9700-bbd3eb4dd1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the last granule downloaded:\n",
    "print(this_name)\n",
    "! h5ls /tmp/{this_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb64caa9-8bce-48e2-af59-3c92a1b6777f",
   "metadata": {
    "tags": []
   },
   "source": [
    "The 'pt1','pt2', and 'pt3' groups contain data for the left, middle and right pair tracks.  It's in these tracks that we find the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c912cf3-52f0-40c9-be00-729dbdf08ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! h5ls /tmp/{this_name}/pt2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a74d5d-3f4a-4ad5-a1dd-0cfc506d7768",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we see 'latitude', 'longitude', 'delta_time', and 'h_corr', which give the location, timing, and height of the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277bd87-c322-4249-8ee5-8fbccb2ce687",
   "metadata": {},
   "source": [
    "### 3.2 Reading ATL11 data into a dictionary\n",
    "\n",
    "To look at the data in the ATL11 files, we will use the _h5py_ package.  We'll \n",
    "1. Store the data from each file in a dictionary \n",
    "2. Store the file-level data in another dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e52b7-0dae-49a4-a9f7-5044975d618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_xyth={}\n",
    "# use the glob package to find the ATL11s that we downloaded\n",
    "for file in glob.glob('/tmp/ATL11*.h5'):\n",
    "    with h5py.File(file,'r') as h5f:\n",
    "        try:\n",
    "            lat=np.array(h5f['pt2']['latitude'])\n",
    "            lon=np.array(h5f['pt2']['longitude'])\n",
    "        except Exception:\n",
    "            pass\n",
    "        x,y = to_xy_crs.transform(lat, lon)\n",
    "        file_xyth[file]={'x':x,'y':y, 'h':np.array(h5f['pt2']['h_corr']), 't':np.array(h5f['pt2']['delta_time'])}\n",
    "        # read the height values\n",
    "        temp=np.array(h5f['pt2']['h_corr'])\n",
    "        # identify invalid heights, and set them to NaN\n",
    "        temp[temp==h5f['pt2/']['h_corr'].attrs['_FillValue']]=np.NaN\n",
    "        \n",
    "        # store the data in a second dictionary:\n",
    "        file_xyth[file]['h']=temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965841a2-1653-414e-9a74-a0094069b8fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.3 Plotting ATL11 measurement locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13593838-2cb1-42a7-86d0-00c153e04dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "plt.imshow(dhdt, cmap='Spectral', extent=extent)\n",
    "plt.gca().set_aspect(1)\n",
    "for filename, D in file_xyth.items():\n",
    "    plt.plot(D['x'], D['y'],'.', label=filename)\n",
    "\n",
    "plt.legend(loc='lower left', bbox_to_anchor=[1, 0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae178b8a-7553-427b-9319-a2f325ef8306",
   "metadata": {},
   "source": [
    "The granules extend well beyond our region of interest, but if you zoom in on the edge of the surge region, you'll see that they all hit (or come close to hitting) the region we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f92611e-636a-4cdb-b58c-9e1452f0a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoomed-in version for rendered tutorial:\n",
    "plt.figure(); \n",
    "plt.imshow(dhdt, cmap='Spectral', extent=extent)\n",
    "plt.gca().set_aspect(1)\n",
    "for filename, D in file_xyth.items():\n",
    "    plt.plot(D['x'], D['y'],'.', label=filename)\n",
    "\n",
    "plt.gca().set_xlim(XR)\n",
    "plt.gca().set_ylim(YR)\n",
    "plt.legend(loc='lower left', bbox_to_anchor=[1, 0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89add0f-2645-4fbd-8911-69c3252c57fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 Plotting ATL11 heights\n",
    "\n",
    "Let's find the points that are within our bounding box for one of the files (the pink one in the plot we just made). We'll use the _glob_ package to find an example on track 548"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f55be-cf4b-4ea1-a893-5037c607eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ATL11_file=glob.glob('/tmp/ATL11_0548*')[0]\n",
    "print(f\"ATL11 file is {ATL11_file}\")\n",
    "xyth=file_xyth[ATL11_file]   #<---------- Try a different ATL11 file here!\n",
    "ind=xyth['x'] > XR[0] \n",
    "ind &= xyth['x'] < XR[1]\n",
    "ind &= xyth['y'] > YR[0]\n",
    "ind &= xyth['y'] < YR[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b2a94-cebb-4d2d-af02-25fb79bcf760",
   "metadata": {
    "tags": []
   },
   "source": [
    "To get an idea of what's in the file, we'll plot the heights against the projected x coordinate for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d377d8-f0ab-4043-a271-d653bcccb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(); \n",
    "for cycle in range(3, 12):\n",
    "    # Cycles in the ATL11 run from 3 to 11, so each cycle's data is in the cycle-3rd column\n",
    "    if np.any(np.isfinite(xyth['h'][ind,cycle-3])):\n",
    "        plt.plot(xyth['x'][ind], xyth['h'][ind,cycle-3], label=f'cycle={cycle}')\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel('polar stereographic x')\n",
    "plt.gca().set_ylabel('ATL11 height, m');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44003b01-b165-4a87-9652-4d2550eb33dd",
   "metadata": {},
   "source": [
    "We can see that on the left-hand side of the ice cap, the height has been constant in time, but the right-hand side of the track samples the region that is thinning rapidly.  Let's find the data for a point in the thinning region and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0e04c-28bf-4370-b9f9-eb976887e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the data close to x=1.036e6 \n",
    "ind_11 = np.argmin(np.abs(xyth['x']-1.036e6)) #<-------------Try a different x location here\n",
    "plt.figure()\n",
    "plt.plot(xyth['t'][ind_11,:]/24/3600/365.25 + 2018, xyth['h'][ind_11,:],'.')\n",
    "plt.gca().set_ylabel('ATL11 height, m')\n",
    "plt.gca().set_xlabel('year')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10185c48-3278-4f89-b2ae-3e72c41f67c5",
   "metadata": {},
   "source": [
    "## 4. ATL06 height data\n",
    "\n",
    "ATL11 data are generated by combining ALT06 data collected over the same ground track, for different cycles.  We can use CMR to identify data from a specific ground track, and we can look at our ATL11 filenames to see what \n",
    "\n",
    "Next, let's look at the ATL06 data that were used to generate the ATL11 data.  If we can acquire the ATL06 files for RGT 0548, subregion 03, we'll have the data that went into this ATL11.\n",
    "### 4.1 Querying CMR for ATL06 on a specific track\n",
    "Let's try icePyx to query CMR again to look for these data.  We can use the same query as before, but requesting ATL06, and specifying the 'tracks' keyword to obtain only RGT 0548.  Our search box is right on a region boundary, so we'll check the filenames to obtain only region 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974c947-4e5d-4102-a3ea-1a21244d4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_b = ipx.Query('ATL06', [lonlims[0], latlims[0], lonlims[1], latlims[1]], ['2018-01-01','2022-06-01'], \\\n",
    "                          start_time='00:00:00', end_time='23:59:59', tracks=[548])\n",
    "region_b.avail_granules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783452ec-9e4e-4938-9c68-ed8c718b339f",
   "metadata": {},
   "source": [
    "As before, we can download these granules using wget, using a regular expression to check that each file is actually in our region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292dbc8e-7d0d-43ca-82fa-c6fcb4a1c2ae",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "# this regular expression matches the track number, the cycle, and the region \n",
    "# in an ATL06 filename\n",
    "ATL06_re=re.compile('ATL06_\\d+_(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)_')\n",
    "\n",
    "for granule in region_b.granules.avail:\n",
    "    this_name=granule['producer_granule_id']\n",
    "    # skip the granule if we're not in region 03\n",
    "    if not ATL06_re.search(this_name).group(3) == \"03\":\n",
    "        continue\n",
    "    # check if each granule has been downloaded already\n",
    "    if os.path.isfile('/tmp/'+this_name):\n",
    "        continue\n",
    "    print(this_name)\n",
    "    ! wget -nc {granule['links'][0]['href']} -q -O /tmp/{this_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a706b5-af78-4b39-be75-23c5fb138d9d",
   "metadata": {},
   "source": [
    "### 4.2 Structure of an ATL06 granule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d61955-aa19-435a-9f33-f21fd5bd5246",
   "metadata": {
    "tags": []
   },
   "source": [
    "And, as before, we can look at the contents of one of these granules using h5ls.  As before, we'll use _glob_ to select the rgt and cycle we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4308b3-ba42-4ade-832c-6f423e0289fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick cycle 5 on track 548\n",
    "ATL06_file=glob.glob('/tmp/ATL06*_054805*.h5')[0]\n",
    "print(f\"ATL06 file is {ATL06_file}\")\n",
    "! h5ls {ATL06_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445a9471-3d5b-4b7f-b8e0-1a3d5cdd5b65",
   "metadata": {
    "tags": []
   },
   "source": [
    "The latitude, longitude, and height data are in the gtxx/land_ice_segments group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc2a9a-06c1-4a67-a8f0-228aed8482ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "! h5ls {ATL06_file}/gt2l/land_ice_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b04d7b5-853c-4b22-938b-8b1a78b8b9e1",
   "metadata": {},
   "source": [
    "### 4.3 Reading ATL06 data into a dictionary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2f926-7159-4498-906c-fed47b1e36fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's read the data, and store them in a dictionary.  We'll also parse the filename to get the cycle for each file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6042c2-8cdf-4023-bf10-1c7816cd2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_D6={}\n",
    "\n",
    "for file in sorted(glob.glob('/tmp/ATL06*.h5')):\n",
    "    this_D={}\n",
    "    with h5py.File(file,'r') as h5f:\n",
    "        for field in ['latitude','longitude','h_li','atl06_quality_summary','ground_track/x_atc']:\n",
    "            temp=np.array(h5f['gt2r/land_ice_segments'][field])\n",
    "            try:\n",
    "                temp[temp==h5f['gt2r/land_ice_segments'][field].attrs['_FillValue']]=np.NaN\n",
    "            except KeyError:\n",
    "                pass\n",
    "            # handle the 'ground_track/x_atc' field\n",
    "            if '/' in field:\n",
    "                field=field.split('/')[1]\n",
    "            this_D[field]=temp\n",
    "    this_D['x'], this_D['y'] = to_xy_crs.transform(this_D['latitude'],this_D['longitude'])\n",
    "    cycle_D6[ATL06_re.search(file).group(2)] = this_D\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747fe85f-eb5e-4f2f-a322-af800fa27373",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's also figure out what points are within our region of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6bef5-a9c3-4bcf-ba77-645c58604b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cycle, D6 in cycle_D6.items():\n",
    "    ind = D6['x'] > XR[0] \n",
    "    ind &= D6['x'] < XR[1]\n",
    "    ind &= D6['y'] > YR[0]\n",
    "    ind &= D6['y'] < YR[1]\n",
    "    D6['index']=ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007476ce-dab7-4d49-9f3c-1b0e3573b500",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's put these granules on the map, using the subset index we just calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4c2db-1034-47e8-9a29-aff205094679",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(dhdt, cmap='Spectral', extent=extent)\n",
    "plt.gca().set_aspect(1)\n",
    "for cycle, D in cycle_D6.items():\n",
    "    plt.plot(D['x'][D['index']], D['y'][D['index']],'.', label=\"cycle \"+cycle)\n",
    "plt.gca().set_xlim(XR)\n",
    "plt.gca().set_ylim(YR)\n",
    "plt.legend(loc='lower left', bbox_to_anchor=[1, 0])\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd5b172-c799-4078-a88a-b5b45eba52bb",
   "metadata": {},
   "source": [
    "If you zoom in, you'll see that there are two cycles (cycle 1 and cycle 2) off by themselves, and the rest are clustered over the RGT. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd6591-f62b-4f55-a6b6-ea58760f725b",
   "metadata": {},
   "source": [
    "### 4.3 Plotting ATL06 data\n",
    "\n",
    "Let's look at heights. We can plot the surface height (h_li) against the along-track coordinates (x_atc).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f49a5-23bc-44ca-a6c3-ba29e7a234bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for cycle, D6 in cycle_D6.items():\n",
    "    plt.plot(D6['x_atc'][D6['index']],D6['h_li'][D6['index']],'.', label=\"cycle \"+cycle)\n",
    "\n",
    "plt.legend(loc='lower left', bbox_to_anchor=[1, 0])\n",
    "\n",
    "plt.gca().set_xlabel('ATL06 along-track x, m')\n",
    "plt.gca().set_ylabel('ATL06 height, m')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98645d71-a41c-47a8-aafc-4025dba228dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "This doesn't look promising.  We have lots of height values that look nothing like a glacier, much less an island.  Fortunately, ATL06 comes with a quality flag (atl06_quality_summary) that identifies segments that are likely good (zero means no problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d7eaed-7b46-475c-80b2-66de7f055b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for cycle, D6 in cycle_D6.items():\n",
    "    good=D6['index'] & (D6['atl06_quality_summary']==0)\n",
    "    if np.any(good):\n",
    "        plt.plot(D6['x_atc'][good],D6['h_li'][good],'.', label=\"cycle \"+cycle)\n",
    "\n",
    "plt.gca().set_xlabel('ATL06 along-track x, m')\n",
    "plt.gca().set_ylabel('ATL06 height, m')\n",
    "        \n",
    "plt.legend(loc='lower left', bbox_to_anchor=[1, 0])\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616bd03-3d96-4bac-bc01-17d7fca14a75",
   "metadata": {},
   "source": [
    "Now we can see what we wanted to see.  The data now run from south to north (the opposite orientation from the previous plots), but we can see where the height is changing.  The height-change signal is somewhat obscured because the tracks are not exactly on top of one another, unlike in ATL11, where the across-track slope is taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d08e3-6d9e-4c7d-9f23-b5cd1d4b06eb",
   "metadata": {},
   "source": [
    "## 5. ATL03 data from SlideRule\n",
    "\n",
    "To go one level deeper, we'll move to a different tool.  The SlideRule project provides ATL03 photon data from a convenient web API.  We'll request data for one cycle, and show the photon heights and classifications.  SlideRule can query specific ATL03 files, and we can determine what ATL03 we want based on the ATL06 filenames we already have.   (SlideRule can also do CMR queries, but for the purposes of this tutorial, it's easiest to choose a specific file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ea379-d063-4ab3-99e9-8fe81bb4d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sliderule import icesat2\n",
    "url=\"icesat2sliderule.org\"\n",
    "icesat2.init(url, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19585323-219b-4bd7-85a2-8bd9f463b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "granule=os.path.basename(glob.glob('/tmp/ATL06*_054804*.h5')[0])\n",
    "print(granule)\n",
    "params= { 'poly':[{'lon':this_lon, 'lat':this_lat} for this_lon, this_lat in zip(corners_lon, corners_lat)],\n",
    "        'srt':3,\n",
    "         'cnf':0,\n",
    "        'len':20, \n",
    "         'track':2,\n",
    "         'pair':0,\n",
    "         'pass_invalid':True}\n",
    "\n",
    "temp=icesat2.atl03s(params, granule.replace('ATL06','ATL03'), \n",
    "                     asset=\"nsidc-s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f695940-2490-4446-a13d-f24704070eaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "The SlideRule atl03s function returns a pandas dataFrame containing photon-level data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f5b09-9130-42a4-931e-2aafc2cb74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad361614-8aa6-4efb-a0a8-0fc79652fc16",
   "metadata": {},
   "source": [
    "In SlideRule, the nomenclature is a little different from elsewhere in the ICESat-2 project : a 'track' in SlideRule is a 'Pair' elsewhere, and the 'pair' variable distinguishes the left and right beams in an ICESat-2 beam pair.  \n",
    "\n",
    "### 5.1  Plotting ATL03 photon heights and confidence values:\n",
    "\n",
    "To plot the data, we'll obtain the along-track location information using the 'segment_dist' and 'distance' fields in the dataframe, and use the 'atl03_cnf' (confidence that the photon is not noise) to color-code each photon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85f302-f744-4674-8565-5a557799062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_p=temp[temp.pair==0]\n",
    "D6=cycle_D6['04']  # get the ATL06 data from cycle 4\n",
    "plt.figure(); \n",
    "\n",
    "colors=['black','gray', 'blue','red','orange']\n",
    "\n",
    "for q_val, color in zip(np.arange(5), colors):\n",
    "    these=temp_p['atl03_cnf']==q_val\n",
    "    plt.plot(temp_p['segment_dist'][these]+temp_p['distance'][these], temp_p.height[these],'.', markersize=0.5)\n",
    "\n",
    "plt.gca().set_xlabel('along-track x, m')\n",
    "plt.gca().set_ylabel('ATL03 height, m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2bc4a-139a-4717-8165-67d008c7e9dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also overlay the ATL06 segment heights on the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c024b-f5f2-4f91-bebc-3e3f5a892556",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(temp_p['segment_dist']+temp_p['distance'], temp_p.height,'.', markersize=0.5)\n",
    "plt.plot(D6['x_atc'][D6['index']], cycle_D6['04']['h_li'][D6['index']],'r.', markersize=1)\n",
    "\n",
    "plt.gca().set_xlabel('along-track x, m')\n",
    "plt.gca().set_ylabel('ATL03/06 height, m');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd8526-439b-42fe-b8f1-7ee60a961ef6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Thanks for following along.  If you look back through the code above, there are a few different places where you can change the regions and the tracks, and see how different data look.  Later on in the hack week you'll see different (better) ways to do some of the tasks that we've done here, but the basic tools (h5py, h5ls, and numpy) are good to have on hand to help debug some of the more advanced techniques.  Happy hacking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
